[
  {
    "objectID": "starter-analysis-exercise/data/readme.html",
    "href": "starter-analysis-exercise/data/readme.html",
    "title": "Connor's Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all data at various stages.\nThis data is being loaded/manipulated/changed/saved with code from the code folders.\nYou should place the raw data in the raw_data folder and not edit it. Ever!\nIdeally, load the raw data into R and do all changes there with code, so everything is automatically reproducible and documented.\nSometimes, you need to edit the files in the format you got. For instance, Excel files are sometimes so poorly formatted that it’s close to impossible to read them into R, or the persons you got the data from used color to code some information, which of course won’t import into R. In those cases, you might have to make modifications in a software other than R. If you need to make edits in whatever format you got the data (e.g. Excel), make a copy and place those copies in a separate folder, AND ONLY EDIT THOSE COPIES. Also, write down somewhere the edits you made.\nAdd as many sub-folders as suitable. If you only have a single processing step, one sub-folder for processed data is enough. If you have multiple stages of cleaning and processing, additional sub-folders might be useful. Adjust based on the complexity of your project.\nI suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data:\nhttp://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata"
  },
  {
    "objectID": "starter-analysis-exercise/results/figures/readme.html",
    "href": "starter-analysis-exercise/results/figures/readme.html",
    "title": "Connor's Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all figures.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/results/tables-files/readme.html",
    "href": "starter-analysis-exercise/results/tables-files/readme.html",
    "title": "Connor's Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all tables (generally stored as Rds files) and other files.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/readme.html",
    "href": "starter-analysis-exercise/code/processing-code/readme.html",
    "title": "Connor's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code for processing data.\nCurrently, there is just a single Quarto file to illustrate how the processing can look like.\nInstead of a Quarto file that contains code, it is also possible to use R scripts or a combination of R scripts and Quarto code. Those approaches are illustrated in the full dataanalysis-template repository."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/readme.html",
    "href": "starter-analysis-exercise/code/eda-code/readme.html",
    "title": "Connor's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory data analysis (EDA) on the processed/cleaned data. The code produces a few tables and figures, which are saved in the appropriate results sub-folder."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda2.html",
    "href": "starter-analysis-exercise/code/eda-code/eda2.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at /Users/cgnorris/Documents/GitHub/MADA (EPID 8060E)/connornorris-MADA-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\nlibrary(ggplot2)\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          5     \n_______________________          \nColumn type frequency:           \n  character                1     \n  factor                   1     \n  numeric                  3     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate min max empty n_unique whitespace\n1 Education             0             1  10  21     0        4          0\n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean   sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0 133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2  45  55  70  80  110 ▇▂▃▂▂\n3 Books_Read            0             1  22.3 19.8   1   3  19  44   46 ▇▂▁▂▆\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\nBoxplot of height by education:\n\np5 &lt;- mydata %&gt;% ggplot(aes(x=Education, y=Height)) + \n  geom_boxplot()\nplot(p5)\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-education.png\")\nggsave(filename = figure_file, plot=p5) \n\nSaving 7 x 5 in image\n\n\nScatterplot of books read vs. weight:\n\np6 &lt;- mydata %&gt;% ggplot(aes(x=Weight, y=Books_Read)) + \n  geom_point() + \n  geom_smooth(method='lm')\nplot(p6)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"books-read_weight.png\")\nggsave(filename = figure_file, plot=p6) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\n``` # Notes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "starter-analysis-exercise/code/analysis-code/readme.html",
    "href": "starter-analysis-exercise/code/analysis-code/readme.html",
    "title": "Connor's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory analysis and statistical analysis on the processed/cleaned data. The code produces a few tables and figures, which are saved in the results folder.\nIt’s the same code done 3 times:\n\nFirst, there is an R script that you can run which does all the computations.\nSecond, there is a Quarto file which contains exactly the same code as the R script.\nThird, my current favorite, is a Quarto file with an approach where the code is pulled in from the R script and run.\n\nThe last version has the advantage of having code in one place for easy writing/debugging, and then being able to pull the code into the Quarto file for a nice combination of text/commentary and code.\nEach way of doing this is a reasonable approach, pick whichever one you prefer or makes the most sense for your setup. Whichever approach you choose, add ample documentation/commentary so you and others can easily understand what’s going on and what is done."
  },
  {
    "objectID": "starter-analysis-exercise/products/readme.html",
    "href": "starter-analysis-exercise/products/readme.html",
    "title": "Connor's Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all the products of your project.\nFor a classical academic project, this will be a peer-reviewed manuscript, and should be placed into a manuscript folder.\nFor our case, since we’ll want to put it on the website, we call it a report.\nOften you need a library of references in bibtex format, as well as a CSL style file that determines reference formatting. Since those files might be used by several of the products, I’m placing them in the main products folder. Feel free to re-organize."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My website and data analysis portfolio",
    "section": "",
    "text": "Hello\nWelcome to my website and data analysis portfolio.\n\nPlease use the Menu Bar above to look around.\nHave fun!"
  },
  {
    "objectID": "cdc-data-exercise/cdc-data-exercise.html",
    "href": "cdc-data-exercise/cdc-data-exercise.html",
    "title": "CDC Data Exercise",
    "section": "",
    "text": "The data that I have chosen for this exercise is the NWSS Public SARS-CoV-2 Wastewater Metric Data from the CDC. The link to the website can be found here. ChatGPT and GitHub Copilot were both used to fine-tune the code below.\n\n#Load required packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at /Users/cgnorris/Documents/GitHub/MADA (EPID 8060E)/connornorris-MADA-portfolio\n\nlibrary(ggplot2)\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\n\nThis dataset was initially too big to fit within Git’s file size guidelines. To reduce file size, I cut some redundant variables as well as restricted the data to only include observations from 2023 and 2024.\n\n#Code to reduce data to acceptable size for Git\n#data_path &lt;- here(\"cdc-data-exercise\", \"NWSS_Public_SARS-CoV-2_Wastewater_Metric_Data_20250206.csv\")\n#ww &lt;- read_csv(data_path)\n#ww &lt;- ww %&gt;%\n  #select(!c(key_plot_id, sample_location_specify, reporting_jurisdiction)) %&gt;% \n  #filter(date_start &gt;= as.Date(\"2023-01-01\") & date_start &lt;= as.Date(\"2024-12-31\"))\n#write.csv(ww, 'wastewater.csv', row.names = FALSE)\n\n\n#Load dataset\ndata_path &lt;- here(\"cdc-data-exercise\", \"wastewater.csv\")\ndf &lt;- read_csv(data_path)\n\nRows: 773779 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): wwtp_jurisdiction, sample_location, county_names, county_fips, sam...\ndbl  (5): wwtp_id, population_served, ptc_15d, detect_prop_15d, percentile\ndate (3): date_start, date_end, first_sample_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nskimr::skim(df)\n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n773779\n\n\nNumber of columns\n13\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n5\n\n\nDate\n3\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nwwtp_jurisdiction\n0\n1\n4\n20\n0\n51\n0\n\n\nsample_location\n0\n1\n15\n22\n0\n2\n0\n\n\ncounty_names\n0\n1\n3\n85\n0\n655\n0\n\n\ncounty_fips\n0\n1\n5\n41\n0\n830\n0\n\n\nsampling_prior\n0\n1\n2\n3\n0\n2\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\ndate_start\n0\n1\n2023-01-01\n2024-12-31\n2024-03-07\n731\n\n\ndate_end\n0\n1\n2023-01-15\n2025-01-14\n2024-03-21\n731\n\n\nfirst_sample_date\n0\n1\n2020-07-05\n2025-01-14\n2022-10-18\n507\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nwwtp_id\n0\n1.00\n1437.05\n849.52\n1\n692\n1453.00\n2203.00\n2939\n▆▇▃▇▆\n\n\npopulation_served\n0\n1.00\n128733.79\n339018.76\n564\n11326\n32000.00\n95000.00\n4000000\n▇▁▁▁▁\n\n\nptc_15d\n62649\n0.92\n3375204.78\n80730338.99\n-100\n-50\n0.00\n97.00\n2147483647\n▇▁▁▁▁\n\n\ndetect_prop_15d\n40325\n0.95\n80.88\n33.27\n0\n75\n100.00\n100.00\n100\n▁▁▁▁▇\n\n\npercentile\n30171\n0.96\n48.29\n41.82\n0\n28\n46.29\n65.75\n999\n▇▁▁▁▁\n\n\n\n\n\n\n#Data cleaning\n\n#Convert dates from characters to Date type\ndf &lt;- df %&gt;%\n  mutate(\n    date_start = as.Date(date_start, format=\"%Y-%m-%d\"),\n    date_end = as.Date(date_end, format=\"%Y-%m-%d\"),\n    first_sample_date = as.Date(first_sample_date, format=\"%Y-%m-%d\")\n  )\n\n#Handle missing values\ndf &lt;- df %&gt;%\n  replace_na(list(ptc_15d = 0, detect_prop_15d = 0, percentile = 0))\n\n#Convert categorical variables to factors\ndf &lt;- df %&gt;%\n  mutate(\n    wwtp_jurisdiction = as.factor(wwtp_jurisdiction),\n    sample_location = as.factor(sample_location),\n    county_names = as.factor(county_names),\n    county_fips = as.factor(county_fips),\n    sampling_prior = as.factor(sampling_prior)\n  )\n\nstr(df)\n\ntibble [773,779 × 13] (S3: tbl_df/tbl/data.frame)\n $ wwtp_jurisdiction: Factor w/ 51 levels \"Alabama\",\"Alaska\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ wwtp_id          : num [1:773779] 1212 1212 1212 1212 1212 ...\n $ sample_location  : Factor w/ 2 levels \"Before treatment plant\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ county_names     : Factor w/ 655 levels \"Acadia\",\"Ada\",..: 337 337 337 337 337 337 337 337 337 337 ...\n $ county_fips      : Factor w/ 830 levels \"01005\",\"01015\",..: 20 20 20 20 20 20 20 20 20 20 ...\n $ population_served: num [1:773779] 12542 12542 12542 12542 12542 ...\n $ date_start       : Date[1:773779], format: \"2024-10-22\" \"2024-10-24\" ...\n $ date_end         : Date[1:773779], format: \"2024-11-05\" \"2024-11-07\" ...\n $ ptc_15d          : num [1:773779] 2545 13166 13406 -72 -44 ...\n $ detect_prop_15d  : num [1:773779] 67 67 80 100 100 100 100 80 80 80 ...\n $ percentile       : num [1:773779] 34.5 45.3 65 96.5 95.8 ...\n $ sampling_prior   : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ first_sample_date: Date[1:773779], format: \"2023-12-19\" \"2023-12-19\" ...\n\nsummary(df)\n\n  wwtp_jurisdiction     wwtp_id                   sample_location  \n New York  :104182   Min.   :   1   Before treatment plant: 35345  \n Michigan  : 64246   1st Qu.: 692   Treatment plant       :738434  \n Illinois  : 59311   Median :1453                                  \n California: 50130   Mean   :1437                                  \n Ohio      : 35977   3rd Qu.:2203                                  \n Missouri  : 35833   Max.   :2939                                  \n (Other)   :424100                                                 \n     county_names     county_fips     population_served   date_start        \n Jefferson : 12793   17031  : 10635   Min.   :    564   Min.   :2023-01-01  \n Orange    : 12781   26139  :  9866   1st Qu.:  11326   1st Qu.:2023-09-08  \n Washington: 10836   26077  :  8041   Median :  32000   Median :2024-03-07  \n Cook      : 10635   17043  :  7310   Mean   : 128734   Mean   :2024-02-17  \n Ottawa    : 10352   36071  :  6478   3rd Qu.:  95000   3rd Qu.:2024-08-13  \n Kalamazoo :  8041   26081  :  5848   Max.   :4000000   Max.   :2024-12-31  \n (Other)   :708341   (Other):725601                                         \n    date_end             ptc_15d           detect_prop_15d    percentile    \n Min.   :2023-01-15   Min.   :      -100   Min.   :  0.00   Min.   :  0.00  \n 1st Qu.:2023-09-22   1st Qu.:       -44   1st Qu.: 50.00   1st Qu.: 25.67  \n Median :2024-03-21   Median :         0   Median :100.00   Median : 44.80  \n Mean   :2024-03-02   Mean   :   3101931   Mean   : 76.67   Mean   : 46.40  \n 3rd Qu.:2024-08-27   3rd Qu.:        75   3rd Qu.:100.00   3rd Qu.: 65.00  \n Max.   :2025-01-14   Max.   :2147483647   Max.   :100.00   Max.   :999.00  \n                                                                            \n sampling_prior first_sample_date   \n no :676556     Min.   :2020-07-05  \n yes: 97223     1st Qu.:2022-02-16  \n                Median :2022-10-18  \n                Mean   :2022-10-10  \n                3rd Qu.:2023-06-26  \n                Max.   :2025-01-14  \n                                    \n\n\n\n#Exploratory Data Analysis\n\n#Names of categorical variables\ncategorical_vars &lt;- c(\"wwtp_jurisdiction\", \"sample_location\", \"county_names\", \"county_fips\", \"sampling_prior\")\n\n#Summary of categorical variables\nfor (var in categorical_vars) {\n  summary_table &lt;- df %&gt;%\n    count(!!sym(var)) %&gt;%\n    mutate(Percent = n / sum(n) * 100)\n  \n  write_csv(summary_table, paste0(var, \"_summary.csv\"))\n}\n\n#Names of continuous variables\ncontinuous_vars &lt;- df %&gt;%\n  select(population_served, ptc_15d, detect_prop_15d, percentile)\n\nfor (var in names(continuous_vars)) {\n  summary_table &lt;- continuous_vars %&gt;%\n    summarise(\n      Variable = var,\n      Mean = mean(.data[[var]], na.rm = TRUE),\n      SD = sd(.data[[var]], na.rm = TRUE),\n      Min = min(.data[[var]], na.rm = TRUE),\n      Max = max(.data[[var]], na.rm = TRUE),\n      Median = median(.data[[var]], na.rm = TRUE)\n    )\n  write_csv(summary_table, paste0(var, \"_summary.csv\"))\n}\n\ndetect_prop_15d_summary &lt;- read_csv(\"detect_prop_15d_summary.csv\")\n\nRows: 1 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Variable\ndbl (5): Mean, SD, Min, Max, Median\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npercentile_summary &lt;- read_csv(\"percentile_summary.csv\")\n\nRows: 1 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Variable\ndbl (5): Mean, SD, Min, Max, Median\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npopulation_served_summary &lt;- read_csv(\"population_served_summary.csv\")\n\nRows: 1 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Variable\ndbl (5): Mean, SD, Min, Max, Median\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nptc_15d_summary &lt;- read_csv(\"ptc_15d_summary.csv\")\n\nRows: 1 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Variable\ndbl (5): Mean, SD, Min, Max, Median\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncontinuous_summary &lt;- rbind(detect_prop_15d_summary, percentile_summary, population_served_summary, ptc_15d_summary)\n\nwrite_csv(continuous_summary, \"continuous_summary.csv\")\n\nsummary(df$ptc_15d)\n\n      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \n      -100        -44          0    3101931         75 2147483647 \n\n\n\n#Plot distributions of continuous variables\nggplot(df, aes(x = population_served)) +\n  geom_histogram(binwidth = 1000, fill = \"blue\", color = \"blue\") +\n  labs(title = \"Distribution of Population Served\", x = \"Population Served\", y = \"Frequency\")\n\n\n\n\n\n\n\nggplot(df, aes(x = ptc_15d)) +\n  geom_histogram(fill = \"green\", color = \"green\") +\n  xlim(-100, 100) +\n  labs(title = \"Distribution of PTC 15d\", x = \"PTC 15d\", y = \"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 175282 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\nggplot(df, aes(x = detect_prop_15d)) +\n  geom_histogram(binwidth = 10, fill = \"red\", color = \"red\") +\n  labs(title = \"Distribution of Detect Prop 15d\", x = \"Detect Prop 15d\", y = \"Frequency\")\n\n\n\n\n\n\n\nggplot(df, aes(x = percentile)) +\n  geom_histogram(binwidth = 5, fill = \"purple\", color = \"purple\") +\n  labs(title = \"Distribution of Percentile\", x = \"Percentile\", y = \"Frequency\")\n\n\n\n\n\n\n\n#Save plots\nggsave(\"population_served_distribution.png\")\n\nSaving 7 x 5 in image\n\nggsave(\"ptc_15d_distribution.png\")\n\nSaving 7 x 5 in image\n\nggsave(\"detect_prop_15d_distribution.png\")\n\nSaving 7 x 5 in image\n\nggsave(\"percentile_distribution.png\")\n\nSaving 7 x 5 in image\n\n#Plot distributions of categorical variables\nggplot(df, aes(x = wwtp_jurisdiction)) +\n  geom_bar(fill = \"blue\") +\n  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +\n  labs(title = \"Distribution of wwtp_jurisdiction\",\n       x = \"wwtp_jurisdiction\",\n       y = \"count\")\n\n\n\n\n\n\n\nggplot(filter(df, wwtp_jurisdiction == \"Georgia\"), aes(x = county_names)) +\n  geom_bar(fill = \"green\") +\n  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +\n  labs(title = \"Distribution of counties in Georgia\",\n       x = \"counties\",\n       y = \"count\")\n\n\n\n\n\n\n\nggplot(df, aes(x = sample_location)) +\n  geom_bar(fill = \"red\") +\n  theme(axis.text.x = element_text(angle = 0.5, hjust = 0.5)) +\n  labs(title = \"Distribution of sample_location\",\n       x = \"sample location\",\n       y = \"count\")\n\n\n\n\n\n\n\nggplot(df, aes(x = sampling_prior)) +\n  geom_bar(fill = \"purple\") +\n  theme(axis.text.x = element_text(angle = 0.5, hjust = 0.5)) +\n  labs(title = \"Distribution of sampling_prior\",\n       x = \"prior sampling\",\n       y = \"count\")\n\n\n\n\n\n\n\nggsave(\"jurisdiction_distribution.png\")\n\nSaving 7 x 5 in image\n\nggsave(\"georgia_counties_distribution.png\")\n\nSaving 7 x 5 in image\n\nggsave(\"sample_location_distribution.png\") \n\nSaving 7 x 5 in image\n\nggsave(\"sampling_prior_distribution.png\")\n\nSaving 7 x 5 in image\n\n\n\n\n\n#some additional exploration\n\nunique(df$wwtp_jurisdiction)\n\n [1] Arizona              Michigan             Texas               \n [4] Pennsylvania         Maine                New Jersey          \n [7] New York             Oklahoma             Virginia            \n[10] Wisconsin            Colorado             New York City       \n[13] Utah                 Washington           West Virginia       \n[16] California           Illinois             New Hampshire       \n[19] Florida              Hawaii               Maryland            \n[22] Missouri             Oregon               Louisiana           \n[25] Vermont              Indiana              North Carolina      \n[28] Rhode Island         Ohio                 South Carolina      \n[31] Minnesota            Kansas               Georgia             \n[34] Massachusetts        Arkansas             Nebraska            \n[37] Nevada               Alabama              Montana             \n[40] Connecticut          Delaware             New Mexico          \n[43] Kentucky             Wyoming              Tennessee           \n[46] District of Columbia Iowa                 Alaska              \n[49] Idaho                South Dakota         Mississippi         \n51 Levels: Alabama Alaska Arizona Arkansas California Colorado ... Wyoming\n\n\n\nunique(df$county_names)\n\n  [1] Maricopa                                                                             \n  [2] Ottawa                                                                               \n  [3] Dallas                                                                               \n  [4] Kalamazoo                                                                            \n  [5] Butler                                                                               \n  [6] Penobscot                                                                            \n  [7] Bergen                                                                               \n  [8] Cumberland                                                                           \n  [9] Delaware                                                                             \n [10] Muskogee                                                                             \n [11] Virginia Beach City,Chesapeake City                                                  \n [12] Marathon                                                                             \n [13] Summit                                                                               \n [14] Queens                                                                               \n [15] Dauphin                                                                              \n [16] Tazewell                                                                             \n [17] Island,Snohomish                                                                     \n [18] Ohio                                                                                 \n [19] Santa Cruz                                                                           \n [20] Saint Clair                                                                          \n [21] Sullivan                                                                             \n [22] Chautauqua                                                                           \n [23] Pinellas                                                                             \n [24] Honolulu                                                                             \n [25] Du Page                                                                              \n [26] Anne Arundel                                                                         \n [27] Salem                                                                                \n [28] Riverside                                                                            \n [29] Cole,Callaway                                                                        \n [30] Westchester                                                                          \n [31] Multnomah                                                                            \n [32] Umatilla                                                                             \n [33] La Crosse                                                                            \n [34] Dunn                                                                                 \n [35] Lafayette                                                                            \n [36] Addison                                                                              \n [37] Mohave                                                                               \n [38] Marion                                                                               \n [39] Otsego                                                                               \n [40] Durham,Orange                                                                        \n [41] New Hanover                                                                          \n [42] Forsyth                                                                              \n [43] Allegheny                                                                            \n [44] Los Angeles                                                                          \n [45] San Francisco,San Mateo                                                              \n [46] Monmouth                                                                             \n [47] Bristol                                                                              \n [48] Butte                                                                                \n [49] St Joseph                                                                            \n [50] Vanderburgh                                                                          \n [51] Huron                                                                                \n [52] Skagit                                                                               \n [53] Santa Clara                                                                          \n [54] Hamilton                                                                             \n [55] Saint Louis                                                                          \n [56] Knox                                                                                 \n [57] Hampton                                                                              \n [58] Horry                                                                                \n [59] El Paso                                                                              \n [60] Kane                                                                                 \n [61] Terrebonne                                                                           \n [62] Union                                                                                \n [63] Genesee                                                                              \n [64] Isanti                                                                               \n [65] Albany                                                                               \n [66] Pierce                                                                               \n [67] Salt Lake                                                                            \n [68] Sonoma                                                                               \n [69] Orange                                                                               \n [70] Boone                                                                                \n [71] Jefferson                                                                            \n [72] Mecklenburg                                                                          \n [73] Erie                                                                                 \n [74] Yamhill                                                                              \n [75] El Dorado                                                                            \n [76] Shawnee                                                                              \n [77] Fulton                                                                               \n [78] Cache                                                                                \n [79] Fairfax,Alexandria City                                                              \n [80] Yakima                                                                               \n [81] Clayton                                                                              \n [82] Seneca                                                                               \n [83] Newport                                                                              \n [84] Milwaukee,Ozaukee                                                                    \n [85] Somerset                                                                             \n [86] Contra Costa                                                                         \n [87] Ouachita                                                                             \n [88] Suffolk,Middlesex                                                                    \n [89] Sandusky                                                                             \n [90] Portsmouth City,Isle Of Wight,Chesapeake City,Suffolk City                           \n [91] Cabell                                                                               \n [92] Mason                                                                                \n [93] Greene                                                                               \n [94] Lincoln                                                                              \n [95] Santa Barbara                                                                        \n [96] Dunklin                                                                              \n [97] Lancaster                                                                            \n [98] Utah                                                                                 \n [99] Carson City                                                                          \n[100] Niagara                                                                              \n[101] Ontario                                                                              \n[102] Saint Tammany                                                                        \n[103] Grafton                                                                              \n[104] Columbia                                                                             \n[105] Dutchess                                                                             \n[106] Pontotoc                                                                             \n[107] Clark                                                                                \n[108] Sumter                                                                               \n[109] Houston                                                                              \n[110] Vigo                                                                                 \n[111] Saint Mary                                                                           \n[112] Essex                                                                                \n[113] Hill                                                                                 \n[114] Wake                                                                                 \n[115] Lawrence                                                                             \n[116] Georgetown                                                                           \n[117] San Juan,Grand                                                                       \n[118] Walworth                                                                             \n[119] Milwaukee,Ozaukee,Racine,Waukesha,Washington                                         \n[120] Mineral                                                                              \n[121] Orleans                                                                              \n[122] Goodhue                                                                              \n[123] Pulaski                                                                              \n[124] Yuma                                                                                 \n[125] Pima                                                                                 \n[126] La Plata                                                                             \n[127] Elkhart                                                                              \n[128] Suffolk,Middlesex,Norfolk                                                            \n[129] Kent                                                                                 \n[130] Gratiot                                                                              \n[131] Rensselaer                                                                           \n[132] Providence                                                                           \n[133] De Kalb                                                                              \n[134] Chippewa                                                                             \n[135] Eaton                                                                                \n[136] Johnson                                                                              \n[137] Hardin                                                                               \n[138] Pender,Duplin                                                                        \n[139] Washington                                                                           \n[140] Broward                                                                              \n[141] Marquette                                                                            \n[142] Hennepin                                                                             \n[143] Clay                                                                                 \n[144] Monroe                                                                               \n[145] Wyandot                                                                              \n[146] King,Snohomish                                                                       \n[147] Noble                                                                                \n[148] Suffolk                                                                              \n[149] Fairfield                                                                            \n[150] Snohomish                                                                            \n[151] Burlington                                                                           \n[152] Wyoming                                                                              \n[153] Philadelphia                                                                         \n[154] Arapahoe,Adams                                                                       \n[155] New Castle                                                                           \n[156] La Salle                                                                             \n[157] Scott,New Madrid                                                                     \n[158] Otero                                                                                \n[159] Schenectady                                                                          \n[160] Saint Lawrence                                                                       \n[161] Andrews                                                                              \n[162] Kanawha                                                                              \n[163] Travis                                                                               \n[164] Wayne                                                                                \n[165] Sussex                                                                               \n[166] Rockingham                                                                           \n[167] Centre                                                                               \n[168] Bedford City,Lynchburg City,Amherst,Bedford,Campbell                                 \n[169] Randolph                                                                             \n[170] Plumas                                                                               \n[171] Muskegon                                                                             \n[172] Nobles                                                                               \n[173] Montgomery                                                                           \n[174] Chelan                                                                               \n[175] Boulder                                                                              \n[176] Grand Traverse                                                                       \n[177] Kewaunee,Brown                                                                       \n[178] Madison                                                                              \n[179] Houghton                                                                             \n[180] Park                                                                                 \n[181] Payne                                                                                \n[182] Hampton City,Newport News City,York,James City                                       \n[183] Richmond                                                                             \n[184] Macon                                                                                \n[185] Miami-Dade                                                                           \n[186] Carlton,Saint Louis                                                                  \n[187] Onslow                                                                               \n[188] Marin                                                                                \n[189] Acadia                                                                               \n[190] Aroostook                                                                            \n[191] Kings                                                                                \n[192] Stafford                                                                             \n[193] Hampton City,Newport News City                                                       \n[194] San Luis Obispo                                                                      \n[195] Jackson                                                                              \n[196] Tompkins                                                                             \n[197] Livingston                                                                           \n[198] Webb                                                                                 \n[199] Sangamon                                                                             \n[200] Bienville                                                                            \n[201] Preston                                                                              \n[202] Rock                                                                                 \n[203] Shelby                                                                               \n[204] Platte,Clay                                                                          \n[205] Schoharie                                                                            \n[206] Benton                                                                               \n[207] Macomb                                                                               \n[208] Mahoning                                                                             \n[209] Guilford                                                                             \n[210] Mclennan                                                                             \n[211] Douglas                                                                              \n[212] Sheridan                                                                             \n[213] Jessamine                                                                            \n[214] Warren                                                                               \n[215] Laramie                                                                              \n[216] Catoosa,Walker,Dade,Hamilton                                                         \n[217] Lee                                                                                  \n[218] Solano                                                                               \n[219] Fairfax,Prince Georges,District Of Columbia,Loudoun,Montgomery,Arlington             \n[220] Winnebago                                                                            \n[221] Marshall                                                                             \n[222] Buncombe,Henderson                                                                   \n[223] Cheshire                                                                             \n[224] Kershaw                                                                              \n[225] Harris                                                                               \n[226] King,Pierce                                                                          \n[227] Isabella                                                                             \n[228] Stanislaus                                                                           \n[229] Arenac                                                                               \n[230] Chemung                                                                              \n[231] Mercer                                                                               \n[232] Nueces                                                                               \n[233] New Haven                                                                            \n[234] Vernon                                                                               \n[235] Jackson,Cass                                                                         \n[236] Bryan                                                                                \n[237] Hunterdon                                                                            \n[238] Lorain                                                                               \n[239] Cook                                                                                 \n[240] Tippecanoe                                                                           \n[241] Calloway                                                                             \n[242] Garland                                                                              \n[243] Franklin                                                                             \n[244] Carbon                                                                               \n[245] Caddo                                                                                \n[246] York                                                                                 \n[247] Outagamie,Winnebago,Calumet                                                          \n[248] Martinsville City,Henry                                                              \n[249] East Feliciana                                                                       \n[250] Brown                                                                                \n[251] Shasta                                                                               \n[252] Oswego                                                                               \n[253] Otter Tail                                                                           \n[254] Middlesex,Somerset,Union                                                             \n[255] Oneida                                                                               \n[256] Lane                                                                                 \n[257] Weber,Davis                                                                          \n[258] Kenosha                                                                              \n[259] Schoolcraft                                                                          \n[260] Saginaw                                                                              \n[261] Rockland                                                                             \n[262] Hawaii                                                                               \n[263] Tuscaloosa                                                                           \n[264] Le Sueur,Scott                                                                       \n[265] Henry                                                                                \n[266] Tarrant                                                                              \n[267] Fond Du Lac                                                                          \n[268] Sherburne,Benton,Stearns                                                             \n[269] Juneau                                                                               \n[270] Warrick                                                                              \n[271] Lewis And Clark                                                                      \n[272] Josephine                                                                            \n[273] Victoria                                                                             \n[274] Iroquois                                                                             \n[275] Champaign                                                                            \n[276] Lake                                                                                 \n[277] Boyd                                                                                 \n[278] Nassau                                                                               \n[279] Deschutes                                                                            \n[280] Greenwood                                                                            \n[281] Alachua                                                                              \n[282] Lucas                                                                                \n[283] Saint Marys                                                                          \n[284] Napa                                                                                 \n[285] Mcdonough                                                                            \n[286] Calhoun                                                                              \n[287] Island                                                                               \n[288] Bulloch                                                                              \n[289] Clare                                                                                \n[290] Lyon                                                                                 \n[291] Henderson                                                                            \n[292] Washtenaw                                                                            \n[293] Weld                                                                                 \n[294] Hudson                                                                               \n[295] Kendall                                                                              \n[296] Vermilion                                                                            \n[297] Richland                                                                             \n[298] Sherburne                                                                            \n[299] Loudoun                                                                              \n[300] Coconino                                                                             \n[301] Peoria                                                                               \n[302] Tulsa                                                                                \n[303] Klamath                                                                              \n[304] Muscatine                                                                            \n[305] Oakland,Wayne                                                                        \n[306] Johnson,Jackson,Cass                                                                 \n[307] Dent                                                                                 \n[308] Nemaha                                                                               \n[309] Cabell,Wayne                                                                         \n[310] Windsor                                                                              \n[311] Concordia                                                                            \n[312] Saratoga                                                                             \n[313] Clackamas                                                                            \n[314] Alameda                                                                              \n[315] Oxford                                                                               \n[316] San Diego                                                                            \n[317] Jasper                                                                               \n[318] Dickinson                                                                            \n[319] Northampton,Halifax                                                                  \n[320] Strafford                                                                            \n[321] Morris                                                                               \n[322] Portage                                                                              \n[323] Sarasota                                                                             \n[324] Leon                                                                                 \n[325] Waupaca                                                                              \n[326] Perry                                                                                \n[327] Reno                                                                                 \n[328] Ulster                                                                               \n[329] Hidalgo                                                                              \n[330] Bedford City,Botetourt,Roanoke,Bland,Salem,Roanoke City,Bedford                      \n[331] Lenoir                                                                               \n[332] Platte                                                                               \n[333] Licking                                                                              \n[334] Ada                                                                                  \n[335] Waldo                                                                                \n[336] Bay                                                                                  \n[337] Le Sueur                                                                             \n[338] Monongalia                                                                           \n[339] Emmet                                                                                \n[340] Newport News City,York,New Kent,Williamsburg City,James City                         \n[341] Sweetwater                                                                           \n[342] Yankton                                                                              \n[343] Martin,Palm Beach                                                                    \n[344] Westmoreland                                                                         \n[345] Mckean                                                                               \n[346] Norton City,Wise                                                                     \n[347] Henrico                                                                              \n[348] Frederick,Winchester City                                                            \n[349] Saint Bernard                                                                        \n[350] Monterey                                                                             \n[351] Hillsborough                                                                         \n[352] Floyd                                                                                \n[353] Cobb                                                                                 \n[354] Carter                                                                               \n[355] Scott,Hennepin,Carver                                                                \n[356] Essex,Union                                                                          \n[357] Radford,Montgomery,Pulaski                                                           \n[358] La Paz                                                                               \n[359] Waukesha,Jefferson                                                                   \n[360] Saint Louis,Jefferson                                                                \n[361] Duchesne                                                                             \n[362] Waukesha                                                                             \n[363] Onondaga                                                                             \n[364] Hampden                                                                              \n[365] Plaquemines                                                                          \n[366] Buffalo                                                                              \n[367] Anchorage                                                                            \n[368] Ventura                                                                              \n[369] Rowan                                                                                \n[370] Iberia                                                                               \n[371] Macomb,Wayne                                                                         \n[372] Forsyth,Guilford,Randolph,Davidson                                                   \n[373] Polk                                                                                 \n[374] Evangeline                                                                           \n[375] Dawes                                                                                \n[376] Olmsted                                                                              \n[377] Tangipahoa                                                                           \n[378] Tuscola                                                                              \n[379] Dodge,Washington                                                                     \n[380] Wyandotte                                                                            \n[381] Latah                                                                                \n[382] Washoe                                                                               \n[383] Pike                                                                                 \n[384] Berkeley,Dorchester,Charleston                                                       \n[385] Bronx                                                                                \n[386] Kern                                                                                 \n[387] Arapahoe                                                                             \n[388] Natchitoches                                                                         \n[389] Anoka,Hennepin,Dakota,Ramsey,Washington                                              \n[390] Atlantic                                                                             \n[391] Harrisonburg City,Rockingham                                                         \n[392] Peach                                                                                \n[393] Beauregard                                                                           \n[394] Scioto                                                                               \n[395] Utah,Salt Lake                                                                       \n[396] Fairfax,Alexandria City,Arlington,Falls Church City                                  \n[397] Palm Beach                                                                           \n[398] Rock Island                                                                          \n[399] Allen                                                                                \n[400] West Baton Rouge                                                                     \n[401] Christian                                                                            \n[402] Silver Bow                                                                           \n[403] Alleghany                                                                            \n[404] Fresno                                                                               \n[405] Smith                                                                                \n[406] Stark                                                                                \n[407] Dakota,Woodbury,Union                                                                \n[408] Howell                                                                               \n[409] Luna                                                                                 \n[410] Beaufort                                                                             \n[411] Uintah                                                                               \n[412] Russell                                                                              \n[413] Wapello                                                                              \n[414] Scotland                                                                             \n[415] New York                                                                             \n[416] Canadian                                                                             \n[417] Wasatch                                                                              \n[418] Davis                                                                                \n[419] Iron                                                                                 \n[420] Kootenai                                                                             \n[421] Morgan                                                                               \n[422] Green                                                                                \n[423] Jo Daviess                                                                           \n[424] Wilson                                                                               \n[425] Kittitas                                                                             \n[426] Grant                                                                                \n[427] Hall                                                                                 \n[428] Placer                                                                               \n[429] Paulding                                                                             \n[430] Johnson,Jackson,Wyandotte                                                            \n[431] Carteret                                                                             \n[432] Westmoreland,Armstrong                                                               \n[433] Spokane                                                                              \n[434] Garfield                                                                             \n[435] Oklahoma                                                                             \n[436] Dane                                                                                 \n[437] Merced                                                                               \n[438] Bernalillo                                                                           \n[439] Pitkin                                                                               \n[440] Pitt                                                                                 \n[441] Putnam                                                                               \n[442] Routt                                                                                \n[443] San Bernardino                                                                       \n[444] Manitowoc                                                                            \n[445] Mille Lacs                                                                           \n[446] Sheboygan                                                                            \n[447] Chittenden                                                                           \n[448] Newton                                                                               \n[449] Carroll                                                                              \n[450] San Francisco                                                                        \n[451] Thurston                                                                             \n[452] Mchenry                                                                              \n[453] Pennington                                                                           \n[454] Durham                                                                               \n[455] Mono                                                                                 \n[456] Bartholomew                                                                          \n[457] Kennebec                                                                             \n[458] Hood River                                                                           \n[459] Allegan                                                                              \n[460] Saint Louis,Saint Charles                                                            \n[461] Doddridge                                                                            \n[462] El Dorado,Nevada,Placer                                                              \n[463] New London                                                                           \n[464] Manistee                                                                             \n[465] Saint Charles                                                                        \n[466] Cumberland,Oxford,Belknap,Carroll                                                    \n[467] Macoupin                                                                             \n[468] Teton                                                                                \n[469] Freeborn                                                                             \n[470] Clinton                                                                              \n[471] Ashtabula                                                                            \n[472] San Joaquin                                                                          \n[473] Dubois                                                                               \n[474] Delta                                                                                \n[475] Cuyahoga                                                                             \n[476] San Mateo                                                                            \n[477] Orange,Seminole                                                                      \n[478] Midland                                                                              \n[479] Dona Ana                                                                             \n[480] Sandoval                                                                             \n[481] Lackawanna                                                                           \n[482] Essex,Hudson,Union,Passaic,Bergen                                                    \n[483] Blue Earth                                                                           \n[484] Norfolk City                                                                         \n[485] Worcester                                                                            \n[486] Ventura,Los Angeles                                                                  \n[487] Whiteside                                                                            \n[488] Dodge                                                                                \n[489] Effingham                                                                            \n[490] Colbert                                                                              \n[491] Bureau                                                                               \n[492] Kanabec                                                                              \n[493] Yates                                                                                \n[494] Galveston                                                                            \n[495] Sutter                                                                               \n[496] Stanton,Madison                                                                      \n[497] Marlboro                                                                             \n[498] Box Elder                                                                            \n[499] Iosco                                                                                \n[500] Lewis                                                                                \n[501] Chenango                                                                             \n[502] Lexington                                                                            \n[503] Volusia                                                                              \n[504] Kandiyohi                                                                            \n[505] Trumbull                                                                             \n[506] Whitman                                                                              \n[507] Muscogee,Chattahoochee                                                               \n[508] Blue Earth,Nicollet                                                                  \n[509] Laclede                                                                              \n[510] Cherokee                                                                             \n[511] Frederick                                                                            \n[512] Orange,Pinellas                                                                      \n[513] Cortland                                                                             \n[514] Hocking                                                                              \n[515] Garrett                                                                              \n[516] San Benito                                                                           \n[517] Defiance                                                                             \n[518] Wichita                                                                              \n[519] Holt                                                                                 \n[520] Wasco                                                                                \n[521] Tillamook                                                                            \n[522] Wood                                                                                 \n[523] Fayette                                                                              \n[524] Linn                                                                                 \n[525] Passaic                                                                              \n[526] Alamosa                                                                              \n[527] Audrain                                                                              \n[528] Broome                                                                               \n[529] Schuyler                                                                             \n[530] Henrico,Richmond City,Goochland                                                      \n[531] Lehigh                                                                               \n[532] Barry                                                                                \n[533] Tioga                                                                                \n[534] Herkimer                                                                             \n[535] Mesa                                                                                 \n[536] Dakota                                                                               \n[537] Anderson                                                                             \n[538] Albemarle,Charlottesville City                                                       \n[539] Lafourche                                                                            \n[540] Bossier                                                                              \n[541] Lenawee                                                                              \n[542] Clarke                                                                               \n[543] Crawford                                                                             \n[544] Portage,Summit                                                                       \n[545] Piscataquis                                                                          \n[546] Del Norte                                                                            \n[547] Buchanan                                                                             \n[548] Mcdowell                                                                             \n[549] Chester                                                                              \n[550] Cumberland,Prince Edward                                                             \n[551] Sagadahoc                                                                            \n[552] Stephenson                                                                           \n[553] Red River                                                                            \n[554] Horry,Columbus                                                                       \n[555] Kankakee                                                                             \n[556] Kaufman,Collin,Dallas                                                                \n[557] Belmont                                                                              \n[558] Gregg                                                                                \n[559] Marathon,Wood                                                                        \n[560] Athens                                                                               \n[561] Taylor                                                                               \n[562] Scott,Cape Girardeau                                                                 \n[563] Gogebic                                                                              \n[564] Greene,Christian                                                                     \n[565] Maui                                                                                 \n[566] Saint Louis,Saint Louis City                                                         \n[567] Richland,Lexington                                                                   \n[568] Harrison                                                                             \n[569] Chisago                                                                              \n[570] Barry,Lawrence                                                                       \n[571] Northampton,Bucks                                                                    \n[572] Yolo                                                                                 \n[573] Petersburg City                                                                      \n[574] Williams                                                                             \n[575] Tooele                                                                               \n[576] Santa Cruz,Monterey                                                                  \n[577] Gallatin                                                                             \n[578] Hampton City,Newport News City,York,Gloucester,Mathews,Poquoson City                 \n[579] Middlesex                                                                            \n[580] Humboldt                                                                             \n[581] Rice,Dakota                                                                          \n[582] Stafford,Prince William                                                              \n[583] Santa Fe                                                                             \n[584] Scott,Hennepin,Dakota                                                                \n[585] Scotts Bluff                                                                         \n[586] Steuben                                                                              \n[587] Saline                                                                               \n[588] Androscoggin                                                                         \n[589] Pueblo                                                                               \n[590] Tuscarawas                                                                           \n[591] Saint Croix,Pierce                                                                   \n[592] Nevada                                                                               \n[593] Mackinac                                                                             \n[594] Brazos                                                                               \n[595] Saint Croix                                                                          \n[596] Swain,Jackson                                                                        \n[597] Worcester,Providence,Norfolk                                                         \n[598] Walla Walla                                                                          \n[599] Medina                                                                               \n[600] Sacramento                                                                           \n[601] Barbour                                                                              \n[602] Fairbanks North Star                                                                 \n[603] Escambia                                                                             \n[604] Watauga                                                                              \n[605] Denver                                                                               \n[606] Scott                                                                                \n[607] Sarpy,Douglas                                                                        \n[608] Cleveland                                                                            \n[609] Prince William,Fairfax                                                               \n[610] Coshocton                                                                            \n[611] Randall,Potter                                                                       \n[612] Muskingum                                                                            \n[613] Clatsop                                                                              \n[614] Chippewa,Eau Claire                                                                  \n[615] Suffolk,Middlesex,Worcester,Plymouth,Norfolk                                         \n[616] Cattaraugus                                                                          \n[617] San Miguel                                                                           \n[618] Kenton                                                                               \n[619] Essex,Hudson,Passaic,Bergen                                                          \n[620] Darke                                                                                \n[621] Pleasants                                                                            \n[622] Will                                                                                 \n[623] Howard                                                                               \n[624] Seminole                                                                             \n[625] Coos                                                                                 \n[626] Mercer,Trumbull                                                                      \n[627] Saint Johns                                                                          \n[628] Gwinnett                                                                             \n[629] Oakland,Macomb                                                                       \n[630] Chaves                                                                               \n[631] Hays                                                                                 \n[632] Beadle                                                                               \n[633] Baraga                                                                               \n[634] Camden                                                                               \n[635] Malheur                                                                              \n[636] Indiana                                                                              \n[637] Morrison                                                                             \n[638] Prince William,Fairfax,Fauquier,Loudoun,Fairfax City,Manassas City,Manassas Park City\n[639] Kosciusko                                                                            \n[640] Calcasieu                                                                            \n[641] Hancock                                                                              \n[642] Pickaway                                                                             \n[643] Adams                                                                                \n[644] Woodward                                                                             \n[645] Cayuga                                                                               \n[646] Preble                                                                               \n[647] Dougherty                                                                            \n[648] Webster                                                                              \n[649] Menominee                                                                            \n[650] Ashland                                                                              \n[651] Marinette                                                                            \n[652] Cass                                                                                 \n[653] Virginia Beach City,Norfolk City,Portsmouth City,Chesapeake City                     \n[654] Dodge,Jefferson                                                                      \n[655] Cooke                                                                                \n655 Levels: Acadia Ada Adams Addison Alachua Alameda Alamosa ... Yuma\n\n\n\n#Utilizing the summary table provided I will create synthetic data for each of the 13 variables\n#First I will define the number of obs.\nn_rows&lt;- 773779\n\n\n# Load necessary libraries\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Define the number of rows for the synthetic dataset\nn_rows &lt;- 773779\n\n# Generate synthetic data for each variable\nset.seed(123)  # For reproducibility\n\n# 1. wwtp_jurisdiction (categorical variable)\n# Include all 50 states and the District of Columbia\nstates &lt;- c(\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \n            \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \n            \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \n            \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \n            \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \n            \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"New York\", \n            \"North Carolina\", \"North Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\", \n            \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \n            \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \n            \"West Virginia\", \"Wisconsin\", \"Wyoming\", \"District of Columbia\")\n\n# Use the same proportions as in the original data (adjust probabilities as needed)\nwwtp_jurisdiction &lt;- sample(states, size = n_rows, replace = TRUE, \n                            prob = c(rep(0.02, 50), 0.01))  # Adjust probabilities as needed\n\n# 2. wwtp_id (numeric variable)\n# Use a uniform distribution with the same min and max as the original data\nwwtp_id &lt;- sample(1:2939, size = n_rows, replace = TRUE)\n\n# 3. sample_location (categorical variable)\n# Use the same proportions as in the original data\nsample_location &lt;- sample(c(\"Before treatment plant\", \"Treatment plant\"), \n                          size = n_rows, replace = TRUE, prob = c(0.046, 0.954))  # Adjust probabilities as needed\n\n# 4. county_names \n# Use the provided list of 655 counties\ncounty_names &lt;- c(\"Maricopa\", \"Ottawa\", \"Dallas\", \"Kalamazoo\", \"Butler\", \"Penobscot\", \"Bergen\", \n                  \"Cumberland\", \"Delaware\", \"Muskogee\", \"Virginia Beach City,Chesapeake City\", \n                  \"Marathon\", \"Summit\", \"Queens\", \"Dauphin\", \"Tazewell\", \"Island,Snohomish\", \n                  \"Ohio\", \"Santa Cruz\", \"Saint Clair\", \"Sullivan\", \"Chautauqua\", \"Pinellas\", \n                  \"Honolulu\", \"Du Page\", \"Anne Arundel\", \"Salem\", \"Riverside\", \"Cole,Callaway\", \n                  \"Westchester\", \"Multnomah\", \"Umatilla\", \"La Crosse\", \"Dunn\", \"Lafayette\", \n                  \"Addison\", \"Mohave\", \"Marion\", \"Otsego\", \"Durham,Orange\", \"New Hanover\", \n                  \"Forsyth\", \"Allegheny\", \"Los Angeles\", \"San Francisco,San Mateo\", \"Monmouth\", \n                  \"Bristol\", \"Butte\", \"St Joseph\", \"Vanderburgh\", \"Huron\", \"Skagit\", \"Santa Clara\", \n                  \"Hamilton\", \"Saint Louis\", \"Knox\", \"Hampton\", \"Horry\", \"El Paso\", \"Kane\", \n                  \"Terrebonne\", \"Union\", \"Genesee\", \"Isanti\", \"Albany\", \"Pierce\", \"Salt Lake\", \n                  \"Sonoma\", \"Orange\", \"Boone\", \"Jefferson\", \"Mecklenburg\", \"Erie\", \"Yamhill\", \n                  \"El Dorado\", \"Shawnee\", \"Fulton\", \"Cache\", \"Fairfax,Alexandria City\", \"Yakima\", \n                  \"Clayton\", \"Seneca\", \"Newport\", \"Milwaukee,Ozaukee\", \"Somerset\", \"Contra Costa\", \n                  \"Ouachita\", \"Suffolk,Middlesex\", \"Sandusky\", \"Portsmouth City,Isle Of Wight,Chesapeake City,Suffolk City\", \n                  \"Cabell\", \"Mason\", \"Greene\", \"Lincoln\", \"Santa Barbara\", \"Dunklin\", \"Lancaster\", \n                  \"Utah\", \"Carson City\", \"Niagara\", \"Ontario\", \"Saint Tammany\", \"Grafton\", \n                  \"Columbia\", \"Dutchess\", \"Pontotoc\", \"Clark\", \"Sumter\", \"Houston\", \"Vigo\", \n                  \"Saint Mary\", \"Essex\", \"Hill\", \"Wake\", \"Lawrence\", \"Georgetown\", \"San Juan,Grand\", \n                  \"Walworth\", \"Milwaukee,Ozaukee,Racine,Waukesha,Washington\", \"Mineral\", \n                  \"Orleans\", \"Goodhue\", \"Pulaski\", \"Yuma\", \"Pima\", \"La Plata\", \"Elkhart\", \n                  \"Suffolk,Middlesex,Norfolk\", \"Kent\", \"Gratiot\", \"Rensselaer\", \"Providence\", \n                  \"De Kalb\", \"Chippewa\", \"Eaton\", \"Johnson\", \"Hardin\", \"Pender,Duplin\", \n                  \"Washington\", \"Broward\", \"Marquette\", \"Hennepin\", \"Clay\", \"Monroe\", \"Wyandot\", \n                  \"King,Snohomish\", \"Noble\", \"Suffolk\", \"Fairfield\", \"Snohomish\", \"Burlington\", \n                  \"Wyoming\", \"Philadelphia\", \"Arapahoe,Adams\", \"New Castle\", \"La Salle\", \n                  \"Scott,New Madrid\", \"Otero\", \"Schenectady\", \"Saint Lawrence\", \"Andrews\", \n                  \"Kanawha\", \"Travis\", \"Wayne\", \"Sussex\", \"Rockingham\", \"Centre\", \n                  \"Bedford City,Lynchburg City,Amherst,Bedford,Campbell\", \"Randolph\", \"Plumas\", \n                  \"Muskegon\", \"Nobles\", \"Montgomery\", \"Chelan\", \"Boulder\", \"Grand Traverse\", \n                  \"Kewaunee,Brown\", \"Madison\", \"Houghton\", \"Park\", \"Payne\", \n                  \"Hampton City,Newport News City,York,James City\", \"Richmond\", \"Macon\", \n                  \"Miami-Dade\", \"Carlton,Saint Louis\", \"Onslow\", \"Marin\", \"Acadia\", \"Aroostook\", \n                  \"Kings\", \"Stafford\", \"Hampton City,Newport News City\", \"San Luis Obispo\", \n                  \"Jackson\", \"Tompkins\", \"Livingston\", \"Webb\", \"Sangamon\", \"Bienville\", \n                  \"Preston\", \"Rock\", \"Shelby\", \"Platte,Clay\", \"Schoharie\", \"Benton\", \"Macomb\", \n                  \"Mahoning\", \"Guilford\", \"Mclennan\", \"Douglas\", \"Sheridan\", \"Jessamine\", \n                  \"Warren\", \"Laramie\", \"Catoosa,Walker,Dade,Hamilton\", \"Lee\", \"Solano\", \n                  \"Fairfax,Prince Georges,District Of Columbia,Loudoun,Montgomery,Arlington\", \n                  \"Winnebago\", \"Marshall\", \"Buncombe,Henderson\", \"Cheshire\", \"Kershaw\", \n                  \"Harris\", \"King,Pierce\", \"Isabella\", \"Stanislaus\", \"Arenac\", \"Chemung\", \n                  \"Mercer\", \"Nueces\", \"New Haven\", \"Vernon\", \"Jackson,Cass\", \"Bryan\", \n                  \"Hunterdon\", \"Lorain\", \"Cook\", \"Tippecanoe\", \"Calloway\", \"Garland\", \n                  \"Franklin\", \"Carbon\", \"Caddo\", \"York\", \"Outagamie,Winnebago,Calumet\", \n                  \"Martinsville City,Henry\", \"East Feliciana\", \"Brown\", \"Shasta\", \"Oswego\", \n                  \"Otter Tail\", \"Middlesex,Somerset,Union\", \"Oneida\", \"Lane\", \"Weber,Davis\", \n                  \"Kenosha\", \"Schoolcraft\", \"Saginaw\", \"Rockland\", \"Hawaii\", \"Tuscaloosa\", \n                  \"Le Sueur,Scott\", \"Henry\", \"Tarrant\", \"Fond Du Lac\", \"Sherburne,Benton,Stearns\", \n                  \"Juneau\", \"Warrick\", \"Lewis And Clark\", \"Josephine\", \"Victoria\", \"Iroquois\", \n                  \"Champaign\", \"Lake\", \"Boyd\", \"Nassau\", \"Deschutes\", \"Greenwood\", \"Alachua\", \n                  \"Lucas\", \"Saint Marys\", \"Napa\", \"Mcdonough\", \"Calhoun\", \"Island\", \"Bulloch\", \n                  \"Clare\", \"Lyon\", \"Henderson\", \"Washtenaw\", \"Weld\", \"Hudson\", \"Kendall\", \n                  \"Vermilion\", \"Richland\", \"Sherburne\", \"Loudoun\", \"Coconino\", \"Peoria\", \n                  \"Tulsa\", \"Klamath\", \"Muscatine\", \"Oakland,Wayne\", \"Johnson,Jackson,Cass\", \n                  \"Dent\", \"Nemaha\", \"Cabell,Wayne\", \"Windsor\", \"Concordia\", \"Saratoga\", \n                  \"Clackamas\", \"Alameda\", \"Oxford\", \"San Diego\", \"Jasper\", \"Dickinson\", \n                  \"Northampton,Halifax\", \"Strafford\", \"Morris\", \"Portage\", \"Sarasota\", \n                  \"Leon\", \"Waupaca\", \"Perry\", \"Reno\", \"Ulster\", \"Hidalgo\", \n                  \"Bedford City,Botetourt,Roanoke,Bland,Salem,Roanoke City,Bedford\", \n                  \"Lenoir\", \"Platte\", \"Licking\", \"Ada\", \"Waldo\", \"Bay\", \"Le Sueur\", \n                  \"Monongalia\", \"Emmet\", \"Newport News City,York,New Kent,Williamsburg City,James City\", \n                  \"Sweetwater\", \"Yankton\", \"Martin,Palm Beach\", \"Westmoreland\", \"Mckean\", \n                  \"Norton City,Wise\", \"Henrico\", \"Frederick,Winchester City\", \"Saint Bernard\", \n                  \"Monterey\", \"Hillsborough\", \"Floyd\", \"Cobb\", \"Carter\", \"Scott,Hennepin,Carver\", \n                  \"Essex,Union\", \"Radford,Montgomery,Pulaski\", \"La Paz\", \"Waukesha,Jefferson\", \n                  \"Saint Louis,Jefferson\", \"Duchesne\", \"Waukesha\", \"Onondaga\", \"Hampden\", \n                  \"Plaquemines\", \"Buffalo\", \"Anchorage\", \"Ventura\", \"Rowan\", \"Iberia\", \n                  \"Macomb,Wayne\", \"Forsyth,Guilford,Randolph,Davidson\", \"Polk\", \"Evangeline\", \n                  \"Dawes\", \"Olmsted\", \"Tangipahoa\", \"Tuscola\", \"Dodge,Washington\", \n                  \"Wyandotte\", \"Latah\", \"Washoe\", \"Pike\", \"Berkeley,Dorchester,Charleston\", \n                  \"Bronx\", \"Kern\", \"Arapahoe\", \"Natchitoches\", \"Anoka,Hennepin,Dakota,Ramsey,Washington\", \n                  \"Atlantic\", \"Harrisonburg City,Rockingham\", \"Peach\", \"Beauregard\", \n                  \"Scioto\", \"Utah,Salt Lake\", \"Fairfax,Alexandria City,Arlington,Falls Church City\", \n                  \"Palm Beach\", \"Rock Island\", \"Allen\", \"West Baton Rouge\", \"Christian\", \n                  \"Silver Bow\", \"Alleghany\", \"Fresno\", \"Smith\", \"Stark\", \"Dakota,Woodbury,Union\", \n                  \"Howell\", \"Luna\", \"Beaufort\", \"Uintah\", \"Russell\", \"Wapello\", \"Scotland\", \n                  \"New York\", \"Canadian\", \"Wasatch\", \"Davis\", \"Iron\", \"Kootenai\", \"Morgan\", \n                  \"Green\", \"Jo Daviess\", \"Wilson\", \"Kittitas\", \"Grant\", \"Hall\", \"Placer\", \n                  \"Paulding\", \"Johnson,Jackson,Wyandotte\", \"Carteret\", \"Westmoreland,Armstrong\", \n                  \"Spokane\", \"Garfield\", \"Oklahoma\", \"Dane\", \"Merced\", \"Bernalillo\", \n                  \"Pitkin\", \"Pitt\", \"Putnam\", \"Routt\", \"San Bernardino\", \"Manitowoc\", \n                  \"Mille Lacs\", \"Sheboygan\", \"Chittenden\", \"Newton\", \"Carroll\", \n                  \"San Francisco\", \"Thurston\", \"Mchenry\", \"Pennington\", \"Durham\", \n                  \"Mono\", \"Bartholomew\", \"Kennebec\", \"Hood River\", \"Allegan\", \n                  \"Saint Louis,Saint Charles\", \"Doddridge\", \"El Dorado,Nevada,Placer\", \n                  \"New London\", \"Manistee\", \"Saint Charles\", \"Cumberland,Oxford,Belknap,Carroll\", \n                  \"Macoupin\", \"Teton\", \"Freeborn\", \"Clinton\", \"Ashtabula\", \"San Joaquin\", \n                  \"Dubois\", \"Delta\", \"Cuyahoga\", \"San Mateo\", \"Orange,Seminole\", \n                  \"Midland\", \"Dona Ana\", \"Sandoval\", \"Lackawanna\", \"Essex,Hudson,Union,Passaic,Bergen\", \n                  \"Blue Earth\", \"Norfolk City\", \"Worcester\", \"Ventura,Los Angeles\", \n                  \"Whiteside\", \"Dodge\", \"Effingham\", \"Colbert\", \"Bureau\", \"Kanabec\", \n                  \"Yates\", \"Galveston\", \"Sutter\", \"Stanton,Madison\", \"Marlboro\", \n                  \"Box Elder\", \"Iosco\", \"Lewis\", \"Chenango\", \"Lexington\", \"Volusia\", \n                  \"Kandiyohi\", \"Trumbull\", \"Whitman\", \"Muscogee,Chattahoochee\", \n                  \"Blue Earth,Nicollet\", \"Laclede\", \"Cherokee\", \"Frederick\", \n                  \"Orange,Pinellas\", \"Cortland\", \"Hocking\", \"Garrett\", \"San Benito\", \n                  \"Defiance\", \"Wichita\", \"Holt\", \"Wasco\", \"Tillamook\", \"Wood\", \n                  \"Fayette\", \"Linn\", \"Passaic\", \"Alamosa\", \"Audrain\", \"Broome\", \n                  \"Schuyler\", \"Henrico,Richmond City,Goochland\", \"Lehigh\", \"Barry\", \n                  \"Tioga\", \"Herkimer\", \"Mesa\", \"Dakota\", \"Anderson\", \"Albemarle,Charlottesville City\", \n                  \"Lafourche\", \"Bossier\", \"Lenawee\", \"Clarke\", \"Crawford\", \"Portage,Summit\", \n                  \"Piscataquis\", \"Del Norte\", \"Buchanan\", \"Mcdowell\", \"Chester\", \n                  \"Cumberland,Prince Edward\", \"Sagadahoc\", \"Stephenson\", \"Red River\", \n                  \"Horry,Columbus\", \"Kankakee\", \"Kaufman,Collin,Dallas\", \"Belmont\", \n                  \"Gregg\", \"Marathon,Wood\", \"Athens\", \"Taylor\", \"Scott,Cape Girardeau\", \n                  \"Gogebic\", \"Greene,Christian\", \"Maui\", \"Saint Louis,Saint Louis City\", \n                  \"Richland,Lexington\", \"Harrison\", \"Chisago\", \"Barry,Lawrence\", \n                  \"Northampton,Bucks\", \"Yolo\", \"Petersburg City\", \"Williams\", \n                  \"Tooele\", \"Santa Cruz,Monterey\", \"Gallatin\", \"Hampton City,Newport News City,York,Gloucester,Mathews,Poquoson City\", \n                  \"Middlesex\", \"Humboldt\", \"Rice,Dakota\", \"Stafford,Prince William\", \n                  \"Santa Fe\", \"Scott,Hennepin,Dakota\", \"Scotts Bluff\", \"Steuben\", \n                  \"Saline\", \"Androscoggin\", \"Pueblo\", \"Tuscarawas\", \"Saint Croix,Pierce\", \n                  \"Nevada\", \"Mackinac\", \"Brazos\", \"Saint Croix\", \"Swain,Jackson\", \n                  \"Worcester,Providence,Norfolk\", \"Walla Walla\", \"Medina\", \"Sacramento\", \n                  \"Barbour\", \"Fairbanks North Star\", \"Escambia\", \"Watauga\", \"Denver\", \n                  \"Scott\", \"Sarpy,Douglas\", \"Cleveland\", \"Prince William,Fairfax\", \n                  \"Coshocton\", \"Randall,Potter\", \"Muskingum\", \"Clatsop\", \"Chippewa,Eau Claire\", \n                  \"Suffolk,Middlesex,Worcester,Plymouth,Norfolk\", \"Cattaraugus\", \n                  \"San Miguel\", \"Kenton\", \"Essex,Hudson,Passaic,Bergen\", \"Darke\", \n                  \"Pleasants\", \"Will\", \"Howard\", \"Seminole\", \"Coos\", \"Mercer,Trumbull\", \n                  \"Saint Johns\", \"Gwinnett\", \"Oakland,Macomb\", \"Chaves\", \"Hays\", \n                  \"Beadle\", \"Baraga\", \"Camden\", \"Malheur\", \"Indiana\", \"Morrison\", \n                  \"Prince William,Fairfax,Fauquier,Loudoun,Fairfax City,Manassas City,Manassas Park City\", \n                  \"Kosciusko\", \"Calcasieu\", \"Hancock\", \"Pickaway\", \"Adams\", \"Woodward\", \n                  \"Cayuga\", \"Preble\", \"Dougherty\", \"Webster\", \"Menominee\", \"Ashland\", \n                  \"Marinette\", \"Cass\", \"Virginia Beach City,Norfolk City,Portsmouth City,Chesapeake City\", \n                  \"Dodge,Jefferson\", \"Cooke\")\n\n# Randomly sample county names\ncounty_names &lt;- sample(county_names, size = n_rows, replace = TRUE)\n\n# 5. population_served (numeric variable)\n# Use a log-normal distribution to better match the original data's skewness\npopulation_served &lt;- round(rlnorm(n_rows, meanlog = log(128734), sdlog = 1))  # Adjust parameters as needed\npopulation_served &lt;- pmax(population_served, 564)  # Ensure minimum value is 564\n\n# 6. date_start and date_end (date variables)\n# Use a uniform distribution for dates within the original range\ndate_start &lt;- seq(as.Date(\"2023-01-01\"), as.Date(\"2024-12-31\"), by = \"day\")\ndate_start &lt;- sample(date_start, size = n_rows, replace = TRUE)\ndate_end &lt;- date_start + sample(1:14, size = n_rows, replace = TRUE)  # Assuming 1-14 days difference\n\n# 7. ptc_15d (numeric variable)\n# Use a mixture distribution to account for the extreme values\nptc_15d &lt;- ifelse(runif(n_rows) &lt; 0.95, \n                  rnorm(n_rows, mean = 0, sd = 100),  # Most values are around 0\n                  runif(n_rows, min = 1e6, max = 2147483647))  # Some extreme values\nptc_15d &lt;- pmax(ptc_15d, -100)  # Ensure minimum value is -100\n\n# 8. detect_prop_15d (numeric variable)\n# Use a discrete distribution to match the original data's values\ndetect_prop_15d &lt;- sample(c(0, 50, 67, 80, 100), size = n_rows, replace = TRUE, \n                          prob = c(0.1, 0.2, 0.2, 0.2, 0.3))  # Adjust probabilities as needed\n\n# 9. percentile (numeric variable)\n# Use a uniform distribution between 0 and 100\npercentile &lt;- runif(n_rows, min = 0, max = 100)\n\n# 10. sampling_prior (categorical variable)\n# Use the same proportions as in the original data\nsampling_prior &lt;- sample(c(\"no\", \"yes\"), size = n_rows, replace = TRUE, prob = c(0.85, 0.15))  # Adjust probabilities as needed\n\n# 11. first_sample_date (date variable)\n# Use a uniform distribution for dates within the original range\nfirst_sample_date &lt;- seq(as.Date(\"2020-07-05\"), as.Date(\"2025-01-14\"), by = \"day\")\nfirst_sample_date &lt;- sample(first_sample_date, size = n_rows, replace = TRUE)\n\n# Combine all variables into a synthetic dataset\nsynthetic_data &lt;- data.frame(\n  wwtp_jurisdiction,\n  wwtp_id,\n  sample_location,\n  county_names,\n  population_served,\n  date_start,\n  date_end,\n  ptc_15d,\n  detect_prop_15d,\n  percentile,\n  sampling_prior,\n  first_sample_date\n)\n\n# View the first few rows of the synthetic dataset\nhead(synthetic_data)\n\n  wwtp_jurisdiction wwtp_id sample_location  county_names population_served\n1            Kansas    2453 Treatment plant   Saint Johns            128212\n2      South Dakota    2483 Treatment plant Saint Bernard            260611\n3          Michigan    1049 Treatment plant          Coos             33282\n4          Virginia    1907 Treatment plant       Garrett            213790\n5         Wisconsin      89 Treatment plant       Tuscola            487343\n6          Arkansas    2003 Treatment plant       Wichita            125338\n  date_start   date_end    ptc_15d detect_prop_15d percentile sampling_prior\n1 2024-06-19 2024-06-26 -100.00000             100   77.13513             no\n2 2023-10-25 2023-11-07   79.20716             100   46.79306             no\n3 2024-08-21 2024-08-29  267.93114             100   74.93197             no\n4 2024-05-25 2024-05-26   34.21115              50   50.04359             no\n5 2023-08-03 2023-08-16   16.10619              80   76.92259             no\n6 2023-08-16 2023-08-18  -27.87697              80   89.09138             no\n  first_sample_date\n1        2024-04-15\n2        2023-08-27\n3        2023-03-15\n4        2024-01-22\n5        2021-11-08\n6        2023-04-26\n\n# Check the distributions of key variables\nsummary(synthetic_data$population_served)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n     824    65550   128757   212086   252673 12371955 \n\nsummary(synthetic_data$ptc_15d)\n\n      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \n      -100        -63          7   53682978         81 2147473613 \n\nsummary(synthetic_data$detect_prop_15d)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00   50.00   80.00   69.44  100.00  100.00 \n\nsummary(synthetic_data$percentile)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n  0.00046  24.93337  49.94348  49.95986  74.94543  99.99988 \n\n\n\nggplot(synthetic_data, aes(x = population_served)) +\n  geom_histogram(binwidth = 1000, fill = \"blue\", color = \"blue\") +\n  labs(title = \"Distribution of Population Served\", x = \"Population Served\", y = \"Frequency\")\n\n\n\n\n\n\n\nggplot(synthetic_data, aes(x = ptc_15d)) +\n  geom_histogram(fill = \"green\", color = \"green\") +\n  xlim(-100, 100) +\n  labs(title = \"Distribution of PTC 15d\", x = \"PTC 15d\", y = \"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 155542 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\nggplot(synthetic_data, aes(x = detect_prop_15d)) +\n  geom_histogram(binwidth = 10, fill = \"red\", color = \"red\") +\n  labs(title = \"Distribution of Detect Prop 15d\", x = \"Detect Prop 15d\", y = \"Frequency\")\n\n\n\n\n\n\n\nggplot(synthetic_data, aes(x = percentile)) +\n  geom_histogram(binwidth = 5, fill = \"purple\", color = \"purple\") +\n  labs(title = \"Distribution of Percentile\", x = \"Percentile\", y = \"Frequency\")\n\n\n\n\n\n\n\n\n\n#Plot distributions of categorical variables\nggplot(df, aes(x = wwtp_jurisdiction)) +\n  geom_bar(fill = \"blue\") +\n  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +\n  labs(title = \"Distribution of wwtp_jurisdiction\",\n       x = \"wwtp_jurisdiction\",\n       y = \"count\")\n\n\n\n\n\n\n\nggplot(filter(df, wwtp_jurisdiction == \"Georgia\"), aes(x = county_names)) +\n  geom_bar(fill = \"green\") +\n  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +\n  labs(title = \"Distribution of counties in Georgia\",\n       x = \"counties\",\n       y = \"count\")\n\n\n\n\n\n\n\nggplot(df, aes(x = sample_location)) +\n  geom_bar(fill = \"red\") +\n  theme(axis.text.x = element_text(angle = 0.5, hjust = 0.5)) +\n  labs(title = \"Distribution of sample_location\",\n       x = \"sample location\",\n       y = \"count\")\n\n\n\n\n\n\n\nggplot(df, aes(x = sampling_prior)) +\n  geom_bar(fill = \"purple\") +\n  theme(axis.text.x = element_text(angle = 0.5, hjust = 0.5)) +\n  labs(title = \"Distribution of sampling_prior\",\n       x = \"prior sampling\",\n       y = \"count\")"
  },
  {
    "objectID": "cdc-data-exercise/cdc-data-exercise.html#part-1",
    "href": "cdc-data-exercise/cdc-data-exercise.html#part-1",
    "title": "CDC Data Exercise",
    "section": "",
    "text": "The data that I have chosen for this exercise is the NWSS Public SARS-CoV-2 Wastewater Metric Data from the CDC. The link to the website can be found here. ChatGPT and GitHub Copilot were both used to fine-tune the code below.\n\n#Load required packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at /Users/cgnorris/Documents/GitHub/MADA (EPID 8060E)/connornorris-MADA-portfolio\n\nlibrary(ggplot2)\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\n\nThis dataset was initially too big to fit within Git’s file size guidelines. To reduce file size, I cut some redundant variables as well as restricted the data to only include observations from 2023 and 2024.\n\n#Code to reduce data to acceptable size for Git\n#data_path &lt;- here(\"cdc-data-exercise\", \"NWSS_Public_SARS-CoV-2_Wastewater_Metric_Data_20250206.csv\")\n#ww &lt;- read_csv(data_path)\n#ww &lt;- ww %&gt;%\n  #select(!c(key_plot_id, sample_location_specify, reporting_jurisdiction)) %&gt;% \n  #filter(date_start &gt;= as.Date(\"2023-01-01\") & date_start &lt;= as.Date(\"2024-12-31\"))\n#write.csv(ww, 'wastewater.csv', row.names = FALSE)\n\n\n#Load dataset\ndata_path &lt;- here(\"cdc-data-exercise\", \"wastewater.csv\")\ndf &lt;- read_csv(data_path)\n\nRows: 773779 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): wwtp_jurisdiction, sample_location, county_names, county_fips, sam...\ndbl  (5): wwtp_id, population_served, ptc_15d, detect_prop_15d, percentile\ndate (3): date_start, date_end, first_sample_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nskimr::skim(df)\n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n773779\n\n\nNumber of columns\n13\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n5\n\n\nDate\n3\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nwwtp_jurisdiction\n0\n1\n4\n20\n0\n51\n0\n\n\nsample_location\n0\n1\n15\n22\n0\n2\n0\n\n\ncounty_names\n0\n1\n3\n85\n0\n655\n0\n\n\ncounty_fips\n0\n1\n5\n41\n0\n830\n0\n\n\nsampling_prior\n0\n1\n2\n3\n0\n2\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\ndate_start\n0\n1\n2023-01-01\n2024-12-31\n2024-03-07\n731\n\n\ndate_end\n0\n1\n2023-01-15\n2025-01-14\n2024-03-21\n731\n\n\nfirst_sample_date\n0\n1\n2020-07-05\n2025-01-14\n2022-10-18\n507\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nwwtp_id\n0\n1.00\n1437.05\n849.52\n1\n692\n1453.00\n2203.00\n2939\n▆▇▃▇▆\n\n\npopulation_served\n0\n1.00\n128733.79\n339018.76\n564\n11326\n32000.00\n95000.00\n4000000\n▇▁▁▁▁\n\n\nptc_15d\n62649\n0.92\n3375204.78\n80730338.99\n-100\n-50\n0.00\n97.00\n2147483647\n▇▁▁▁▁\n\n\ndetect_prop_15d\n40325\n0.95\n80.88\n33.27\n0\n75\n100.00\n100.00\n100\n▁▁▁▁▇\n\n\npercentile\n30171\n0.96\n48.29\n41.82\n0\n28\n46.29\n65.75\n999\n▇▁▁▁▁\n\n\n\n\n\n\n#Data cleaning\n\n#Convert dates from characters to Date type\ndf &lt;- df %&gt;%\n  mutate(\n    date_start = as.Date(date_start, format=\"%Y-%m-%d\"),\n    date_end = as.Date(date_end, format=\"%Y-%m-%d\"),\n    first_sample_date = as.Date(first_sample_date, format=\"%Y-%m-%d\")\n  )\n\n#Handle missing values\ndf &lt;- df %&gt;%\n  replace_na(list(ptc_15d = 0, detect_prop_15d = 0, percentile = 0))\n\n#Convert categorical variables to factors\ndf &lt;- df %&gt;%\n  mutate(\n    wwtp_jurisdiction = as.factor(wwtp_jurisdiction),\n    sample_location = as.factor(sample_location),\n    county_names = as.factor(county_names),\n    county_fips = as.factor(county_fips),\n    sampling_prior = as.factor(sampling_prior)\n  )\n\nstr(df)\n\ntibble [773,779 × 13] (S3: tbl_df/tbl/data.frame)\n $ wwtp_jurisdiction: Factor w/ 51 levels \"Alabama\",\"Alaska\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ wwtp_id          : num [1:773779] 1212 1212 1212 1212 1212 ...\n $ sample_location  : Factor w/ 2 levels \"Before treatment plant\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ county_names     : Factor w/ 655 levels \"Acadia\",\"Ada\",..: 337 337 337 337 337 337 337 337 337 337 ...\n $ county_fips      : Factor w/ 830 levels \"01005\",\"01015\",..: 20 20 20 20 20 20 20 20 20 20 ...\n $ population_served: num [1:773779] 12542 12542 12542 12542 12542 ...\n $ date_start       : Date[1:773779], format: \"2024-10-22\" \"2024-10-24\" ...\n $ date_end         : Date[1:773779], format: \"2024-11-05\" \"2024-11-07\" ...\n $ ptc_15d          : num [1:773779] 2545 13166 13406 -72 -44 ...\n $ detect_prop_15d  : num [1:773779] 67 67 80 100 100 100 100 80 80 80 ...\n $ percentile       : num [1:773779] 34.5 45.3 65 96.5 95.8 ...\n $ sampling_prior   : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ first_sample_date: Date[1:773779], format: \"2023-12-19\" \"2023-12-19\" ...\n\nsummary(df)\n\n  wwtp_jurisdiction     wwtp_id                   sample_location  \n New York  :104182   Min.   :   1   Before treatment plant: 35345  \n Michigan  : 64246   1st Qu.: 692   Treatment plant       :738434  \n Illinois  : 59311   Median :1453                                  \n California: 50130   Mean   :1437                                  \n Ohio      : 35977   3rd Qu.:2203                                  \n Missouri  : 35833   Max.   :2939                                  \n (Other)   :424100                                                 \n     county_names     county_fips     population_served   date_start        \n Jefferson : 12793   17031  : 10635   Min.   :    564   Min.   :2023-01-01  \n Orange    : 12781   26139  :  9866   1st Qu.:  11326   1st Qu.:2023-09-08  \n Washington: 10836   26077  :  8041   Median :  32000   Median :2024-03-07  \n Cook      : 10635   17043  :  7310   Mean   : 128734   Mean   :2024-02-17  \n Ottawa    : 10352   36071  :  6478   3rd Qu.:  95000   3rd Qu.:2024-08-13  \n Kalamazoo :  8041   26081  :  5848   Max.   :4000000   Max.   :2024-12-31  \n (Other)   :708341   (Other):725601                                         \n    date_end             ptc_15d           detect_prop_15d    percentile    \n Min.   :2023-01-15   Min.   :      -100   Min.   :  0.00   Min.   :  0.00  \n 1st Qu.:2023-09-22   1st Qu.:       -44   1st Qu.: 50.00   1st Qu.: 25.67  \n Median :2024-03-21   Median :         0   Median :100.00   Median : 44.80  \n Mean   :2024-03-02   Mean   :   3101931   Mean   : 76.67   Mean   : 46.40  \n 3rd Qu.:2024-08-27   3rd Qu.:        75   3rd Qu.:100.00   3rd Qu.: 65.00  \n Max.   :2025-01-14   Max.   :2147483647   Max.   :100.00   Max.   :999.00  \n                                                                            \n sampling_prior first_sample_date   \n no :676556     Min.   :2020-07-05  \n yes: 97223     1st Qu.:2022-02-16  \n                Median :2022-10-18  \n                Mean   :2022-10-10  \n                3rd Qu.:2023-06-26  \n                Max.   :2025-01-14  \n                                    \n\n\n\n#Exploratory Data Analysis\n\n#Names of categorical variables\ncategorical_vars &lt;- c(\"wwtp_jurisdiction\", \"sample_location\", \"county_names\", \"county_fips\", \"sampling_prior\")\n\n#Summary of categorical variables\nfor (var in categorical_vars) {\n  summary_table &lt;- df %&gt;%\n    count(!!sym(var)) %&gt;%\n    mutate(Percent = n / sum(n) * 100)\n  \n  write_csv(summary_table, paste0(var, \"_summary.csv\"))\n}\n\n#Names of continuous variables\ncontinuous_vars &lt;- df %&gt;%\n  select(population_served, ptc_15d, detect_prop_15d, percentile)\n\nfor (var in names(continuous_vars)) {\n  summary_table &lt;- continuous_vars %&gt;%\n    summarise(\n      Variable = var,\n      Mean = mean(.data[[var]], na.rm = TRUE),\n      SD = sd(.data[[var]], na.rm = TRUE),\n      Min = min(.data[[var]], na.rm = TRUE),\n      Max = max(.data[[var]], na.rm = TRUE),\n      Median = median(.data[[var]], na.rm = TRUE)\n    )\n  write_csv(summary_table, paste0(var, \"_summary.csv\"))\n}\n\ndetect_prop_15d_summary &lt;- read_csv(\"detect_prop_15d_summary.csv\")\n\nRows: 1 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Variable\ndbl (5): Mean, SD, Min, Max, Median\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npercentile_summary &lt;- read_csv(\"percentile_summary.csv\")\n\nRows: 1 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Variable\ndbl (5): Mean, SD, Min, Max, Median\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npopulation_served_summary &lt;- read_csv(\"population_served_summary.csv\")\n\nRows: 1 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Variable\ndbl (5): Mean, SD, Min, Max, Median\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nptc_15d_summary &lt;- read_csv(\"ptc_15d_summary.csv\")\n\nRows: 1 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Variable\ndbl (5): Mean, SD, Min, Max, Median\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncontinuous_summary &lt;- rbind(detect_prop_15d_summary, percentile_summary, population_served_summary, ptc_15d_summary)\n\nwrite_csv(continuous_summary, \"continuous_summary.csv\")\n\nsummary(df$ptc_15d)\n\n      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \n      -100        -44          0    3101931         75 2147483647 \n\n\n\n#Plot distributions of continuous variables\nggplot(df, aes(x = population_served)) +\n  geom_histogram(binwidth = 1000, fill = \"blue\", color = \"blue\") +\n  labs(title = \"Distribution of Population Served\", x = \"Population Served\", y = \"Frequency\")\n\n\n\n\n\n\n\nggplot(df, aes(x = ptc_15d)) +\n  geom_histogram(fill = \"green\", color = \"green\") +\n  xlim(-100, 100) +\n  labs(title = \"Distribution of PTC 15d\", x = \"PTC 15d\", y = \"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 175282 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\nggplot(df, aes(x = detect_prop_15d)) +\n  geom_histogram(binwidth = 10, fill = \"red\", color = \"red\") +\n  labs(title = \"Distribution of Detect Prop 15d\", x = \"Detect Prop 15d\", y = \"Frequency\")\n\n\n\n\n\n\n\nggplot(df, aes(x = percentile)) +\n  geom_histogram(binwidth = 5, fill = \"purple\", color = \"purple\") +\n  labs(title = \"Distribution of Percentile\", x = \"Percentile\", y = \"Frequency\")\n\n\n\n\n\n\n\n#Save plots\nggsave(\"population_served_distribution.png\")\n\nSaving 7 x 5 in image\n\nggsave(\"ptc_15d_distribution.png\")\n\nSaving 7 x 5 in image\n\nggsave(\"detect_prop_15d_distribution.png\")\n\nSaving 7 x 5 in image\n\nggsave(\"percentile_distribution.png\")\n\nSaving 7 x 5 in image\n\n#Plot distributions of categorical variables\nggplot(df, aes(x = wwtp_jurisdiction)) +\n  geom_bar(fill = \"blue\") +\n  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +\n  labs(title = \"Distribution of wwtp_jurisdiction\",\n       x = \"wwtp_jurisdiction\",\n       y = \"count\")\n\n\n\n\n\n\n\nggplot(filter(df, wwtp_jurisdiction == \"Georgia\"), aes(x = county_names)) +\n  geom_bar(fill = \"green\") +\n  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +\n  labs(title = \"Distribution of counties in Georgia\",\n       x = \"counties\",\n       y = \"count\")\n\n\n\n\n\n\n\nggplot(df, aes(x = sample_location)) +\n  geom_bar(fill = \"red\") +\n  theme(axis.text.x = element_text(angle = 0.5, hjust = 0.5)) +\n  labs(title = \"Distribution of sample_location\",\n       x = \"sample location\",\n       y = \"count\")\n\n\n\n\n\n\n\nggplot(df, aes(x = sampling_prior)) +\n  geom_bar(fill = \"purple\") +\n  theme(axis.text.x = element_text(angle = 0.5, hjust = 0.5)) +\n  labs(title = \"Distribution of sampling_prior\",\n       x = \"prior sampling\",\n       y = \"count\")\n\n\n\n\n\n\n\nggsave(\"jurisdiction_distribution.png\")\n\nSaving 7 x 5 in image\n\nggsave(\"georgia_counties_distribution.png\")\n\nSaving 7 x 5 in image\n\nggsave(\"sample_location_distribution.png\") \n\nSaving 7 x 5 in image\n\nggsave(\"sampling_prior_distribution.png\")\n\nSaving 7 x 5 in image\n\n\n\n\n\n#some additional exploration\n\nunique(df$wwtp_jurisdiction)\n\n [1] Arizona              Michigan             Texas               \n [4] Pennsylvania         Maine                New Jersey          \n [7] New York             Oklahoma             Virginia            \n[10] Wisconsin            Colorado             New York City       \n[13] Utah                 Washington           West Virginia       \n[16] California           Illinois             New Hampshire       \n[19] Florida              Hawaii               Maryland            \n[22] Missouri             Oregon               Louisiana           \n[25] Vermont              Indiana              North Carolina      \n[28] Rhode Island         Ohio                 South Carolina      \n[31] Minnesota            Kansas               Georgia             \n[34] Massachusetts        Arkansas             Nebraska            \n[37] Nevada               Alabama              Montana             \n[40] Connecticut          Delaware             New Mexico          \n[43] Kentucky             Wyoming              Tennessee           \n[46] District of Columbia Iowa                 Alaska              \n[49] Idaho                South Dakota         Mississippi         \n51 Levels: Alabama Alaska Arizona Arkansas California Colorado ... Wyoming\n\n\n\nunique(df$county_names)\n\n  [1] Maricopa                                                                             \n  [2] Ottawa                                                                               \n  [3] Dallas                                                                               \n  [4] Kalamazoo                                                                            \n  [5] Butler                                                                               \n  [6] Penobscot                                                                            \n  [7] Bergen                                                                               \n  [8] Cumberland                                                                           \n  [9] Delaware                                                                             \n [10] Muskogee                                                                             \n [11] Virginia Beach City,Chesapeake City                                                  \n [12] Marathon                                                                             \n [13] Summit                                                                               \n [14] Queens                                                                               \n [15] Dauphin                                                                              \n [16] Tazewell                                                                             \n [17] Island,Snohomish                                                                     \n [18] Ohio                                                                                 \n [19] Santa Cruz                                                                           \n [20] Saint Clair                                                                          \n [21] Sullivan                                                                             \n [22] Chautauqua                                                                           \n [23] Pinellas                                                                             \n [24] Honolulu                                                                             \n [25] Du Page                                                                              \n [26] Anne Arundel                                                                         \n [27] Salem                                                                                \n [28] Riverside                                                                            \n [29] Cole,Callaway                                                                        \n [30] Westchester                                                                          \n [31] Multnomah                                                                            \n [32] Umatilla                                                                             \n [33] La Crosse                                                                            \n [34] Dunn                                                                                 \n [35] Lafayette                                                                            \n [36] Addison                                                                              \n [37] Mohave                                                                               \n [38] Marion                                                                               \n [39] Otsego                                                                               \n [40] Durham,Orange                                                                        \n [41] New Hanover                                                                          \n [42] Forsyth                                                                              \n [43] Allegheny                                                                            \n [44] Los Angeles                                                                          \n [45] San Francisco,San Mateo                                                              \n [46] Monmouth                                                                             \n [47] Bristol                                                                              \n [48] Butte                                                                                \n [49] St Joseph                                                                            \n [50] Vanderburgh                                                                          \n [51] Huron                                                                                \n [52] Skagit                                                                               \n [53] Santa Clara                                                                          \n [54] Hamilton                                                                             \n [55] Saint Louis                                                                          \n [56] Knox                                                                                 \n [57] Hampton                                                                              \n [58] Horry                                                                                \n [59] El Paso                                                                              \n [60] Kane                                                                                 \n [61] Terrebonne                                                                           \n [62] Union                                                                                \n [63] Genesee                                                                              \n [64] Isanti                                                                               \n [65] Albany                                                                               \n [66] Pierce                                                                               \n [67] Salt Lake                                                                            \n [68] Sonoma                                                                               \n [69] Orange                                                                               \n [70] Boone                                                                                \n [71] Jefferson                                                                            \n [72] Mecklenburg                                                                          \n [73] Erie                                                                                 \n [74] Yamhill                                                                              \n [75] El Dorado                                                                            \n [76] Shawnee                                                                              \n [77] Fulton                                                                               \n [78] Cache                                                                                \n [79] Fairfax,Alexandria City                                                              \n [80] Yakima                                                                               \n [81] Clayton                                                                              \n [82] Seneca                                                                               \n [83] Newport                                                                              \n [84] Milwaukee,Ozaukee                                                                    \n [85] Somerset                                                                             \n [86] Contra Costa                                                                         \n [87] Ouachita                                                                             \n [88] Suffolk,Middlesex                                                                    \n [89] Sandusky                                                                             \n [90] Portsmouth City,Isle Of Wight,Chesapeake City,Suffolk City                           \n [91] Cabell                                                                               \n [92] Mason                                                                                \n [93] Greene                                                                               \n [94] Lincoln                                                                              \n [95] Santa Barbara                                                                        \n [96] Dunklin                                                                              \n [97] Lancaster                                                                            \n [98] Utah                                                                                 \n [99] Carson City                                                                          \n[100] Niagara                                                                              \n[101] Ontario                                                                              \n[102] Saint Tammany                                                                        \n[103] Grafton                                                                              \n[104] Columbia                                                                             \n[105] Dutchess                                                                             \n[106] Pontotoc                                                                             \n[107] Clark                                                                                \n[108] Sumter                                                                               \n[109] Houston                                                                              \n[110] Vigo                                                                                 \n[111] Saint Mary                                                                           \n[112] Essex                                                                                \n[113] Hill                                                                                 \n[114] Wake                                                                                 \n[115] Lawrence                                                                             \n[116] Georgetown                                                                           \n[117] San Juan,Grand                                                                       \n[118] Walworth                                                                             \n[119] Milwaukee,Ozaukee,Racine,Waukesha,Washington                                         \n[120] Mineral                                                                              \n[121] Orleans                                                                              \n[122] Goodhue                                                                              \n[123] Pulaski                                                                              \n[124] Yuma                                                                                 \n[125] Pima                                                                                 \n[126] La Plata                                                                             \n[127] Elkhart                                                                              \n[128] Suffolk,Middlesex,Norfolk                                                            \n[129] Kent                                                                                 \n[130] Gratiot                                                                              \n[131] Rensselaer                                                                           \n[132] Providence                                                                           \n[133] De Kalb                                                                              \n[134] Chippewa                                                                             \n[135] Eaton                                                                                \n[136] Johnson                                                                              \n[137] Hardin                                                                               \n[138] Pender,Duplin                                                                        \n[139] Washington                                                                           \n[140] Broward                                                                              \n[141] Marquette                                                                            \n[142] Hennepin                                                                             \n[143] Clay                                                                                 \n[144] Monroe                                                                               \n[145] Wyandot                                                                              \n[146] King,Snohomish                                                                       \n[147] Noble                                                                                \n[148] Suffolk                                                                              \n[149] Fairfield                                                                            \n[150] Snohomish                                                                            \n[151] Burlington                                                                           \n[152] Wyoming                                                                              \n[153] Philadelphia                                                                         \n[154] Arapahoe,Adams                                                                       \n[155] New Castle                                                                           \n[156] La Salle                                                                             \n[157] Scott,New Madrid                                                                     \n[158] Otero                                                                                \n[159] Schenectady                                                                          \n[160] Saint Lawrence                                                                       \n[161] Andrews                                                                              \n[162] Kanawha                                                                              \n[163] Travis                                                                               \n[164] Wayne                                                                                \n[165] Sussex                                                                               \n[166] Rockingham                                                                           \n[167] Centre                                                                               \n[168] Bedford City,Lynchburg City,Amherst,Bedford,Campbell                                 \n[169] Randolph                                                                             \n[170] Plumas                                                                               \n[171] Muskegon                                                                             \n[172] Nobles                                                                               \n[173] Montgomery                                                                           \n[174] Chelan                                                                               \n[175] Boulder                                                                              \n[176] Grand Traverse                                                                       \n[177] Kewaunee,Brown                                                                       \n[178] Madison                                                                              \n[179] Houghton                                                                             \n[180] Park                                                                                 \n[181] Payne                                                                                \n[182] Hampton City,Newport News City,York,James City                                       \n[183] Richmond                                                                             \n[184] Macon                                                                                \n[185] Miami-Dade                                                                           \n[186] Carlton,Saint Louis                                                                  \n[187] Onslow                                                                               \n[188] Marin                                                                                \n[189] Acadia                                                                               \n[190] Aroostook                                                                            \n[191] Kings                                                                                \n[192] Stafford                                                                             \n[193] Hampton City,Newport News City                                                       \n[194] San Luis Obispo                                                                      \n[195] Jackson                                                                              \n[196] Tompkins                                                                             \n[197] Livingston                                                                           \n[198] Webb                                                                                 \n[199] Sangamon                                                                             \n[200] Bienville                                                                            \n[201] Preston                                                                              \n[202] Rock                                                                                 \n[203] Shelby                                                                               \n[204] Platte,Clay                                                                          \n[205] Schoharie                                                                            \n[206] Benton                                                                               \n[207] Macomb                                                                               \n[208] Mahoning                                                                             \n[209] Guilford                                                                             \n[210] Mclennan                                                                             \n[211] Douglas                                                                              \n[212] Sheridan                                                                             \n[213] Jessamine                                                                            \n[214] Warren                                                                               \n[215] Laramie                                                                              \n[216] Catoosa,Walker,Dade,Hamilton                                                         \n[217] Lee                                                                                  \n[218] Solano                                                                               \n[219] Fairfax,Prince Georges,District Of Columbia,Loudoun,Montgomery,Arlington             \n[220] Winnebago                                                                            \n[221] Marshall                                                                             \n[222] Buncombe,Henderson                                                                   \n[223] Cheshire                                                                             \n[224] Kershaw                                                                              \n[225] Harris                                                                               \n[226] King,Pierce                                                                          \n[227] Isabella                                                                             \n[228] Stanislaus                                                                           \n[229] Arenac                                                                               \n[230] Chemung                                                                              \n[231] Mercer                                                                               \n[232] Nueces                                                                               \n[233] New Haven                                                                            \n[234] Vernon                                                                               \n[235] Jackson,Cass                                                                         \n[236] Bryan                                                                                \n[237] Hunterdon                                                                            \n[238] Lorain                                                                               \n[239] Cook                                                                                 \n[240] Tippecanoe                                                                           \n[241] Calloway                                                                             \n[242] Garland                                                                              \n[243] Franklin                                                                             \n[244] Carbon                                                                               \n[245] Caddo                                                                                \n[246] York                                                                                 \n[247] Outagamie,Winnebago,Calumet                                                          \n[248] Martinsville City,Henry                                                              \n[249] East Feliciana                                                                       \n[250] Brown                                                                                \n[251] Shasta                                                                               \n[252] Oswego                                                                               \n[253] Otter Tail                                                                           \n[254] Middlesex,Somerset,Union                                                             \n[255] Oneida                                                                               \n[256] Lane                                                                                 \n[257] Weber,Davis                                                                          \n[258] Kenosha                                                                              \n[259] Schoolcraft                                                                          \n[260] Saginaw                                                                              \n[261] Rockland                                                                             \n[262] Hawaii                                                                               \n[263] Tuscaloosa                                                                           \n[264] Le Sueur,Scott                                                                       \n[265] Henry                                                                                \n[266] Tarrant                                                                              \n[267] Fond Du Lac                                                                          \n[268] Sherburne,Benton,Stearns                                                             \n[269] Juneau                                                                               \n[270] Warrick                                                                              \n[271] Lewis And Clark                                                                      \n[272] Josephine                                                                            \n[273] Victoria                                                                             \n[274] Iroquois                                                                             \n[275] Champaign                                                                            \n[276] Lake                                                                                 \n[277] Boyd                                                                                 \n[278] Nassau                                                                               \n[279] Deschutes                                                                            \n[280] Greenwood                                                                            \n[281] Alachua                                                                              \n[282] Lucas                                                                                \n[283] Saint Marys                                                                          \n[284] Napa                                                                                 \n[285] Mcdonough                                                                            \n[286] Calhoun                                                                              \n[287] Island                                                                               \n[288] Bulloch                                                                              \n[289] Clare                                                                                \n[290] Lyon                                                                                 \n[291] Henderson                                                                            \n[292] Washtenaw                                                                            \n[293] Weld                                                                                 \n[294] Hudson                                                                               \n[295] Kendall                                                                              \n[296] Vermilion                                                                            \n[297] Richland                                                                             \n[298] Sherburne                                                                            \n[299] Loudoun                                                                              \n[300] Coconino                                                                             \n[301] Peoria                                                                               \n[302] Tulsa                                                                                \n[303] Klamath                                                                              \n[304] Muscatine                                                                            \n[305] Oakland,Wayne                                                                        \n[306] Johnson,Jackson,Cass                                                                 \n[307] Dent                                                                                 \n[308] Nemaha                                                                               \n[309] Cabell,Wayne                                                                         \n[310] Windsor                                                                              \n[311] Concordia                                                                            \n[312] Saratoga                                                                             \n[313] Clackamas                                                                            \n[314] Alameda                                                                              \n[315] Oxford                                                                               \n[316] San Diego                                                                            \n[317] Jasper                                                                               \n[318] Dickinson                                                                            \n[319] Northampton,Halifax                                                                  \n[320] Strafford                                                                            \n[321] Morris                                                                               \n[322] Portage                                                                              \n[323] Sarasota                                                                             \n[324] Leon                                                                                 \n[325] Waupaca                                                                              \n[326] Perry                                                                                \n[327] Reno                                                                                 \n[328] Ulster                                                                               \n[329] Hidalgo                                                                              \n[330] Bedford City,Botetourt,Roanoke,Bland,Salem,Roanoke City,Bedford                      \n[331] Lenoir                                                                               \n[332] Platte                                                                               \n[333] Licking                                                                              \n[334] Ada                                                                                  \n[335] Waldo                                                                                \n[336] Bay                                                                                  \n[337] Le Sueur                                                                             \n[338] Monongalia                                                                           \n[339] Emmet                                                                                \n[340] Newport News City,York,New Kent,Williamsburg City,James City                         \n[341] Sweetwater                                                                           \n[342] Yankton                                                                              \n[343] Martin,Palm Beach                                                                    \n[344] Westmoreland                                                                         \n[345] Mckean                                                                               \n[346] Norton City,Wise                                                                     \n[347] Henrico                                                                              \n[348] Frederick,Winchester City                                                            \n[349] Saint Bernard                                                                        \n[350] Monterey                                                                             \n[351] Hillsborough                                                                         \n[352] Floyd                                                                                \n[353] Cobb                                                                                 \n[354] Carter                                                                               \n[355] Scott,Hennepin,Carver                                                                \n[356] Essex,Union                                                                          \n[357] Radford,Montgomery,Pulaski                                                           \n[358] La Paz                                                                               \n[359] Waukesha,Jefferson                                                                   \n[360] Saint Louis,Jefferson                                                                \n[361] Duchesne                                                                             \n[362] Waukesha                                                                             \n[363] Onondaga                                                                             \n[364] Hampden                                                                              \n[365] Plaquemines                                                                          \n[366] Buffalo                                                                              \n[367] Anchorage                                                                            \n[368] Ventura                                                                              \n[369] Rowan                                                                                \n[370] Iberia                                                                               \n[371] Macomb,Wayne                                                                         \n[372] Forsyth,Guilford,Randolph,Davidson                                                   \n[373] Polk                                                                                 \n[374] Evangeline                                                                           \n[375] Dawes                                                                                \n[376] Olmsted                                                                              \n[377] Tangipahoa                                                                           \n[378] Tuscola                                                                              \n[379] Dodge,Washington                                                                     \n[380] Wyandotte                                                                            \n[381] Latah                                                                                \n[382] Washoe                                                                               \n[383] Pike                                                                                 \n[384] Berkeley,Dorchester,Charleston                                                       \n[385] Bronx                                                                                \n[386] Kern                                                                                 \n[387] Arapahoe                                                                             \n[388] Natchitoches                                                                         \n[389] Anoka,Hennepin,Dakota,Ramsey,Washington                                              \n[390] Atlantic                                                                             \n[391] Harrisonburg City,Rockingham                                                         \n[392] Peach                                                                                \n[393] Beauregard                                                                           \n[394] Scioto                                                                               \n[395] Utah,Salt Lake                                                                       \n[396] Fairfax,Alexandria City,Arlington,Falls Church City                                  \n[397] Palm Beach                                                                           \n[398] Rock Island                                                                          \n[399] Allen                                                                                \n[400] West Baton Rouge                                                                     \n[401] Christian                                                                            \n[402] Silver Bow                                                                           \n[403] Alleghany                                                                            \n[404] Fresno                                                                               \n[405] Smith                                                                                \n[406] Stark                                                                                \n[407] Dakota,Woodbury,Union                                                                \n[408] Howell                                                                               \n[409] Luna                                                                                 \n[410] Beaufort                                                                             \n[411] Uintah                                                                               \n[412] Russell                                                                              \n[413] Wapello                                                                              \n[414] Scotland                                                                             \n[415] New York                                                                             \n[416] Canadian                                                                             \n[417] Wasatch                                                                              \n[418] Davis                                                                                \n[419] Iron                                                                                 \n[420] Kootenai                                                                             \n[421] Morgan                                                                               \n[422] Green                                                                                \n[423] Jo Daviess                                                                           \n[424] Wilson                                                                               \n[425] Kittitas                                                                             \n[426] Grant                                                                                \n[427] Hall                                                                                 \n[428] Placer                                                                               \n[429] Paulding                                                                             \n[430] Johnson,Jackson,Wyandotte                                                            \n[431] Carteret                                                                             \n[432] Westmoreland,Armstrong                                                               \n[433] Spokane                                                                              \n[434] Garfield                                                                             \n[435] Oklahoma                                                                             \n[436] Dane                                                                                 \n[437] Merced                                                                               \n[438] Bernalillo                                                                           \n[439] Pitkin                                                                               \n[440] Pitt                                                                                 \n[441] Putnam                                                                               \n[442] Routt                                                                                \n[443] San Bernardino                                                                       \n[444] Manitowoc                                                                            \n[445] Mille Lacs                                                                           \n[446] Sheboygan                                                                            \n[447] Chittenden                                                                           \n[448] Newton                                                                               \n[449] Carroll                                                                              \n[450] San Francisco                                                                        \n[451] Thurston                                                                             \n[452] Mchenry                                                                              \n[453] Pennington                                                                           \n[454] Durham                                                                               \n[455] Mono                                                                                 \n[456] Bartholomew                                                                          \n[457] Kennebec                                                                             \n[458] Hood River                                                                           \n[459] Allegan                                                                              \n[460] Saint Louis,Saint Charles                                                            \n[461] Doddridge                                                                            \n[462] El Dorado,Nevada,Placer                                                              \n[463] New London                                                                           \n[464] Manistee                                                                             \n[465] Saint Charles                                                                        \n[466] Cumberland,Oxford,Belknap,Carroll                                                    \n[467] Macoupin                                                                             \n[468] Teton                                                                                \n[469] Freeborn                                                                             \n[470] Clinton                                                                              \n[471] Ashtabula                                                                            \n[472] San Joaquin                                                                          \n[473] Dubois                                                                               \n[474] Delta                                                                                \n[475] Cuyahoga                                                                             \n[476] San Mateo                                                                            \n[477] Orange,Seminole                                                                      \n[478] Midland                                                                              \n[479] Dona Ana                                                                             \n[480] Sandoval                                                                             \n[481] Lackawanna                                                                           \n[482] Essex,Hudson,Union,Passaic,Bergen                                                    \n[483] Blue Earth                                                                           \n[484] Norfolk City                                                                         \n[485] Worcester                                                                            \n[486] Ventura,Los Angeles                                                                  \n[487] Whiteside                                                                            \n[488] Dodge                                                                                \n[489] Effingham                                                                            \n[490] Colbert                                                                              \n[491] Bureau                                                                               \n[492] Kanabec                                                                              \n[493] Yates                                                                                \n[494] Galveston                                                                            \n[495] Sutter                                                                               \n[496] Stanton,Madison                                                                      \n[497] Marlboro                                                                             \n[498] Box Elder                                                                            \n[499] Iosco                                                                                \n[500] Lewis                                                                                \n[501] Chenango                                                                             \n[502] Lexington                                                                            \n[503] Volusia                                                                              \n[504] Kandiyohi                                                                            \n[505] Trumbull                                                                             \n[506] Whitman                                                                              \n[507] Muscogee,Chattahoochee                                                               \n[508] Blue Earth,Nicollet                                                                  \n[509] Laclede                                                                              \n[510] Cherokee                                                                             \n[511] Frederick                                                                            \n[512] Orange,Pinellas                                                                      \n[513] Cortland                                                                             \n[514] Hocking                                                                              \n[515] Garrett                                                                              \n[516] San Benito                                                                           \n[517] Defiance                                                                             \n[518] Wichita                                                                              \n[519] Holt                                                                                 \n[520] Wasco                                                                                \n[521] Tillamook                                                                            \n[522] Wood                                                                                 \n[523] Fayette                                                                              \n[524] Linn                                                                                 \n[525] Passaic                                                                              \n[526] Alamosa                                                                              \n[527] Audrain                                                                              \n[528] Broome                                                                               \n[529] Schuyler                                                                             \n[530] Henrico,Richmond City,Goochland                                                      \n[531] Lehigh                                                                               \n[532] Barry                                                                                \n[533] Tioga                                                                                \n[534] Herkimer                                                                             \n[535] Mesa                                                                                 \n[536] Dakota                                                                               \n[537] Anderson                                                                             \n[538] Albemarle,Charlottesville City                                                       \n[539] Lafourche                                                                            \n[540] Bossier                                                                              \n[541] Lenawee                                                                              \n[542] Clarke                                                                               \n[543] Crawford                                                                             \n[544] Portage,Summit                                                                       \n[545] Piscataquis                                                                          \n[546] Del Norte                                                                            \n[547] Buchanan                                                                             \n[548] Mcdowell                                                                             \n[549] Chester                                                                              \n[550] Cumberland,Prince Edward                                                             \n[551] Sagadahoc                                                                            \n[552] Stephenson                                                                           \n[553] Red River                                                                            \n[554] Horry,Columbus                                                                       \n[555] Kankakee                                                                             \n[556] Kaufman,Collin,Dallas                                                                \n[557] Belmont                                                                              \n[558] Gregg                                                                                \n[559] Marathon,Wood                                                                        \n[560] Athens                                                                               \n[561] Taylor                                                                               \n[562] Scott,Cape Girardeau                                                                 \n[563] Gogebic                                                                              \n[564] Greene,Christian                                                                     \n[565] Maui                                                                                 \n[566] Saint Louis,Saint Louis City                                                         \n[567] Richland,Lexington                                                                   \n[568] Harrison                                                                             \n[569] Chisago                                                                              \n[570] Barry,Lawrence                                                                       \n[571] Northampton,Bucks                                                                    \n[572] Yolo                                                                                 \n[573] Petersburg City                                                                      \n[574] Williams                                                                             \n[575] Tooele                                                                               \n[576] Santa Cruz,Monterey                                                                  \n[577] Gallatin                                                                             \n[578] Hampton City,Newport News City,York,Gloucester,Mathews,Poquoson City                 \n[579] Middlesex                                                                            \n[580] Humboldt                                                                             \n[581] Rice,Dakota                                                                          \n[582] Stafford,Prince William                                                              \n[583] Santa Fe                                                                             \n[584] Scott,Hennepin,Dakota                                                                \n[585] Scotts Bluff                                                                         \n[586] Steuben                                                                              \n[587] Saline                                                                               \n[588] Androscoggin                                                                         \n[589] Pueblo                                                                               \n[590] Tuscarawas                                                                           \n[591] Saint Croix,Pierce                                                                   \n[592] Nevada                                                                               \n[593] Mackinac                                                                             \n[594] Brazos                                                                               \n[595] Saint Croix                                                                          \n[596] Swain,Jackson                                                                        \n[597] Worcester,Providence,Norfolk                                                         \n[598] Walla Walla                                                                          \n[599] Medina                                                                               \n[600] Sacramento                                                                           \n[601] Barbour                                                                              \n[602] Fairbanks North Star                                                                 \n[603] Escambia                                                                             \n[604] Watauga                                                                              \n[605] Denver                                                                               \n[606] Scott                                                                                \n[607] Sarpy,Douglas                                                                        \n[608] Cleveland                                                                            \n[609] Prince William,Fairfax                                                               \n[610] Coshocton                                                                            \n[611] Randall,Potter                                                                       \n[612] Muskingum                                                                            \n[613] Clatsop                                                                              \n[614] Chippewa,Eau Claire                                                                  \n[615] Suffolk,Middlesex,Worcester,Plymouth,Norfolk                                         \n[616] Cattaraugus                                                                          \n[617] San Miguel                                                                           \n[618] Kenton                                                                               \n[619] Essex,Hudson,Passaic,Bergen                                                          \n[620] Darke                                                                                \n[621] Pleasants                                                                            \n[622] Will                                                                                 \n[623] Howard                                                                               \n[624] Seminole                                                                             \n[625] Coos                                                                                 \n[626] Mercer,Trumbull                                                                      \n[627] Saint Johns                                                                          \n[628] Gwinnett                                                                             \n[629] Oakland,Macomb                                                                       \n[630] Chaves                                                                               \n[631] Hays                                                                                 \n[632] Beadle                                                                               \n[633] Baraga                                                                               \n[634] Camden                                                                               \n[635] Malheur                                                                              \n[636] Indiana                                                                              \n[637] Morrison                                                                             \n[638] Prince William,Fairfax,Fauquier,Loudoun,Fairfax City,Manassas City,Manassas Park City\n[639] Kosciusko                                                                            \n[640] Calcasieu                                                                            \n[641] Hancock                                                                              \n[642] Pickaway                                                                             \n[643] Adams                                                                                \n[644] Woodward                                                                             \n[645] Cayuga                                                                               \n[646] Preble                                                                               \n[647] Dougherty                                                                            \n[648] Webster                                                                              \n[649] Menominee                                                                            \n[650] Ashland                                                                              \n[651] Marinette                                                                            \n[652] Cass                                                                                 \n[653] Virginia Beach City,Norfolk City,Portsmouth City,Chesapeake City                     \n[654] Dodge,Jefferson                                                                      \n[655] Cooke                                                                                \n655 Levels: Acadia Ada Adams Addison Alachua Alameda Alamosa ... Yuma\n\n\n\n#Utilizing the summary table provided I will create synthetic data for each of the 13 variables\n#First I will define the number of obs.\nn_rows&lt;- 773779\n\n\n# Load necessary libraries\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Define the number of rows for the synthetic dataset\nn_rows &lt;- 773779\n\n# Generate synthetic data for each variable\nset.seed(123)  # For reproducibility\n\n# 1. wwtp_jurisdiction (categorical variable)\n# Include all 50 states and the District of Columbia\nstates &lt;- c(\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \n            \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \n            \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \n            \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \n            \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \n            \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"New York\", \n            \"North Carolina\", \"North Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\", \n            \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \n            \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \n            \"West Virginia\", \"Wisconsin\", \"Wyoming\", \"District of Columbia\")\n\n# Use the same proportions as in the original data (adjust probabilities as needed)\nwwtp_jurisdiction &lt;- sample(states, size = n_rows, replace = TRUE, \n                            prob = c(rep(0.02, 50), 0.01))  # Adjust probabilities as needed\n\n# 2. wwtp_id (numeric variable)\n# Use a uniform distribution with the same min and max as the original data\nwwtp_id &lt;- sample(1:2939, size = n_rows, replace = TRUE)\n\n# 3. sample_location (categorical variable)\n# Use the same proportions as in the original data\nsample_location &lt;- sample(c(\"Before treatment plant\", \"Treatment plant\"), \n                          size = n_rows, replace = TRUE, prob = c(0.046, 0.954))  # Adjust probabilities as needed\n\n# 4. county_names \n# Use the provided list of 655 counties\ncounty_names &lt;- c(\"Maricopa\", \"Ottawa\", \"Dallas\", \"Kalamazoo\", \"Butler\", \"Penobscot\", \"Bergen\", \n                  \"Cumberland\", \"Delaware\", \"Muskogee\", \"Virginia Beach City,Chesapeake City\", \n                  \"Marathon\", \"Summit\", \"Queens\", \"Dauphin\", \"Tazewell\", \"Island,Snohomish\", \n                  \"Ohio\", \"Santa Cruz\", \"Saint Clair\", \"Sullivan\", \"Chautauqua\", \"Pinellas\", \n                  \"Honolulu\", \"Du Page\", \"Anne Arundel\", \"Salem\", \"Riverside\", \"Cole,Callaway\", \n                  \"Westchester\", \"Multnomah\", \"Umatilla\", \"La Crosse\", \"Dunn\", \"Lafayette\", \n                  \"Addison\", \"Mohave\", \"Marion\", \"Otsego\", \"Durham,Orange\", \"New Hanover\", \n                  \"Forsyth\", \"Allegheny\", \"Los Angeles\", \"San Francisco,San Mateo\", \"Monmouth\", \n                  \"Bristol\", \"Butte\", \"St Joseph\", \"Vanderburgh\", \"Huron\", \"Skagit\", \"Santa Clara\", \n                  \"Hamilton\", \"Saint Louis\", \"Knox\", \"Hampton\", \"Horry\", \"El Paso\", \"Kane\", \n                  \"Terrebonne\", \"Union\", \"Genesee\", \"Isanti\", \"Albany\", \"Pierce\", \"Salt Lake\", \n                  \"Sonoma\", \"Orange\", \"Boone\", \"Jefferson\", \"Mecklenburg\", \"Erie\", \"Yamhill\", \n                  \"El Dorado\", \"Shawnee\", \"Fulton\", \"Cache\", \"Fairfax,Alexandria City\", \"Yakima\", \n                  \"Clayton\", \"Seneca\", \"Newport\", \"Milwaukee,Ozaukee\", \"Somerset\", \"Contra Costa\", \n                  \"Ouachita\", \"Suffolk,Middlesex\", \"Sandusky\", \"Portsmouth City,Isle Of Wight,Chesapeake City,Suffolk City\", \n                  \"Cabell\", \"Mason\", \"Greene\", \"Lincoln\", \"Santa Barbara\", \"Dunklin\", \"Lancaster\", \n                  \"Utah\", \"Carson City\", \"Niagara\", \"Ontario\", \"Saint Tammany\", \"Grafton\", \n                  \"Columbia\", \"Dutchess\", \"Pontotoc\", \"Clark\", \"Sumter\", \"Houston\", \"Vigo\", \n                  \"Saint Mary\", \"Essex\", \"Hill\", \"Wake\", \"Lawrence\", \"Georgetown\", \"San Juan,Grand\", \n                  \"Walworth\", \"Milwaukee,Ozaukee,Racine,Waukesha,Washington\", \"Mineral\", \n                  \"Orleans\", \"Goodhue\", \"Pulaski\", \"Yuma\", \"Pima\", \"La Plata\", \"Elkhart\", \n                  \"Suffolk,Middlesex,Norfolk\", \"Kent\", \"Gratiot\", \"Rensselaer\", \"Providence\", \n                  \"De Kalb\", \"Chippewa\", \"Eaton\", \"Johnson\", \"Hardin\", \"Pender,Duplin\", \n                  \"Washington\", \"Broward\", \"Marquette\", \"Hennepin\", \"Clay\", \"Monroe\", \"Wyandot\", \n                  \"King,Snohomish\", \"Noble\", \"Suffolk\", \"Fairfield\", \"Snohomish\", \"Burlington\", \n                  \"Wyoming\", \"Philadelphia\", \"Arapahoe,Adams\", \"New Castle\", \"La Salle\", \n                  \"Scott,New Madrid\", \"Otero\", \"Schenectady\", \"Saint Lawrence\", \"Andrews\", \n                  \"Kanawha\", \"Travis\", \"Wayne\", \"Sussex\", \"Rockingham\", \"Centre\", \n                  \"Bedford City,Lynchburg City,Amherst,Bedford,Campbell\", \"Randolph\", \"Plumas\", \n                  \"Muskegon\", \"Nobles\", \"Montgomery\", \"Chelan\", \"Boulder\", \"Grand Traverse\", \n                  \"Kewaunee,Brown\", \"Madison\", \"Houghton\", \"Park\", \"Payne\", \n                  \"Hampton City,Newport News City,York,James City\", \"Richmond\", \"Macon\", \n                  \"Miami-Dade\", \"Carlton,Saint Louis\", \"Onslow\", \"Marin\", \"Acadia\", \"Aroostook\", \n                  \"Kings\", \"Stafford\", \"Hampton City,Newport News City\", \"San Luis Obispo\", \n                  \"Jackson\", \"Tompkins\", \"Livingston\", \"Webb\", \"Sangamon\", \"Bienville\", \n                  \"Preston\", \"Rock\", \"Shelby\", \"Platte,Clay\", \"Schoharie\", \"Benton\", \"Macomb\", \n                  \"Mahoning\", \"Guilford\", \"Mclennan\", \"Douglas\", \"Sheridan\", \"Jessamine\", \n                  \"Warren\", \"Laramie\", \"Catoosa,Walker,Dade,Hamilton\", \"Lee\", \"Solano\", \n                  \"Fairfax,Prince Georges,District Of Columbia,Loudoun,Montgomery,Arlington\", \n                  \"Winnebago\", \"Marshall\", \"Buncombe,Henderson\", \"Cheshire\", \"Kershaw\", \n                  \"Harris\", \"King,Pierce\", \"Isabella\", \"Stanislaus\", \"Arenac\", \"Chemung\", \n                  \"Mercer\", \"Nueces\", \"New Haven\", \"Vernon\", \"Jackson,Cass\", \"Bryan\", \n                  \"Hunterdon\", \"Lorain\", \"Cook\", \"Tippecanoe\", \"Calloway\", \"Garland\", \n                  \"Franklin\", \"Carbon\", \"Caddo\", \"York\", \"Outagamie,Winnebago,Calumet\", \n                  \"Martinsville City,Henry\", \"East Feliciana\", \"Brown\", \"Shasta\", \"Oswego\", \n                  \"Otter Tail\", \"Middlesex,Somerset,Union\", \"Oneida\", \"Lane\", \"Weber,Davis\", \n                  \"Kenosha\", \"Schoolcraft\", \"Saginaw\", \"Rockland\", \"Hawaii\", \"Tuscaloosa\", \n                  \"Le Sueur,Scott\", \"Henry\", \"Tarrant\", \"Fond Du Lac\", \"Sherburne,Benton,Stearns\", \n                  \"Juneau\", \"Warrick\", \"Lewis And Clark\", \"Josephine\", \"Victoria\", \"Iroquois\", \n                  \"Champaign\", \"Lake\", \"Boyd\", \"Nassau\", \"Deschutes\", \"Greenwood\", \"Alachua\", \n                  \"Lucas\", \"Saint Marys\", \"Napa\", \"Mcdonough\", \"Calhoun\", \"Island\", \"Bulloch\", \n                  \"Clare\", \"Lyon\", \"Henderson\", \"Washtenaw\", \"Weld\", \"Hudson\", \"Kendall\", \n                  \"Vermilion\", \"Richland\", \"Sherburne\", \"Loudoun\", \"Coconino\", \"Peoria\", \n                  \"Tulsa\", \"Klamath\", \"Muscatine\", \"Oakland,Wayne\", \"Johnson,Jackson,Cass\", \n                  \"Dent\", \"Nemaha\", \"Cabell,Wayne\", \"Windsor\", \"Concordia\", \"Saratoga\", \n                  \"Clackamas\", \"Alameda\", \"Oxford\", \"San Diego\", \"Jasper\", \"Dickinson\", \n                  \"Northampton,Halifax\", \"Strafford\", \"Morris\", \"Portage\", \"Sarasota\", \n                  \"Leon\", \"Waupaca\", \"Perry\", \"Reno\", \"Ulster\", \"Hidalgo\", \n                  \"Bedford City,Botetourt,Roanoke,Bland,Salem,Roanoke City,Bedford\", \n                  \"Lenoir\", \"Platte\", \"Licking\", \"Ada\", \"Waldo\", \"Bay\", \"Le Sueur\", \n                  \"Monongalia\", \"Emmet\", \"Newport News City,York,New Kent,Williamsburg City,James City\", \n                  \"Sweetwater\", \"Yankton\", \"Martin,Palm Beach\", \"Westmoreland\", \"Mckean\", \n                  \"Norton City,Wise\", \"Henrico\", \"Frederick,Winchester City\", \"Saint Bernard\", \n                  \"Monterey\", \"Hillsborough\", \"Floyd\", \"Cobb\", \"Carter\", \"Scott,Hennepin,Carver\", \n                  \"Essex,Union\", \"Radford,Montgomery,Pulaski\", \"La Paz\", \"Waukesha,Jefferson\", \n                  \"Saint Louis,Jefferson\", \"Duchesne\", \"Waukesha\", \"Onondaga\", \"Hampden\", \n                  \"Plaquemines\", \"Buffalo\", \"Anchorage\", \"Ventura\", \"Rowan\", \"Iberia\", \n                  \"Macomb,Wayne\", \"Forsyth,Guilford,Randolph,Davidson\", \"Polk\", \"Evangeline\", \n                  \"Dawes\", \"Olmsted\", \"Tangipahoa\", \"Tuscola\", \"Dodge,Washington\", \n                  \"Wyandotte\", \"Latah\", \"Washoe\", \"Pike\", \"Berkeley,Dorchester,Charleston\", \n                  \"Bronx\", \"Kern\", \"Arapahoe\", \"Natchitoches\", \"Anoka,Hennepin,Dakota,Ramsey,Washington\", \n                  \"Atlantic\", \"Harrisonburg City,Rockingham\", \"Peach\", \"Beauregard\", \n                  \"Scioto\", \"Utah,Salt Lake\", \"Fairfax,Alexandria City,Arlington,Falls Church City\", \n                  \"Palm Beach\", \"Rock Island\", \"Allen\", \"West Baton Rouge\", \"Christian\", \n                  \"Silver Bow\", \"Alleghany\", \"Fresno\", \"Smith\", \"Stark\", \"Dakota,Woodbury,Union\", \n                  \"Howell\", \"Luna\", \"Beaufort\", \"Uintah\", \"Russell\", \"Wapello\", \"Scotland\", \n                  \"New York\", \"Canadian\", \"Wasatch\", \"Davis\", \"Iron\", \"Kootenai\", \"Morgan\", \n                  \"Green\", \"Jo Daviess\", \"Wilson\", \"Kittitas\", \"Grant\", \"Hall\", \"Placer\", \n                  \"Paulding\", \"Johnson,Jackson,Wyandotte\", \"Carteret\", \"Westmoreland,Armstrong\", \n                  \"Spokane\", \"Garfield\", \"Oklahoma\", \"Dane\", \"Merced\", \"Bernalillo\", \n                  \"Pitkin\", \"Pitt\", \"Putnam\", \"Routt\", \"San Bernardino\", \"Manitowoc\", \n                  \"Mille Lacs\", \"Sheboygan\", \"Chittenden\", \"Newton\", \"Carroll\", \n                  \"San Francisco\", \"Thurston\", \"Mchenry\", \"Pennington\", \"Durham\", \n                  \"Mono\", \"Bartholomew\", \"Kennebec\", \"Hood River\", \"Allegan\", \n                  \"Saint Louis,Saint Charles\", \"Doddridge\", \"El Dorado,Nevada,Placer\", \n                  \"New London\", \"Manistee\", \"Saint Charles\", \"Cumberland,Oxford,Belknap,Carroll\", \n                  \"Macoupin\", \"Teton\", \"Freeborn\", \"Clinton\", \"Ashtabula\", \"San Joaquin\", \n                  \"Dubois\", \"Delta\", \"Cuyahoga\", \"San Mateo\", \"Orange,Seminole\", \n                  \"Midland\", \"Dona Ana\", \"Sandoval\", \"Lackawanna\", \"Essex,Hudson,Union,Passaic,Bergen\", \n                  \"Blue Earth\", \"Norfolk City\", \"Worcester\", \"Ventura,Los Angeles\", \n                  \"Whiteside\", \"Dodge\", \"Effingham\", \"Colbert\", \"Bureau\", \"Kanabec\", \n                  \"Yates\", \"Galveston\", \"Sutter\", \"Stanton,Madison\", \"Marlboro\", \n                  \"Box Elder\", \"Iosco\", \"Lewis\", \"Chenango\", \"Lexington\", \"Volusia\", \n                  \"Kandiyohi\", \"Trumbull\", \"Whitman\", \"Muscogee,Chattahoochee\", \n                  \"Blue Earth,Nicollet\", \"Laclede\", \"Cherokee\", \"Frederick\", \n                  \"Orange,Pinellas\", \"Cortland\", \"Hocking\", \"Garrett\", \"San Benito\", \n                  \"Defiance\", \"Wichita\", \"Holt\", \"Wasco\", \"Tillamook\", \"Wood\", \n                  \"Fayette\", \"Linn\", \"Passaic\", \"Alamosa\", \"Audrain\", \"Broome\", \n                  \"Schuyler\", \"Henrico,Richmond City,Goochland\", \"Lehigh\", \"Barry\", \n                  \"Tioga\", \"Herkimer\", \"Mesa\", \"Dakota\", \"Anderson\", \"Albemarle,Charlottesville City\", \n                  \"Lafourche\", \"Bossier\", \"Lenawee\", \"Clarke\", \"Crawford\", \"Portage,Summit\", \n                  \"Piscataquis\", \"Del Norte\", \"Buchanan\", \"Mcdowell\", \"Chester\", \n                  \"Cumberland,Prince Edward\", \"Sagadahoc\", \"Stephenson\", \"Red River\", \n                  \"Horry,Columbus\", \"Kankakee\", \"Kaufman,Collin,Dallas\", \"Belmont\", \n                  \"Gregg\", \"Marathon,Wood\", \"Athens\", \"Taylor\", \"Scott,Cape Girardeau\", \n                  \"Gogebic\", \"Greene,Christian\", \"Maui\", \"Saint Louis,Saint Louis City\", \n                  \"Richland,Lexington\", \"Harrison\", \"Chisago\", \"Barry,Lawrence\", \n                  \"Northampton,Bucks\", \"Yolo\", \"Petersburg City\", \"Williams\", \n                  \"Tooele\", \"Santa Cruz,Monterey\", \"Gallatin\", \"Hampton City,Newport News City,York,Gloucester,Mathews,Poquoson City\", \n                  \"Middlesex\", \"Humboldt\", \"Rice,Dakota\", \"Stafford,Prince William\", \n                  \"Santa Fe\", \"Scott,Hennepin,Dakota\", \"Scotts Bluff\", \"Steuben\", \n                  \"Saline\", \"Androscoggin\", \"Pueblo\", \"Tuscarawas\", \"Saint Croix,Pierce\", \n                  \"Nevada\", \"Mackinac\", \"Brazos\", \"Saint Croix\", \"Swain,Jackson\", \n                  \"Worcester,Providence,Norfolk\", \"Walla Walla\", \"Medina\", \"Sacramento\", \n                  \"Barbour\", \"Fairbanks North Star\", \"Escambia\", \"Watauga\", \"Denver\", \n                  \"Scott\", \"Sarpy,Douglas\", \"Cleveland\", \"Prince William,Fairfax\", \n                  \"Coshocton\", \"Randall,Potter\", \"Muskingum\", \"Clatsop\", \"Chippewa,Eau Claire\", \n                  \"Suffolk,Middlesex,Worcester,Plymouth,Norfolk\", \"Cattaraugus\", \n                  \"San Miguel\", \"Kenton\", \"Essex,Hudson,Passaic,Bergen\", \"Darke\", \n                  \"Pleasants\", \"Will\", \"Howard\", \"Seminole\", \"Coos\", \"Mercer,Trumbull\", \n                  \"Saint Johns\", \"Gwinnett\", \"Oakland,Macomb\", \"Chaves\", \"Hays\", \n                  \"Beadle\", \"Baraga\", \"Camden\", \"Malheur\", \"Indiana\", \"Morrison\", \n                  \"Prince William,Fairfax,Fauquier,Loudoun,Fairfax City,Manassas City,Manassas Park City\", \n                  \"Kosciusko\", \"Calcasieu\", \"Hancock\", \"Pickaway\", \"Adams\", \"Woodward\", \n                  \"Cayuga\", \"Preble\", \"Dougherty\", \"Webster\", \"Menominee\", \"Ashland\", \n                  \"Marinette\", \"Cass\", \"Virginia Beach City,Norfolk City,Portsmouth City,Chesapeake City\", \n                  \"Dodge,Jefferson\", \"Cooke\")\n\n# Randomly sample county names\ncounty_names &lt;- sample(county_names, size = n_rows, replace = TRUE)\n\n# 5. population_served (numeric variable)\n# Use a log-normal distribution to better match the original data's skewness\npopulation_served &lt;- round(rlnorm(n_rows, meanlog = log(128734), sdlog = 1))  # Adjust parameters as needed\npopulation_served &lt;- pmax(population_served, 564)  # Ensure minimum value is 564\n\n# 6. date_start and date_end (date variables)\n# Use a uniform distribution for dates within the original range\ndate_start &lt;- seq(as.Date(\"2023-01-01\"), as.Date(\"2024-12-31\"), by = \"day\")\ndate_start &lt;- sample(date_start, size = n_rows, replace = TRUE)\ndate_end &lt;- date_start + sample(1:14, size = n_rows, replace = TRUE)  # Assuming 1-14 days difference\n\n# 7. ptc_15d (numeric variable)\n# Use a mixture distribution to account for the extreme values\nptc_15d &lt;- ifelse(runif(n_rows) &lt; 0.95, \n                  rnorm(n_rows, mean = 0, sd = 100),  # Most values are around 0\n                  runif(n_rows, min = 1e6, max = 2147483647))  # Some extreme values\nptc_15d &lt;- pmax(ptc_15d, -100)  # Ensure minimum value is -100\n\n# 8. detect_prop_15d (numeric variable)\n# Use a discrete distribution to match the original data's values\ndetect_prop_15d &lt;- sample(c(0, 50, 67, 80, 100), size = n_rows, replace = TRUE, \n                          prob = c(0.1, 0.2, 0.2, 0.2, 0.3))  # Adjust probabilities as needed\n\n# 9. percentile (numeric variable)\n# Use a uniform distribution between 0 and 100\npercentile &lt;- runif(n_rows, min = 0, max = 100)\n\n# 10. sampling_prior (categorical variable)\n# Use the same proportions as in the original data\nsampling_prior &lt;- sample(c(\"no\", \"yes\"), size = n_rows, replace = TRUE, prob = c(0.85, 0.15))  # Adjust probabilities as needed\n\n# 11. first_sample_date (date variable)\n# Use a uniform distribution for dates within the original range\nfirst_sample_date &lt;- seq(as.Date(\"2020-07-05\"), as.Date(\"2025-01-14\"), by = \"day\")\nfirst_sample_date &lt;- sample(first_sample_date, size = n_rows, replace = TRUE)\n\n# Combine all variables into a synthetic dataset\nsynthetic_data &lt;- data.frame(\n  wwtp_jurisdiction,\n  wwtp_id,\n  sample_location,\n  county_names,\n  population_served,\n  date_start,\n  date_end,\n  ptc_15d,\n  detect_prop_15d,\n  percentile,\n  sampling_prior,\n  first_sample_date\n)\n\n# View the first few rows of the synthetic dataset\nhead(synthetic_data)\n\n  wwtp_jurisdiction wwtp_id sample_location  county_names population_served\n1            Kansas    2453 Treatment plant   Saint Johns            128212\n2      South Dakota    2483 Treatment plant Saint Bernard            260611\n3          Michigan    1049 Treatment plant          Coos             33282\n4          Virginia    1907 Treatment plant       Garrett            213790\n5         Wisconsin      89 Treatment plant       Tuscola            487343\n6          Arkansas    2003 Treatment plant       Wichita            125338\n  date_start   date_end    ptc_15d detect_prop_15d percentile sampling_prior\n1 2024-06-19 2024-06-26 -100.00000             100   77.13513             no\n2 2023-10-25 2023-11-07   79.20716             100   46.79306             no\n3 2024-08-21 2024-08-29  267.93114             100   74.93197             no\n4 2024-05-25 2024-05-26   34.21115              50   50.04359             no\n5 2023-08-03 2023-08-16   16.10619              80   76.92259             no\n6 2023-08-16 2023-08-18  -27.87697              80   89.09138             no\n  first_sample_date\n1        2024-04-15\n2        2023-08-27\n3        2023-03-15\n4        2024-01-22\n5        2021-11-08\n6        2023-04-26\n\n# Check the distributions of key variables\nsummary(synthetic_data$population_served)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n     824    65550   128757   212086   252673 12371955 \n\nsummary(synthetic_data$ptc_15d)\n\n      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \n      -100        -63          7   53682978         81 2147473613 \n\nsummary(synthetic_data$detect_prop_15d)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00   50.00   80.00   69.44  100.00  100.00 \n\nsummary(synthetic_data$percentile)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n  0.00046  24.93337  49.94348  49.95986  74.94543  99.99988 \n\n\n\nggplot(synthetic_data, aes(x = population_served)) +\n  geom_histogram(binwidth = 1000, fill = \"blue\", color = \"blue\") +\n  labs(title = \"Distribution of Population Served\", x = \"Population Served\", y = \"Frequency\")\n\n\n\n\n\n\n\nggplot(synthetic_data, aes(x = ptc_15d)) +\n  geom_histogram(fill = \"green\", color = \"green\") +\n  xlim(-100, 100) +\n  labs(title = \"Distribution of PTC 15d\", x = \"PTC 15d\", y = \"Frequency\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 155542 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\nggplot(synthetic_data, aes(x = detect_prop_15d)) +\n  geom_histogram(binwidth = 10, fill = \"red\", color = \"red\") +\n  labs(title = \"Distribution of Detect Prop 15d\", x = \"Detect Prop 15d\", y = \"Frequency\")\n\n\n\n\n\n\n\nggplot(synthetic_data, aes(x = percentile)) +\n  geom_histogram(binwidth = 5, fill = \"purple\", color = \"purple\") +\n  labs(title = \"Distribution of Percentile\", x = \"Percentile\", y = \"Frequency\")\n\n\n\n\n\n\n\n\n\n#Plot distributions of categorical variables\nggplot(df, aes(x = wwtp_jurisdiction)) +\n  geom_bar(fill = \"blue\") +\n  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +\n  labs(title = \"Distribution of wwtp_jurisdiction\",\n       x = \"wwtp_jurisdiction\",\n       y = \"count\")\n\n\n\n\n\n\n\nggplot(filter(df, wwtp_jurisdiction == \"Georgia\"), aes(x = county_names)) +\n  geom_bar(fill = \"green\") +\n  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +\n  labs(title = \"Distribution of counties in Georgia\",\n       x = \"counties\",\n       y = \"count\")\n\n\n\n\n\n\n\nggplot(df, aes(x = sample_location)) +\n  geom_bar(fill = \"red\") +\n  theme(axis.text.x = element_text(angle = 0.5, hjust = 0.5)) +\n  labs(title = \"Distribution of sample_location\",\n       x = \"sample location\",\n       y = \"count\")\n\n\n\n\n\n\n\nggplot(df, aes(x = sampling_prior)) +\n  geom_bar(fill = \"purple\") +\n  theme(axis.text.x = element_text(angle = 0.5, hjust = 0.5)) +\n  labs(title = \"Distribution of sampling_prior\",\n       x = \"prior sampling\",\n       y = \"count\")"
  },
  {
    "objectID": "data-exercise/data-exercise.html",
    "href": "data-exercise/data-exercise.html",
    "title": "Synthetic Data Exercise",
    "section": "",
    "text": "For this exercise, I will write code to create a synthetic data set describing regional case rates of vector-borne disease as well as data on the climate of each reported region and then do an initial exploratory analysis on that data.\n#Load required packages\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(purrr)\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(ggplot2)\nlibrary(here)\n\nhere() starts at /Users/cgnorris/Documents/GitHub/MADA (EPID 8060E)/connornorris-MADA-portfolio\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nlibrary(pscl)\n\nClasses and Methods for R originally developed in the\nPolitical Science Computational Laboratory\nDepartment of Political Science\nStanford University (2002-2015),\nby and under the direction of Simon Jackman.\nhurdle and zeroinfl functions by Achim Zeileis.\n\nlibrary(boot)\nlibrary(psych)\n\n\nAttaching package: 'psych'\n\n\nThe following object is masked from 'package:boot':\n\n    logit\n\n\nThe following objects are masked from 'package:ggplot2':\n\n    %+%, alpha\n\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:psych':\n\n    logit\n\n\nThe following object is masked from 'package:boot':\n\n    logit\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n#Set seed for reproducibility\nset.seed(123)\n\n#Set the number of observations to generate\nn_regions &lt;- 200"
  },
  {
    "objectID": "data-exercise/data-exercise.html#generating-data",
    "href": "data-exercise/data-exercise.html#generating-data",
    "title": "Synthetic Data Exercise",
    "section": "Generating Data",
    "text": "Generating Data\n\n#Create an empty data frame\nregion_data &lt;- data.frame(\n  RegionID = numeric(n_regions),\n  Type = factor(character(n_regions), 4),\n  Temperature = numeric(n_regions),\n  Humidity = numeric(n_regions),\n  Rainfall = numeric(n_regions),\n  Mosquito_Density = integer(n_regions),\n  Sanitation_Quality = factor(numeric(n_regions), 5, ordered = TRUE),\n  Population_Density = integer(n_regions),\n  Access_Clean_Water = factor(character(n_regions), 2),\n  Vector_Borne_Cases = integer(n_regions)\n)\n\n#Fill each variable\n#Variable 1: Region ID\nregion_data$RegionID &lt;- 1:n_regions\n\n#Variable 2: Type (factor with four levels)\nregion_data$Type &lt;- sample(c(\"Coastal\", \"Urban\", \"Rural\", \"Mountainous\"), n_regions, replace = TRUE)\n\n#Climate-based variables (3-5): vary by region type\n#Variable 3: Temperature (numeric, degrees Celcius)\nregion_data$Temperature &lt;- round(rnorm(n_regions, \n                                       mean = ifelse(region_data$Type == \"Coastal\", 30, \n                                              ifelse(region_data$Type == \"Urban\", 25,\n                                              ifelse(region_data$Type == \"Rural\", 28, 18))),\n                                       sd = 3), 1)\n\n#Variable 4: Humidity (numeric, percentage)\nregion_data$Humidity &lt;- round(rnorm(n_regions, \n                                    mean = ifelse(region_data$Type == \"Coastal\", 80,\n                                           ifelse(region_data$Type == \"Urban\", 60,\n                                           ifelse(region_data$Type == \"Rural\", 70, 50))),\n                                    sd = 5), 0)\n\n#Variable 5: Rainfall (numeric, mm)\nregion_data$Rainfall &lt;- round(rnorm(n_regions,\n                                    mean = ifelse(region_data$Type == \"Coastal\", 300,\n                                           ifelse(region_data$Type == \"Urban\", 100,\n                                           ifelse(region_data$Type == \"Rural\", 200, 50))),\n                                    sd = 50), 1) %&gt;%\n  pmax(0, region_data$Rainfall) #No negatives\n\n#Variable 6: Mosquito Population Density (integer)\n#Increases with temperature, humidity, and rainfall\nregion_data$Mosquito_Density &lt;- round(500 + \n    50 * (region_data$Temperature - 15) +\n    20 * (region_data$Humidity - 40) +\n    0.5 * region_data$Rainfall +\n    rnorm(n_regions, mean = 0, sd = 5000)) %&gt;%\n  pmax(0, region_data$Mosquito_Density) #No negatives\n\n#Variable 7: Sanitation Quality (ordered factor 1-5, 1 is worst quality)\n#Slightly higher in urban areas\nregion_data$Sanitation_Quality &lt;- ifelse(region_data$Type == \"Urban\",\n                                         sample(2:5, n_regions, replace = T, \n                                                prob = c(0.05, 0.3, 0.4, 0.25)),\n                                         sample(1:4, n_regions, replace = T,\n                                                prob = c(0.2, 0.3, 0.3, 0.2)))\n\n#Variable 8: Population Density (integer)\n#Higher in urban areas\nregion_data$Population_Density &lt;- round(rnorm(n_regions,\n                                              mean = ifelse(region_data$Type == \"Urban\", 5000,\n                                                     ifelse(region_data$Type == \"Coastal\", 2000,\n                                                     ifelse(region_data$Type == \"Rural\", 500, 300))),\n                                              sd = 800), 0) %&gt;%\n  pmax(50, region_data$Population_Density)\n\n#Variable 9: Access to Clean Water (yes/no)\n#Higher in urban/coastal areas\nregion_data$Access_Clean_Water &lt;- ifelse(region_data$Type == \"Urban\" | region_data$Type == \"Coastal\",\n                                         sample(c(\"Yes\", \"No\"), n_regions, replace = T,\n                                                prob = c(0.9, 0.1)),\n                                         sample(c(\"Yes\", \"No\"), n_regions, replace = T,\n                                                prob = c(0.7, 0.3)))\n\n#Variable 10: Vector-Borne Disease Counts\n#Increases with mosquito density, humidity, and temperature\nregion_data$Vector_Borne_Cases &lt;- round(0.01 * region_data$Mosquito_Density +\n                                        2.5 * region_data$Temperature +\n                                        1.5 * region_data$Humidity + \n                                        0.05 * region_data$Population_Density +\n                                        rnorm(n_regions, mean = 0, sd = 25)) %&gt;%\n  pmax(0, region_data$Vector_Borne_Cases) #No negative cases"
  },
  {
    "objectID": "data-exercise/data-exercise.html#exploring-the-data",
    "href": "data-exercise/data-exercise.html#exploring-the-data",
    "title": "Synthetic Data Exercise",
    "section": "Exploring the Data",
    "text": "Exploring the Data\n\n#Determine the type of object that the data is\nclass(region_data)\n\n[1] \"data.frame\"\n\n#Get an overview of data structure\nstr(region_data)\n\n'data.frame':   200 obs. of  10 variables:\n $ RegionID          : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Type              : chr  \"Rural\" \"Rural\" \"Rural\" \"Urban\" ...\n $ Temperature       : num  25.9 28.8 27.3 24 25.1 24.9 22.6 20 26.9 32.8 ...\n $ Humidity          : num  66 66 65 55 68 62 50 61 76 90 ...\n $ Rainfall          : num  170 150 251 138 124 ...\n $ Mosquito_Density  : num  0 0 0 1913 0 ...\n $ Sanitation_Quality: int  2 2 2 3 3 4 5 3 2 2 ...\n $ Population_Density: num  1233 1140 50 3879 628 ...\n $ Access_Clean_Water: chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ Vector_Borne_Cases: num  205 220 146 371 224 492 377 373 240 409 ...\n\n#Get a data summary\nsummary(region_data)\n\n    RegionID          Type            Temperature       Humidity    \n Min.   :  1.00   Length:200         Min.   :13.00   Min.   :40.00  \n 1st Qu.: 50.75   Class :character   1st Qu.:22.68   1st Qu.:56.00  \n Median :100.50   Mode  :character   Median :25.95   Median :65.50  \n Mean   :100.50                      Mean   :25.55   Mean   :65.49  \n 3rd Qu.:150.25                      3rd Qu.:28.90   3rd Qu.:75.00  \n Max.   :200.00                      Max.   :37.70   Max.   :93.00  \n    Rainfall     Mosquito_Density Sanitation_Quality Population_Density\n Min.   :  0.0   Min.   :    0    Min.   :1.00       Min.   :  50.0    \n 1st Qu.: 78.5   1st Qu.:    0    1st Qu.:2.00       1st Qu.: 404.5    \n Median :147.5   Median : 2330    Median :3.00       Median :1402.5    \n Mean   :160.1   Mean   : 3304    Mean   :2.86       Mean   :2109.2    \n 3rd Qu.:242.2   3rd Qu.: 5382    3rd Qu.:4.00       3rd Qu.:3786.0    \n Max.   :404.3   Max.   :15409    Max.   :5.00       Max.   :6940.0    \n Access_Clean_Water Vector_Borne_Cases\n Length:200         Min.   : 77.0     \n Class :character   1st Qu.:208.0     \n Mode  :character   Median :299.5     \n                    Mean   :302.2     \n                    3rd Qu.:397.5     \n                    Max.   :547.0     \n\n\nThe code above created a data frame with 200 observations with 10 variables. The first variable assigns a unique identifier to each region represented in the dataset. The second variable is a character factor with four levels that describes the type of region listed: coastal, urban, rural, or mountainous. The next three variables are all climate based, describing the average temperature, humidity, and rainfall in each region. The sixth and eighth variables describe the population densities of mosquitoes and people in each region, respectively. The seventh variable is an ordered five-level factor describing the sanitation infrastructure quality of each region, with ‘1’ indicating the poorest quality and ‘5’ indicating the highest quality. The ninth variable is a dichotomous variable indicating whether or not those in the region have access to clean water. The tenth variable indicates the number of vector-borne disease cases in each region."
  },
  {
    "objectID": "data-exercise/data-exercise.html#plotting",
    "href": "data-exercise/data-exercise.html#plotting",
    "title": "Synthetic Data Exercise",
    "section": "Plotting",
    "text": "Plotting\nThe following is a scatterplot matrix showing the relationships between the numerical variables in this dataset.\n\nreduced_data &lt;- region_data %&gt;%\n  dplyr::select(Temperature, Humidity, Rainfall, Mosquito_Density, Sanitation_Quality, Population_Density, Vector_Borne_Cases)\nscatterplotMatrix(reduced_data)\n\n\n\n\n\n\n\n\nThe following are boxplots comparing the relationships between the type of region and climate-related varaibles.\n\n#Temperature by region type\nggplot(data = region_data, aes(x = Type, y = Temperature)) +\n  geom_boxplot() +\n  labs(\n    title = \"Temperature ranges by region type\",\n    x = \"Region type\",\n    y = \"Temperature (°C)\") + \n  theme_classic()\n\n\n\n\n\n\n\n#Humidity by region type\nggplot(data = region_data, aes(x = Type, y = Humidity)) +\n  geom_boxplot() +\n  labs(\n    title = \"Humidity ranges by region type\",\n    x = \"Region type\",\n    y = \"Humidity (%)\") + \n  theme_classic()\n\n\n\n\n\n\n\n#Rainfall by region type\nggplot(data = region_data, aes(x = Type, y = Rainfall)) +\n  geom_boxplot() +\n  labs(\n    title = \"Rainfall ranges by region type\",\n    x = \"Region type\",\n    y = \"Rainfall (mm)\") + \n  theme_classic()"
  },
  {
    "objectID": "data-exercise/data-exercise.html#modeling-case-counts",
    "href": "data-exercise/data-exercise.html#modeling-case-counts",
    "title": "Synthetic Data Exercise",
    "section": "Modeling Case Counts",
    "text": "Modeling Case Counts\nSince this data depicts count data, Poisson regression or other similar models should be used.\n\nattach(region_data)\n\n#Check for overdispersion of data\nmean(Vector_Borne_Cases) - var(Vector_Borne_Cases)\n\n[1] -12171.47\n\n#Poisson model for vector-borne cases\nlm1 &lt;- glm(Vector_Borne_Cases ~ Temperature + Humidity + Rainfall + Sanitation_Quality + Mosquito_Density + Population_Density + Access_Clean_Water, family = \"poisson\")\n\nsummary(lm1)\n\n\nCall:\nglm(formula = Vector_Borne_Cases ~ Temperature + Humidity + Rainfall + \n    Sanitation_Quality + Mosquito_Density + Population_Density + \n    Access_Clean_Water, family = \"poisson\")\n\nCoefficients:\n                        Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)            4.540e+00  3.639e-02 124.763  &lt; 2e-16 ***\nTemperature            5.550e-03  1.291e-03   4.301  1.7e-05 ***\nHumidity               7.450e-03  6.152e-04  12.110  &lt; 2e-16 ***\nRainfall               1.679e-04  6.855e-05   2.450   0.0143 *  \nSanitation_Quality    -2.269e-03  4.047e-03  -0.561   0.5750    \nMosquito_Density       3.235e-05  1.079e-06  29.987  &lt; 2e-16 ***\nPopulation_Density     1.667e-04  2.434e-06  68.490  &lt; 2e-16 ***\nAccess_Clean_WaterYes -4.602e-03  1.091e-02  -0.422   0.6731    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 8510.47  on 199  degrees of freedom\nResidual deviance:  676.41  on 192  degrees of freedom\nAIC: 2186.9\n\nNumber of Fisher Scoring iterations: 4\n\n\nThe variance of the vector-borne cases being greater than the mean of the vector-borne cases indicates that this data is overdispersed. In addition, certain variables in the Poisson model are significant even though there is not much biological rational for them to be significant. For example, I would expect access to clean water to only have much of an impact on waterborne disease cases, but it is significant in the Poisson model for vector-borne disease cases. Switching to a negative binomial model should better account for the problems present in the Poisson model.\n\n#Fit a negative binomial model for vector-borne disease counts\nlm2 &lt;- glm.nb(Vector_Borne_Cases ~ Type + Temperature + Humidity + Rainfall + Sanitation_Quality + Mosquito_Density + Population_Density + Access_Clean_Water)\nsummary(lm2)\n\n\nCall:\nglm.nb(formula = Vector_Borne_Cases ~ Type + Temperature + Humidity + \n    Rainfall + Sanitation_Quality + Mosquito_Density + Population_Density + \n    Access_Clean_Water, init.theta = 113.5405425, link = log)\n\nCoefficients:\n                        Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)            4.482e+00  1.627e-01  27.554  &lt; 2e-16 ***\nTypeMountainous        4.602e-04  7.946e-02   0.006   0.9954    \nTypeRural              8.749e-04  3.608e-02   0.024   0.9807    \nTypeUrban              4.903e-02  6.962e-02   0.704   0.4812    \nTemperature            5.971e-03  2.783e-03   2.146   0.0319 *  \nHumidity               7.906e-03  1.582e-03   4.999 5.77e-07 ***\nRainfall               2.280e-04  1.688e-04   1.351   0.1767    \nSanitation_Quality    -6.026e-03  8.151e-03  -0.739   0.4597    \nMosquito_Density       3.552e-05  2.206e-06  16.100  &lt; 2e-16 ***\nPopulation_Density     1.636e-04  1.165e-05  14.045  &lt; 2e-16 ***\nAccess_Clean_WaterYes -6.934e-03  2.027e-02  -0.342   0.7323    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(113.5405) family taken to be 1)\n\n    Null deviance: 2452.26  on 199  degrees of freedom\nResidual deviance:  216.25  on 189  degrees of freedom\nAIC: 1986.9\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  113.5 \n          Std. Err.:  17.2 \n\n 2 x log-likelihood:  -1962.887 \n\n#Stepwise removal of non-significant terms\nstep(lm2, direction = \"backward\")\n\nStart:  AIC=1984.89\nVector_Borne_Cases ~ Type + Temperature + Humidity + Rainfall + \n    Sanitation_Quality + Mosquito_Density + Population_Density + \n    Access_Clean_Water\n\n                     Df Deviance    AIC\n- Type                3   217.13 1979.8\n- Access_Clean_Water  1   216.37 1983.0\n- Sanitation_Quality  1   216.80 1983.4\n- Rainfall            1   218.08 1984.7\n&lt;none&gt;                    216.25 1984.9\n- Temperature         1   220.83 1987.5\n- Humidity            1   240.99 2007.6\n- Population_Density  1   416.73 2183.4\n- Mosquito_Density    1   477.74 2244.4\n\nStep:  AIC=1979.76\nVector_Borne_Cases ~ Temperature + Humidity + Rainfall + Sanitation_Quality + \n    Mosquito_Density + Population_Density + Access_Clean_Water\n\n                     Df Deviance    AIC\n- Access_Clean_Water  1   215.74 1977.9\n- Sanitation_Quality  1   215.81 1978.0\n&lt;none&gt;                    215.58 1979.8\n- Rainfall            1   217.59 1979.8\n- Temperature         1   221.80 1984.0\n- Humidity            1   256.86 2019.0\n- Mosquito_Density    1   477.56 2239.7\n- Population_Density  1  1553.96 3316.1\n\nStep:  AIC=1977.92\nVector_Borne_Cases ~ Temperature + Humidity + Rainfall + Sanitation_Quality + \n    Mosquito_Density + Population_Density\n\n                     Df Deviance    AIC\n- Sanitation_Quality  1   215.86 1976.1\n- Rainfall            1   217.57 1977.8\n&lt;none&gt;                    215.64 1977.9\n- Temperature         1   221.86 1982.1\n- Humidity            1   256.91 2017.2\n- Mosquito_Density    1   479.40 2239.7\n- Population_Density  1  1611.83 3372.1\n\nStep:  AIC=1976.14\nVector_Borne_Cases ~ Temperature + Humidity + Rainfall + Mosquito_Density + \n    Population_Density\n\n                     Df Deviance    AIC\n&lt;none&gt;                    215.66 1976.1\n- Rainfall            1   217.72 1976.2\n- Temperature         1   221.95 1980.4\n- Humidity            1   256.99 2015.5\n- Mosquito_Density    1   485.87 2244.3\n- Population_Density  1  1945.52 3704.0\n\n\n\nCall:  glm.nb(formula = Vector_Borne_Cases ~ Temperature + Humidity + \n    Rainfall + Mosquito_Density + Population_Density, init.theta = 112.0723353, \n    link = log)\n\nCoefficients:\n       (Intercept)         Temperature            Humidity            Rainfall  \n         4.471e+00           6.247e-03           7.684e-03           1.893e-04  \n  Mosquito_Density  Population_Density  \n         3.551e-05           1.709e-04  \n\nDegrees of Freedom: 199 Total (i.e. Null);  194 Residual\nNull Deviance:      2430 \nResidual Deviance: 215.7    AIC: 1978\n\n#Final model for vector-borne cases\nvector_model &lt;- glm.nb(Vector_Borne_Cases ~ Temperature + Humidity + Mosquito_Density + Population_Density)\nsummary(vector_model)\n\n\nCall:\nglm.nb(formula = Vector_Borne_Cases ~ Temperature + Humidity + \n    Mosquito_Density + Population_Density, init.theta = 110.2600906, \n    link = log)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)        4.415e+00  4.785e-02  92.266  &lt; 2e-16 ***\nTemperature        7.467e-03  2.342e-03   3.188  0.00143 ** \nHumidity           8.568e-03  1.028e-03   8.331  &lt; 2e-16 ***\nMosquito_Density   3.550e-05  2.185e-06  16.247  &lt; 2e-16 ***\nPopulation_Density 1.697e-04  4.075e-06  41.651  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(110.2601) family taken to be 1)\n\n    Null deviance: 2402.03  on 199  degrees of freedom\nResidual deviance:  215.39  on 195  degrees of freedom\nAIC: 1978.2\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  110.3 \n          Std. Err.:  16.5 \n\n 2 x log-likelihood:  -1966.184 \n\n\nThe AIC value of the full negative binomial model was much lower than the AIC value for the full Poisson model, indicating that the negative binomial model was a better fit. A backwards stepwise regression with this model confirmed that temperature, humidity, mosquito density, and popularion density were significant predictors in this model for this data."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html",
    "href": "coding-exercise/coding-exercise.html",
    "title": "R Coding Exercise",
    "section": "",
    "text": "#Load the dslabs package to get the gapminder dataset and other necessary packages\nlibrary(dslabs)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(tidyr)\n\n\n#Exploratory analysis of the gapminder dataset\n\n#Use the help method to get a data description\nhelp(\"gapminder\")\n\n#Get an overview of data structure\nstr(gapminder)\n\n'data.frame':   10545 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  115.4 148.2 208 NA 59.9 ...\n $ life_expectancy : num  62.9 47.5 36 63 65.4 ...\n $ fertility       : num  6.19 7.65 7.32 4.43 3.11 4.55 4.82 3.45 2.7 5.57 ...\n $ population      : num  1636054 11124892 5270844 54681 20619075 ...\n $ gdp             : num  NA 1.38e+10 NA NA 1.08e+11 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 4 1 1 2 2 3 2 5 4 3 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 19 11 10 2 15 21 2 1 22 21 ...\n\n#Get a data summary\nsummary(gapminder)\n\n                country           year      infant_mortality life_expectancy\n Albania            :   57   Min.   :1960   Min.   :  1.50   Min.   :13.20  \n Algeria            :   57   1st Qu.:1974   1st Qu.: 16.00   1st Qu.:57.50  \n Angola             :   57   Median :1988   Median : 41.50   Median :67.54  \n Antigua and Barbuda:   57   Mean   :1988   Mean   : 55.31   Mean   :64.81  \n Argentina          :   57   3rd Qu.:2002   3rd Qu.: 85.10   3rd Qu.:73.00  \n Armenia            :   57   Max.   :2016   Max.   :276.90   Max.   :83.90  \n (Other)            :10203                  NA's   :1453                    \n   fertility       population             gdp               continent   \n Min.   :0.840   Min.   :3.124e+04   Min.   :4.040e+07   Africa  :2907  \n 1st Qu.:2.200   1st Qu.:1.333e+06   1st Qu.:1.846e+09   Americas:2052  \n Median :3.750   Median :5.009e+06   Median :7.794e+09   Asia    :2679  \n Mean   :4.084   Mean   :2.701e+07   Mean   :1.480e+11   Europe  :2223  \n 3rd Qu.:6.000   3rd Qu.:1.523e+07   3rd Qu.:5.540e+10   Oceania : 684  \n Max.   :9.220   Max.   :1.376e+09   Max.   :1.174e+13                  \n NA's   :187     NA's   :185         NA's   :2972                       \n             region    \n Western Asia   :1026  \n Eastern Africa : 912  \n Western Africa : 912  \n Caribbean      : 741  \n South America  : 684  \n Southern Europe: 684  \n (Other)        :5586  \n\n#Determine the type of object that gapminder is\nclass(gapminder)\n\n[1] \"data.frame\"\n\n\n\n#Data processing\n\n#Create a subset of the data just containing countries from Africa\nafricadata &lt;- gapminder %&gt;%\n  filter(continent == \"Africa\")\n\n#Examine the structure of the Africa-specific data\nstr(africadata)\n\n'data.frame':   2907 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n $ fertility       : num  7.65 7.32 6.28 6.62 6.29 6.95 5.65 6.89 5.84 6.25 ...\n $ population      : num  11124892 5270844 2431620 524029 4829291 ...\n $ gdp             : num  1.38e+10 NA 6.22e+08 1.24e+08 5.97e+08 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\nsummary(africadata)\n\n         country          year      infant_mortality life_expectancy\n Algeria     :  57   Min.   :1960   Min.   : 11.40   Min.   :13.20  \n Angola      :  57   1st Qu.:1974   1st Qu.: 62.20   1st Qu.:48.23  \n Benin       :  57   Median :1988   Median : 93.40   Median :53.98  \n Botswana    :  57   Mean   :1988   Mean   : 95.12   Mean   :54.38  \n Burkina Faso:  57   3rd Qu.:2002   3rd Qu.:124.70   3rd Qu.:60.10  \n Burundi     :  57   Max.   :2016   Max.   :237.40   Max.   :77.60  \n (Other)     :2565                  NA's   :226                     \n   fertility       population             gdp               continent   \n Min.   :1.500   Min.   :    41538   Min.   :4.659e+07   Africa  :2907  \n 1st Qu.:5.160   1st Qu.:  1605232   1st Qu.:8.373e+08   Americas:   0  \n Median :6.160   Median :  5570982   Median :2.448e+09   Asia    :   0  \n Mean   :5.851   Mean   : 12235961   Mean   :9.346e+09   Europe  :   0  \n 3rd Qu.:6.860   3rd Qu.: 13888152   3rd Qu.:6.552e+09   Oceania :   0  \n Max.   :8.450   Max.   :182201962   Max.   :1.935e+11                  \n NA's   :51      NA's   :51          NA's   :637                        \n                       region   \n Eastern Africa           :912  \n Western Africa           :912  \n Middle Africa            :456  \n Northern Africa          :342  \n Southern Africa          :285  \n Australia and New Zealand:  0  \n (Other)                  :  0  \n\n#Create a new subset of the africa data only containing data on infant mortality and life expectancy\nafrica1 &lt;- africadata %&gt;%\n  select(c('infant_mortality', 'life_expectancy'))\n\n#Examine the structure of the first subset\nstr(africa1)\n\n'data.frame':   2907 obs. of  2 variables:\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n\nsummary(africa1)\n\n infant_mortality life_expectancy\n Min.   : 11.40   Min.   :13.20  \n 1st Qu.: 62.20   1st Qu.:48.23  \n Median : 93.40   Median :53.98  \n Mean   : 95.12   Mean   :54.38  \n 3rd Qu.:124.70   3rd Qu.:60.10  \n Max.   :237.40   Max.   :77.60  \n NA's   :226                     \n\n#Create a new subset of the africa data only containing data on population and life expectancy\nafrica2 &lt;- africadata %&gt;%\n  select(c('population', 'life_expectancy'))\n\n#Examine the structure of the second subset\nstr(africa2)\n\n'data.frame':   2907 obs. of  2 variables:\n $ population     : num  11124892 5270844 2431620 524029 4829291 ...\n $ life_expectancy: num  47.5 36 38.3 50.3 35.2 ...\n\nsummary(africa2)\n\n   population        life_expectancy\n Min.   :    41538   Min.   :13.20  \n 1st Qu.:  1605232   1st Qu.:48.23  \n Median :  5570982   Median :53.98  \n Mean   : 12235961   Mean   :54.38  \n 3rd Qu.: 13888152   3rd Qu.:60.10  \n Max.   :182201962   Max.   :77.60  \n NA's   :51                         \n\n\n\n#Data plotting\n\n#Plot life expectancy as a function of infant mortality\nggplot(africa1, aes(x = infant_mortality, y = life_expectancy)) + \n  geom_point() + #Make it a scatter plot\n  labs(\n    title = \"Life expectancy vs. infant mortality in Africa\",\n    x = \"Infant Mortality (deaths per 1,000)\",\n    y = \"Life Expectancy (years)\"\n  ) + #Add plot title and axis labels\n  theme_minimal() #Clean theme\n\nWarning: Removed 226 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n#Plot life expectancy as a function of population\nggplot(africa2, aes(x = log(population), y = life_expectancy)) +\n  geom_point() + #Make it a scatter plot\n  labs(\n    title = \"Life expectancy vs. log population in Africa\",\n    x = \"Log Population\",\n    y = \"Life Expectancy (years)\"\n  ) + #Add plot title and axis labels\n  theme_minimal() #Clean theme\n\nWarning: Removed 51 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThe plot of life expectancy vs. infant mortality shows a slight negative association. The plot of life expectancy vs. log population shows a strong positive association between population size and life expectancy. The data displayed in both graphs, especially in the second graph shows individualized trends in life expectancy. This is likely because data for every year included in the dataset is included in the plot, showing individual trends within countries as well as overall trends in the continent.\n\n#Further data processing\n\n#Find the year with the most data (non-NA values) for infant mortality\nmost_data_year &lt;- africadata %&gt;% #Make a copy of the africa data\n  group_by(year) %&gt;% #Group by year\n  summarize(non_na_count = sum(!is.na(infant_mortality))) %&gt;% #Count non-NA values, summarize in new column\n  arrange(desc(non_na_count)) #Sort non_na_count in descending order\n\n#Display year with most data\nprint(most_data_year)\n\n# A tibble: 57 × 2\n    year non_na_count\n   &lt;int&gt;        &lt;int&gt;\n 1  1982           51\n 2  1983           51\n 3  1984           51\n 4  1985           51\n 5  1986           51\n 6  1987           51\n 7  1988           51\n 8  1989           51\n 9  1990           51\n10  1991           51\n# ℹ 47 more rows\n\n\nBy inspecting the number of available infant mortality data points by year, complete data is available from 1982-2015. Years before and after that period have at least one missing data point. For further analysis, I will focus on the year 2000.\n\n#Filter africa data to only include the year 2000\nafricadata_2000 &lt;- africadata %&gt;%\n  filter(year == 2000)\n\n#Plot life expectancy as a function of infant mortality in the year 2000\nggplot(africadata_2000, aes(x = infant_mortality, y = life_expectancy)) + \n  geom_point() + #Make it a scatter plot\n  labs(\n    title = \"Life expectancy vs. infant mortality in Africa in 2000\",\n    x = \"Infant Mortality (deaths per 1,000)\",\n    y = \"Life Expectancy (years)\"\n  ) + #Add plot title and axis labels\n  theme_minimal() #Clean theme\n\n\n\n\n\n\n\n#Plot life expectancy as a function of population\nggplot(africadata_2000, aes(x = log(population), y = life_expectancy)) +\n  geom_point() + #Make it a scatter plot\n  labs(\n    title = \"Life expectancy vs. log population in Africa in 2000\",\n    x = \"Log Population\",\n    y = \"Life Expectancy (years)\"\n  ) + #Add plot title and axis labels\n  theme_minimal() #Clean theme\n\n\n\n\n\n\n\n\nAfter plotting only the data from 2000, a negative association was still seen between life expectancy and infant mortality. However, no clear association can be seen between population size and infant mortality using only data from 2000.\n\n#Regression modeling\n\n#Fit a linear model predicting life expectancy from infant mortality with data from 2000\nlm1 &lt;- lm(life_expectancy ~ infant_mortality, africadata_2000)\nsummary(lm1) #Present summary statistics of the linear model\n\n\nCall:\nlm(formula = life_expectancy ~ infant_mortality, data = africadata_2000)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.6651  -3.7087   0.9914   4.0408   8.6817 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      71.29331    2.42611  29.386  &lt; 2e-16 ***\ninfant_mortality -0.18916    0.02869  -6.594 2.83e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.221 on 49 degrees of freedom\nMultiple R-squared:  0.4701,    Adjusted R-squared:  0.4593 \nF-statistic: 43.48 on 1 and 49 DF,  p-value: 2.826e-08\n\n#Fit a linear model predicting life expectancy from log population with data from 2000\nlm2 &lt;- lm(life_expectancy ~ log(population), africadata_2000)\nsummary(lm2) #Present summary statistics of the linear model\n\n\nCall:\nlm(formula = life_expectancy ~ log(population), data = africadata_2000)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.113  -4.809  -1.554   3.907  18.863 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      65.3243    12.5203   5.217 3.65e-06 ***\nlog(population)  -0.5711     0.7943  -0.719    0.476    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.502 on 49 degrees of freedom\nMultiple R-squared:  0.01044,   Adjusted R-squared:  -0.009755 \nF-statistic: 0.517 on 1 and 49 DF,  p-value: 0.4755\n\n\nFrom the linear model fitting life expectancy from infant mortality, I can conclude that infant mortality is a significant predictor of life expectancy (t = -6.594, p &lt; 0.001). From the linear model fitting life expectancy from log population size, I cannot conclude that population size is a significant predictor of life expectancy (t = -0.719, p = 0.476)."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#exploratory-analysis-of-the-gapminder-dataset",
    "href": "coding-exercise/coding-exercise.html#exploratory-analysis-of-the-gapminder-dataset",
    "title": "R Coding Exercise",
    "section": "",
    "text": "#Load the dslabs package to get the gapminder dataset and other necessary packages\nlibrary(dslabs)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(tidyr)\n\n\n#Exploratory analysis of the gapminder dataset\n\n#Use the help method to get a data description\nhelp(\"gapminder\")\n\n#Get an overview of data structure\nstr(gapminder)\n\n'data.frame':   10545 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  115.4 148.2 208 NA 59.9 ...\n $ life_expectancy : num  62.9 47.5 36 63 65.4 ...\n $ fertility       : num  6.19 7.65 7.32 4.43 3.11 4.55 4.82 3.45 2.7 5.57 ...\n $ population      : num  1636054 11124892 5270844 54681 20619075 ...\n $ gdp             : num  NA 1.38e+10 NA NA 1.08e+11 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 4 1 1 2 2 3 2 5 4 3 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 19 11 10 2 15 21 2 1 22 21 ...\n\n#Get a data summary\nsummary(gapminder)\n\n                country           year      infant_mortality life_expectancy\n Albania            :   57   Min.   :1960   Min.   :  1.50   Min.   :13.20  \n Algeria            :   57   1st Qu.:1974   1st Qu.: 16.00   1st Qu.:57.50  \n Angola             :   57   Median :1988   Median : 41.50   Median :67.54  \n Antigua and Barbuda:   57   Mean   :1988   Mean   : 55.31   Mean   :64.81  \n Argentina          :   57   3rd Qu.:2002   3rd Qu.: 85.10   3rd Qu.:73.00  \n Armenia            :   57   Max.   :2016   Max.   :276.90   Max.   :83.90  \n (Other)            :10203                  NA's   :1453                    \n   fertility       population             gdp               continent   \n Min.   :0.840   Min.   :3.124e+04   Min.   :4.040e+07   Africa  :2907  \n 1st Qu.:2.200   1st Qu.:1.333e+06   1st Qu.:1.846e+09   Americas:2052  \n Median :3.750   Median :5.009e+06   Median :7.794e+09   Asia    :2679  \n Mean   :4.084   Mean   :2.701e+07   Mean   :1.480e+11   Europe  :2223  \n 3rd Qu.:6.000   3rd Qu.:1.523e+07   3rd Qu.:5.540e+10   Oceania : 684  \n Max.   :9.220   Max.   :1.376e+09   Max.   :1.174e+13                  \n NA's   :187     NA's   :185         NA's   :2972                       \n             region    \n Western Asia   :1026  \n Eastern Africa : 912  \n Western Africa : 912  \n Caribbean      : 741  \n South America  : 684  \n Southern Europe: 684  \n (Other)        :5586  \n\n#Determine the type of object that gapminder is\nclass(gapminder)\n\n[1] \"data.frame\"\n\n\n\n#Data processing\n\n#Create a subset of the data just containing countries from Africa\nafricadata &lt;- gapminder %&gt;%\n  filter(continent == \"Africa\")\n\n#Examine the structure of the Africa-specific data\nstr(africadata)\n\n'data.frame':   2907 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n $ fertility       : num  7.65 7.32 6.28 6.62 6.29 6.95 5.65 6.89 5.84 6.25 ...\n $ population      : num  11124892 5270844 2431620 524029 4829291 ...\n $ gdp             : num  1.38e+10 NA 6.22e+08 1.24e+08 5.97e+08 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\nsummary(africadata)\n\n         country          year      infant_mortality life_expectancy\n Algeria     :  57   Min.   :1960   Min.   : 11.40   Min.   :13.20  \n Angola      :  57   1st Qu.:1974   1st Qu.: 62.20   1st Qu.:48.23  \n Benin       :  57   Median :1988   Median : 93.40   Median :53.98  \n Botswana    :  57   Mean   :1988   Mean   : 95.12   Mean   :54.38  \n Burkina Faso:  57   3rd Qu.:2002   3rd Qu.:124.70   3rd Qu.:60.10  \n Burundi     :  57   Max.   :2016   Max.   :237.40   Max.   :77.60  \n (Other)     :2565                  NA's   :226                     \n   fertility       population             gdp               continent   \n Min.   :1.500   Min.   :    41538   Min.   :4.659e+07   Africa  :2907  \n 1st Qu.:5.160   1st Qu.:  1605232   1st Qu.:8.373e+08   Americas:   0  \n Median :6.160   Median :  5570982   Median :2.448e+09   Asia    :   0  \n Mean   :5.851   Mean   : 12235961   Mean   :9.346e+09   Europe  :   0  \n 3rd Qu.:6.860   3rd Qu.: 13888152   3rd Qu.:6.552e+09   Oceania :   0  \n Max.   :8.450   Max.   :182201962   Max.   :1.935e+11                  \n NA's   :51      NA's   :51          NA's   :637                        \n                       region   \n Eastern Africa           :912  \n Western Africa           :912  \n Middle Africa            :456  \n Northern Africa          :342  \n Southern Africa          :285  \n Australia and New Zealand:  0  \n (Other)                  :  0  \n\n#Create a new subset of the africa data only containing data on infant mortality and life expectancy\nafrica1 &lt;- africadata %&gt;%\n  select(c('infant_mortality', 'life_expectancy'))\n\n#Examine the structure of the first subset\nstr(africa1)\n\n'data.frame':   2907 obs. of  2 variables:\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n\nsummary(africa1)\n\n infant_mortality life_expectancy\n Min.   : 11.40   Min.   :13.20  \n 1st Qu.: 62.20   1st Qu.:48.23  \n Median : 93.40   Median :53.98  \n Mean   : 95.12   Mean   :54.38  \n 3rd Qu.:124.70   3rd Qu.:60.10  \n Max.   :237.40   Max.   :77.60  \n NA's   :226                     \n\n#Create a new subset of the africa data only containing data on population and life expectancy\nafrica2 &lt;- africadata %&gt;%\n  select(c('population', 'life_expectancy'))\n\n#Examine the structure of the second subset\nstr(africa2)\n\n'data.frame':   2907 obs. of  2 variables:\n $ population     : num  11124892 5270844 2431620 524029 4829291 ...\n $ life_expectancy: num  47.5 36 38.3 50.3 35.2 ...\n\nsummary(africa2)\n\n   population        life_expectancy\n Min.   :    41538   Min.   :13.20  \n 1st Qu.:  1605232   1st Qu.:48.23  \n Median :  5570982   Median :53.98  \n Mean   : 12235961   Mean   :54.38  \n 3rd Qu.: 13888152   3rd Qu.:60.10  \n Max.   :182201962   Max.   :77.60  \n NA's   :51                         \n\n\n\n#Data plotting\n\n#Plot life expectancy as a function of infant mortality\nggplot(africa1, aes(x = infant_mortality, y = life_expectancy)) + \n  geom_point() + #Make it a scatter plot\n  labs(\n    title = \"Life expectancy vs. infant mortality in Africa\",\n    x = \"Infant Mortality (deaths per 1,000)\",\n    y = \"Life Expectancy (years)\"\n  ) + #Add plot title and axis labels\n  theme_minimal() #Clean theme\n\nWarning: Removed 226 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n#Plot life expectancy as a function of population\nggplot(africa2, aes(x = log(population), y = life_expectancy)) +\n  geom_point() + #Make it a scatter plot\n  labs(\n    title = \"Life expectancy vs. log population in Africa\",\n    x = \"Log Population\",\n    y = \"Life Expectancy (years)\"\n  ) + #Add plot title and axis labels\n  theme_minimal() #Clean theme\n\nWarning: Removed 51 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThe plot of life expectancy vs. infant mortality shows a slight negative association. The plot of life expectancy vs. log population shows a strong positive association between population size and life expectancy. The data displayed in both graphs, especially in the second graph shows individualized trends in life expectancy. This is likely because data for every year included in the dataset is included in the plot, showing individual trends within countries as well as overall trends in the continent.\n\n#Further data processing\n\n#Find the year with the most data (non-NA values) for infant mortality\nmost_data_year &lt;- africadata %&gt;% #Make a copy of the africa data\n  group_by(year) %&gt;% #Group by year\n  summarize(non_na_count = sum(!is.na(infant_mortality))) %&gt;% #Count non-NA values, summarize in new column\n  arrange(desc(non_na_count)) #Sort non_na_count in descending order\n\n#Display year with most data\nprint(most_data_year)\n\n# A tibble: 57 × 2\n    year non_na_count\n   &lt;int&gt;        &lt;int&gt;\n 1  1982           51\n 2  1983           51\n 3  1984           51\n 4  1985           51\n 5  1986           51\n 6  1987           51\n 7  1988           51\n 8  1989           51\n 9  1990           51\n10  1991           51\n# ℹ 47 more rows\n\n\nBy inspecting the number of available infant mortality data points by year, complete data is available from 1982-2015. Years before and after that period have at least one missing data point. For further analysis, I will focus on the year 2000.\n\n#Filter africa data to only include the year 2000\nafricadata_2000 &lt;- africadata %&gt;%\n  filter(year == 2000)\n\n#Plot life expectancy as a function of infant mortality in the year 2000\nggplot(africadata_2000, aes(x = infant_mortality, y = life_expectancy)) + \n  geom_point() + #Make it a scatter plot\n  labs(\n    title = \"Life expectancy vs. infant mortality in Africa in 2000\",\n    x = \"Infant Mortality (deaths per 1,000)\",\n    y = \"Life Expectancy (years)\"\n  ) + #Add plot title and axis labels\n  theme_minimal() #Clean theme\n\n\n\n\n\n\n\n#Plot life expectancy as a function of population\nggplot(africadata_2000, aes(x = log(population), y = life_expectancy)) +\n  geom_point() + #Make it a scatter plot\n  labs(\n    title = \"Life expectancy vs. log population in Africa in 2000\",\n    x = \"Log Population\",\n    y = \"Life Expectancy (years)\"\n  ) + #Add plot title and axis labels\n  theme_minimal() #Clean theme\n\n\n\n\n\n\n\n\nAfter plotting only the data from 2000, a negative association was still seen between life expectancy and infant mortality. However, no clear association can be seen between population size and infant mortality using only data from 2000.\n\n#Regression modeling\n\n#Fit a linear model predicting life expectancy from infant mortality with data from 2000\nlm1 &lt;- lm(life_expectancy ~ infant_mortality, africadata_2000)\nsummary(lm1) #Present summary statistics of the linear model\n\n\nCall:\nlm(formula = life_expectancy ~ infant_mortality, data = africadata_2000)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.6651  -3.7087   0.9914   4.0408   8.6817 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      71.29331    2.42611  29.386  &lt; 2e-16 ***\ninfant_mortality -0.18916    0.02869  -6.594 2.83e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.221 on 49 degrees of freedom\nMultiple R-squared:  0.4701,    Adjusted R-squared:  0.4593 \nF-statistic: 43.48 on 1 and 49 DF,  p-value: 2.826e-08\n\n#Fit a linear model predicting life expectancy from log population with data from 2000\nlm2 &lt;- lm(life_expectancy ~ log(population), africadata_2000)\nsummary(lm2) #Present summary statistics of the linear model\n\n\nCall:\nlm(formula = life_expectancy ~ log(population), data = africadata_2000)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.113  -4.809  -1.554   3.907  18.863 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      65.3243    12.5203   5.217 3.65e-06 ***\nlog(population)  -0.5711     0.7943  -0.719    0.476    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.502 on 49 degrees of freedom\nMultiple R-squared:  0.01044,   Adjusted R-squared:  -0.009755 \nF-statistic: 0.517 on 1 and 49 DF,  p-value: 0.4755\n\n\nFrom the linear model fitting life expectancy from infant mortality, I can conclude that infant mortality is a significant predictor of life expectancy (t = -6.594, p &lt; 0.001). From the linear model fitting life expectancy from log population size, I cannot conclude that population size is a significant predictor of life expectancy (t = -0.719, p = 0.476)."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#exploratory-analysis-of-the-breast-cancer-wisconsin-diagnostic-dataset-from-uci-machine-learning-repository-dataset",
    "href": "coding-exercise/coding-exercise.html#exploratory-analysis-of-the-breast-cancer-wisconsin-diagnostic-dataset-from-uci-machine-learning-repository-dataset",
    "title": "R Coding Exercise",
    "section": "Exploratory analysis of the Breast Cancer Wisconsin Diagnostic Dataset from UCI Machine Learning Repository dataset",
    "text": "Exploratory analysis of the Breast Cancer Wisconsin Diagnostic Dataset from UCI Machine Learning Repository dataset\nThis part is contributed by Hope Grismer.\n\n#Use the help method to get a data description\nhelp(brca)\n\n#Get an overview of data structure\nstr(brca)\n\nList of 2\n $ x: num [1:569, 1:30] 13.5 13.1 9.5 13 8.2 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : NULL\n  .. ..$ : chr [1:30] \"radius_mean\" \"texture_mean\" \"perimeter_mean\" \"area_mean\" ...\n $ y: Factor w/ 2 levels \"B\",\"M\": 1 1 1 1 1 1 1 1 1 1 ...\n\n#Get a data summary\nsummary(brca)\n\n  Length Class  Mode   \nx 17070  -none- numeric\ny   569  factor numeric\n\n#Determine the type of object that death probability data set is\nclass(brca) \n\n[1] \"list\"\n\n\n\n#Data processing of brca data to only include the associated data of the malignant tumors\n\n#Create a subset of the data just malignant tumors, used this syntax instead of filter as it is a list not a data.frame\n\n# Extract indices of malignant tumors\nmalignant_indices &lt;- which(brca$y == \"M\")\n\n# Subset the features (x) based on the indices\nmalignant_tumors_features &lt;- brca$x[malignant_indices, ] \n\n# View the first few rows of the malignant tumors' features\nhead(malignant_tumors_features)\n\n     radius_mean texture_mean perimeter_mean area_mean smoothness_mean\n[1,]       17.99        10.38         122.80    1001.0         0.11840\n[2,]       20.57        17.77         132.90    1326.0         0.08474\n[3,]       19.69        21.25         130.00    1203.0         0.10960\n[4,]       11.42        20.38          77.58     386.1         0.14250\n[5,]       20.29        14.34         135.10    1297.0         0.10030\n[6,]       12.45        15.70          82.57     477.1         0.12780\n     compactness_mean concavity_mean concave_pts_mean symmetry_mean\n[1,]          0.27760         0.3001          0.14710        0.2419\n[2,]          0.07864         0.0869          0.07017        0.1812\n[3,]          0.15990         0.1974          0.12790        0.2069\n[4,]          0.28390         0.2414          0.10520        0.2597\n[5,]          0.13280         0.1980          0.10430        0.1809\n[6,]          0.17000         0.1578          0.08089        0.2087\n     fractal_dim_mean radius_se texture_se perimeter_se area_se smoothness_se\n[1,]          0.07871    1.0950     0.9053        8.589  153.40      0.006399\n[2,]          0.05667    0.5435     0.7339        3.398   74.08      0.005225\n[3,]          0.05999    0.7456     0.7869        4.585   94.03      0.006150\n[4,]          0.09744    0.4956     1.1560        3.445   27.23      0.009110\n[5,]          0.05883    0.7572     0.7813        5.438   94.44      0.011490\n[6,]          0.07613    0.3345     0.8902        2.217   27.19      0.007510\n     compactness_se concavity_se concave_pts_se symmetry_se fractal_dim_se\n[1,]        0.04904      0.05373        0.01587     0.03003       0.006193\n[2,]        0.01308      0.01860        0.01340     0.01389       0.003532\n[3,]        0.04006      0.03832        0.02058     0.02250       0.004571\n[4,]        0.07458      0.05661        0.01867     0.05963       0.009208\n[5,]        0.02461      0.05688        0.01885     0.01756       0.005115\n[6,]        0.03345      0.03672        0.01137     0.02165       0.005082\n     radius_worst texture_worst perimeter_worst area_worst smoothness_worst\n[1,]        25.38         17.33          184.60     2019.0           0.1622\n[2,]        24.99         23.41          158.80     1956.0           0.1238\n[3,]        23.57         25.53          152.50     1709.0           0.1444\n[4,]        14.91         26.50           98.87      567.7           0.2098\n[5,]        22.54         16.67          152.20     1575.0           0.1374\n[6,]        15.47         23.75          103.40      741.6           0.1791\n     compactness_worst concavity_worst concave_pts_worst symmetry_worst\n[1,]            0.6656          0.7119            0.2654         0.4601\n[2,]            0.1866          0.2416            0.1860         0.2750\n[3,]            0.4245          0.4504            0.2430         0.3613\n[4,]            0.8663          0.6869            0.2575         0.6638\n[5,]            0.2050          0.4000            0.1625         0.2364\n[6,]            0.5249          0.5355            0.1741         0.3985\n     fractal_dim_worst\n[1,]           0.11890\n[2,]           0.08902\n[3,]           0.08758\n[4,]           0.17300\n[5,]           0.07678\n[6,]           0.12440\n\n\n\n# Creating new objects to store the symmetry mean and radius mean data for the malignant tumors\nsymmetry_mean_malignant &lt;- malignant_tumors_features[, \"symmetry_mean\"]\nradius_mean_malignant &lt;- malignant_tumors_features[, \"radius_mean\"]\n\n# View the first few rows of the extracted variables\nhead(symmetry_mean_malignant)\n\n[1] 0.2419 0.1812 0.2069 0.2597 0.1809 0.2087\n\nhead(radius_mean_malignant)\n\n[1] 17.99 20.57 19.69 11.42 20.29 12.45\n\n\n\n# Create a data frame for plotting\nmalignant_tumor_data &lt;- data.frame(\n  radius_mean = radius_mean_malignant,\n  symmetry_mean = symmetry_mean_malignant\n)\n\n# Create the scatter plot\nggplot(malignant_tumor_data, aes(x = radius_mean, y = symmetry_mean)) +\n  geom_point() +\n  labs(\n    x = \"Radius Mean\",\n    y = \"Symmetry Mean\",\n    title = \"Symmetry Mean vs. Radius Mean for Malignant Tumors\"\n  )\n\n\n\n\n\n\n\n\n\n# Creating new objects to store the nucleus texture mean (standard deviation of grayscale values) and smoothness mean data for the malignant tumors \ntexture_mean_malignant &lt;- malignant_tumors_features[, \"texture_mean\"]\nsmoothness_mean_malignant &lt;- malignant_tumors_features[, \"smoothness_mean\"]\n\n\n# Create a data frame for plotting\nmalignant_tumor_data &lt;- data.frame(\n  texture_mean = texture_mean_malignant,\n  smoothness_mean = smoothness_mean_malignant\n)\n\n# Create the scatter plot\nggplot(malignant_tumor_data, aes(x = texture_mean, y = smoothness_mean)) +\n  geom_point() +\n  labs(\n    x = \"Texture Mean\",\n    y = \"Smoothness Mean\",\n    title = \"Texture Mean vs. Smoothness Mean for Malignant Tumors\"\n  )\n\n\n\n\n\n\n\n\n\n#Regression modeling for the brca data set\n\n#Fit a linear model predicting the mean nuclear symmetry from the mean nuclear radius of the malignant tumors  \nlm3 &lt;- lm(symmetry_mean_malignant ~ radius_mean_malignant, data = malignant_tumor_data) \n\nsummary(lm3) #Present summary statistics of the linear model\n\n\nCall:\nlm(formula = symmetry_mean_malignant ~ radius_mean_malignant, \n    data = malignant_tumor_data)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.062137 -0.018015 -0.003442  0.016858  0.110878 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            0.2044241  0.0105368  19.401   &lt;2e-16 ***\nradius_mean_malignant -0.0006594  0.0005935  -1.111    0.268    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02762 on 210 degrees of freedom\nMultiple R-squared:  0.005843,  Adjusted R-squared:  0.001109 \nF-statistic: 1.234 on 1 and 210 DF,  p-value: 0.2678\n\n#Fit a linear model predicting nuclear texture in the form of standard deviation of grayscale from the nuclear smoothness (local variation in radius lengths)\n\nlm4 &lt;- lm(texture_mean_malignant ~ smoothness_mean_malignant, data = malignant_tumor_data)\n\n#Present summary statistics of the linear model\nsummary(lm4) \n\n\nCall:\nlm(formula = texture_mean_malignant ~ smoothness_mean_malignant, \n    data = malignant_tumor_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.5561  -2.3959  -0.1349   1.9000  17.4689 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                 26.044      2.122  12.273   &lt;2e-16 ***\nsmoothness_mean_malignant  -43.142     20.470  -2.108   0.0363 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.749 on 210 degrees of freedom\nMultiple R-squared:  0.02071,   Adjusted R-squared:  0.01605 \nF-statistic: 4.442 on 1 and 210 DF,  p-value: 0.03626\n\n\nFrom the linear model fitting mean nuclear symmetry from from the mean nuclear radius of the malignant tumors, I cannot conclude that mean nuclear symmetry is a significant predictor of mean nuclear radius (p-value: 0.2678). From the linear model fitting nuclear texture in the form of standard deviation of grayscale from the nuclear smoothness, I can conclude that nuclear texture is a significant predictor of nuclear smoothness (p-value: 0.03626)"
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About me",
    "section": "",
    "text": "My name is Connor Norris, and I am in my second year of the Master of Public Health program at UGA with a concentration in Epidemiology.\n\n\n\nPhoto credit: Morgan Wynn (Instagram: @morganwynnmedia)\n\n\n\n\nI’ve lived in Georgia all my life, originally growing up in Marietta, GA, and now living in Athens. I attended UGA for my undergraduate years and graduated with a Bachelor’s of Science in Microbiology in May 2024. Outside of school, most of my hobbies are music-related. For example, I’ve played the trombone in the Redcoat Marching Band all five years that I’ve attended UGA, and I’ve played trombone for twelve years in total. I also enjoy cooking when I have the time to do so, and one of my goals in 2025 is to venture more into baking!\n\n\n\nMy research interests are broadly related to studying bacterial foodborne pathogens in agriculture. In my first semester at UGA, I was introduced to the idea of One Health, which suggests that human health is closely tied to the health of animals and of the environment, and my research experiences have been related to that view. I currently work in the Shariat Lab, which mostly studies Salmonella population dynamics in food animal agriculture and in the environment. I, however, work with Campylobacter, a similar pathogen that is commonly found in livestock and poultry.\nMuch of my work in the Shariat Lab is related to genomics and phylogenetics, the studies of the complete genetic content of an organism and of the evolutionary relationships between organisms, respectively. The figure below shows an example of such analysis:\n\n\n\nCitation: Richards, A. K., Kue, S., Norris, C. G., & Shariat, N. W. (2023). Genomic and phenotypic characterization of Salmonella enterica serovar Kentucky. Microbial genomics, 9(9), 001089. https://doi.org/10.1099/mgen.0.001089\n\n\nThe part on the far left is called a phylogenetic tree. It shows the relatedness of several bacterial isolates from different lineages of Salmonella enterica serovar Kentucky, one kind of Salmonella. Such analysis compares the differences between common regions of the genomes of these isolates, and isolates that are closer together on the tree are more closely related to each other evolutionarily. For example, in this tree, the isolates from the Group 1 of serovar Kentucky (in blue) are most closely related to the isolate from Group 4 (in lime green).\n\n\n\nMost of my programming skills have been self taught. I started at UGA with a Computer Science minor, but I quickly had to drop it due to scheduling issues. Since then, I’ve been able to gain programming experience through my research experience, with most of my data analysis work being done in R and with software that’s accessed through a command terminal. As far as biostatistics goes, I wasn’t able to learn much from my microbiology major. However, I’ve been fortunate to take multiple biostatisics classes (BIOS 7010, 7020, and 8010) at UGA since starting my masters.\nIn taking this course, I hope to expand upon the skills I already have with some formal education. I also hope to develop a final product that I could use to demonstrate my skills to potential employers as I begin the job hunt this year."
  },
  {
    "objectID": "aboutme.html#background",
    "href": "aboutme.html#background",
    "title": "About me",
    "section": "",
    "text": "I’ve lived in Georgia all my life, originally growing up in Marietta, GA, and now living in Athens. I attended UGA for my undergraduate years and graduated with a Bachelor’s of Science in Microbiology in May 2024. Outside of school, most of my hobbies are music-related. For example, I’ve played the trombone in the Redcoat Marching Band all five years that I’ve attended UGA, and I’ve played trombone for twelve years in total. I also enjoy cooking when I have the time to do so, and one of my goals in 2025 is to venture more into baking!"
  },
  {
    "objectID": "aboutme.html#research-interests",
    "href": "aboutme.html#research-interests",
    "title": "About me",
    "section": "",
    "text": "My research interests are broadly related to studying bacterial foodborne pathogens in agriculture. In my first semester at UGA, I was introduced to the idea of One Health, which suggests that human health is closely tied to the health of animals and of the environment, and my research experiences have been related to that view. I currently work in the Shariat Lab, which mostly studies Salmonella population dynamics in food animal agriculture and in the environment. I, however, work with Campylobacter, a similar pathogen that is commonly found in livestock and poultry.\nMuch of my work in the Shariat Lab is related to genomics and phylogenetics, the studies of the complete genetic content of an organism and of the evolutionary relationships between organisms, respectively. The figure below shows an example of such analysis:\n\n\n\nCitation: Richards, A. K., Kue, S., Norris, C. G., & Shariat, N. W. (2023). Genomic and phenotypic characterization of Salmonella enterica serovar Kentucky. Microbial genomics, 9(9), 001089. https://doi.org/10.1099/mgen.0.001089\n\n\nThe part on the far left is called a phylogenetic tree. It shows the relatedness of several bacterial isolates from different lineages of Salmonella enterica serovar Kentucky, one kind of Salmonella. Such analysis compares the differences between common regions of the genomes of these isolates, and isolates that are closer together on the tree are more closely related to each other evolutionarily. For example, in this tree, the isolates from the Group 1 of serovar Kentucky (in blue) are most closely related to the isolate from Group 4 (in lime green)."
  },
  {
    "objectID": "aboutme.html#programming-and-biostatistics-experience",
    "href": "aboutme.html#programming-and-biostatistics-experience",
    "title": "About me",
    "section": "",
    "text": "Most of my programming skills have been self taught. I started at UGA with a Computer Science minor, but I quickly had to drop it due to scheduling issues. Since then, I’ve been able to gain programming experience through my research experience, with most of my data analysis work being done in R and with software that’s accessed through a command terminal. As far as biostatistics goes, I wasn’t able to learn much from my microbiology major. However, I’ve been fortunate to take multiple biostatisics classes (BIOS 7010, 7020, and 8010) at UGA since starting my masters.\nIn taking this course, I hope to expand upon the skills I already have with some formal education. I also hope to develop a final product that I could use to demonstrate my skills to potential employers as I begin the job hunt this year."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html",
    "href": "presentation-exercise/presentation-exercise.html",
    "title": "Presentation Exercise",
    "section": "",
    "text": "For this exercise, I will find publicly available data from news sources and try to replicate one of their figures or tables. This article from ABC News 538 discusses trends in the reasons behind members of congress’ resignations, and can be found at the following link.\n\n\nThe figure from this article I have chosen to replicate is shown below: \nThe figure is a stacked bar graph showing the distribution of reasons for congressional resignations in each term from the 57th Congress to the 115th Congress in years ranging from 1901 to 2018.\n\n#Load required packages\nlibrary(here)\n\nhere() starts at /Users/cgnorris/Documents/GitHub/MADA (EPID 8060E)/connornorris-MADA-portfolio\n\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.4     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(grid)\n\n\n#Load data\ndata_path &lt;- here(\"presentation-exercise\", \"congressional_resignations.csv\")\ndata &lt;- read_csv(data_path)\n\nRows: 615 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Member, Party, District, Congress, Resignation Date, Reason, Source...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#Get data structure and summary\nstr(data)\n\nspc_tbl_ [615 × 8] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Member          : chr [1:615] \"Pat Tiberi\" \"Al Franken\" \"Trent Franks\" \"John Conyers\" ...\n $ Party           : chr [1:615] \"R\" \"D\" \"R\" \"D\" ...\n $ District        : chr [1:615] \"OH-12\" \"MN-SEN\" \"AZ-08\" \"MI-13\" ...\n $ Congress        : chr [1:615] \"115th\" \"115th\" \"115th\" \"115th\" ...\n $ Resignation Date: chr [1:615] \"1/15/2018\" \"1/2/2018\" \"12/8/2017\" \"12/5/2017\" ...\n $ Reason          : chr [1:615] \"Took a job with the Ohio Business Roundtable\" \"Sexual harassment/groping/unwanted kissing\" \"Sexual harassment/surrogacy pressure\" \"Sexual harassment/inappropriate touching\" ...\n $ Source          : chr [1:615] \"New York Times\" \"Minneapolis Star Tribune\" \"CNN\" \"Washington Post\" ...\n $ Category        : chr [1:615] \"D\" \"X\" \"X\" \"X\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Member = col_character(),\n  ..   Party = col_character(),\n  ..   District = col_character(),\n  ..   Congress = col_character(),\n  ..   `Resignation Date` = col_character(),\n  ..   Reason = col_character(),\n  ..   Source = col_character(),\n  ..   Category = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nsummary(data)\n\n    Member             Party             District           Congress        \n Length:615         Length:615         Length:615         Length:615        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n Resignation Date      Reason             Source            Category        \n Length:615         Length:615         Length:615         Length:615        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n\n\nThe following code for reproducing the figure was generated by ChatGPT following the prompt, “Using the data in the attached csv file, write code in R to reproduce the attached graph.” A png screenshot of the figure and the data in a csv file were provided as input along with the prompt.\n\ndf &lt;- data\n\n# Convert Congress to numeric\ndf$Congress &lt;- as.numeric(gsub(\"th|rd|nd|st\", \"\", df$Congress))\n\n# Define category color mapping\ncategory_colors &lt;- c(\n  \"OTHER OFFICE\" = \"#FFC72C\",\n  \"PRIVATE SECTOR\" = \"#007CC3\",\n  \"EARLY DEPARTURE\" = \"#8B5E83\",\n  \"SEXUAL MISCONDUCT\" = \"#E81D2C\",\n  \"ELECTION OVERTURNED\" = \"#1D1D1D\",\n  \"HEALTH/FAMILY\" = \"#509E2F\",\n  \"MILITARY SERVICE\" = \"#A05A2C\",\n  \"CONSENSUAL SEX SCANDALS\" = \"#FFB6C1\",\n  \"OTHER SCANDAL\" = \"#D3D3D3\",\n  \"OTHER REASON\" = \"#BEBEBE\"\n)\n\n# Aggregate data for plotting\ndf_plot &lt;- df %&gt;%\n  count(Congress, Category) %&gt;%\n  filter(!is.na(Congress))\n\n# Create stacked bar plot\nggplot(df_plot, aes(x = Congress, y = n, fill = Category)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = category_colors) +\n  labs(\n    title = \"Congressional resignations over time\",\n    subtitle = \"Number of congressional resignations by session and public reason for resignation since 1901\",\n    x = \"Congress\",\n    y = \"Number of Resignations\",\n    fill = \"Reason\"\n  ) +\n  theme_minimal()\n\nWarning: No shared levels found between `names(values)` of the manual scale and the\ndata's fill values.\nNo shared levels found between `names(values)` of the manual scale and the\ndata's fill values.\n\n\n\n\n\n\n\n\n\nThe bars themselves are correct, but many other elements are missing. The colors aren’t there, the theme doesn’t match the original, and there’s no legend.\n\n#Separate rows with multiple reported reasons for resignation\ndf &lt;- df %&gt;%\n  separate_rows(Category, sep = \" & \") %&gt;%\n  mutate(across(where(is.character), str_trim)) #Remove any possible white space\n\n# Define category code mapping\ndf &lt;- df %&gt;%\n  mutate(Category = case_when(\n    Category == \"X\" ~ \"SEXUAL MISCONDUCT\",\n    Category == \"A\" ~ \"CONSENSUAL SEX SCANDALS\",\n    Category == \"B\" ~ \"OTHER SCANDAL\",\n    Category == \"C\" ~ \"OTHER OFFICE\",\n    Category == \"D\" ~ \"PRIVATE SECTOR\",\n    Category == \"E\" ~ \"HEALTH/FAMILY\",\n    Category == \"F\" ~ \"OTHER REASON\",\n    Category == \"G\" ~ \"EARLY DEPARTURE\",\n    Category == \"H\" ~ \"MILITARY SERVICE\",\n    Category == \"I\" ~ \"ELECTION OVERTURNED\",\n    TRUE ~ NA_character_\n  ))\n\n#Define category color mapping\ncategory_colors &lt;- c(\n  \"OTHER OFFICE\" = \"#F5B902\",\n  \"ELECTION OVERTURNED\" = \"#1D1D1D\",\n  \"PRIVATE SECTOR\" = \"#038FD5\",  \n  \"HEALTH/FAMILY\" = \"#44AB43\",\n  \"EARLY DEPARTURE\" = \"#AE66AB\",  \n  \"MILITARY SERVICE\" = \"#A4522E\",\n  \"SEXUAL MISCONDUCT\" = \"#E81D2C\",\n  \"CONSENSUAL SEX SCANDALS\" = \"#FFB6C1\",\n  \"OTHER SCANDAL\" = \"#FFE3D7\",\n  \"OTHER REASON\" = \"#BEBEBE\"\n)\n\n#Ensure stacking order in the correct sequence\ndf$Category &lt;- factor(df$Category, levels = names(category_colors))\n\n#Aggregate data for plotting\ndf_plot &lt;- df %&gt;%\n  count(Congress, Category) %&gt;%\n  filter(!is.na(Congress))\n\n\n#Replot\nggplot(df_plot, aes(x = Congress, y = n, fill = Category)) +\n  geom_bar(stat = \"identity\", position = position_stack(reverse = TRUE)) + #Specifify stacked bar graph\n  scale_fill_manual(values = category_colors) + #Fill with colors as defined above\n  scale_x_continuous(breaks = seq(60, 115, by = 5), expand = c(0.01, 0.01)) + #X-axis formatting\n  scale_y_continuous(breaks = seq(0, 40, by = 10)) + #Y-axis formatting\n  #Make figure, axis titles\n  labs(\n    title = \"Congressional resignations over time\",\n    subtitle = \"Number of congressional resignations by session and public reason for resignation since 1901\",\n    x = \"Congress\",\n    y = \" \",\n    fill = \"Category\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"), #Format plot title\n    plot.subtitle = element_text(hjust = 0.5), #Format plot subtitle\n    legend.position = \"top\", #Specify legend position\n    legend.title = element_blank(), #No legend title\n    legend.text = element_text(size = 7), #Format legend text\n    legend.key.size = unit(0.3, \"cm\"), #Format legend keys\n    legend.spacing.x = unit(0.5, \"cm\"), #Format legend spacing left to right\n    legend.spacing.y = unit(0.3, \"cm\"), #Format legend spacign top to bottom\n    legend.box = \"horizontal\", #Format legend orientation (vertical vs. horizontal)\n    legend.direction = \"horizontal\", #Format lengend orientation (vertical vs. horizontal)\n    legend.box.just = \"center\", #Format legend justification\n    axis.title.x = element_text(size = 12, face = \"bold\"), #Format x axis title\n    axis.text.x = element_text(size = 10, family = \"Courier New\", color = \"grey40\"), #Format x-axis text\n    axis.text.y = element_text(size = 10, family = \"Courier New\", color = \"grey40\"), #Format y-axis text\n    panel.grid.major.x = element_blank(), #Remove vertical grid lines\n    panel.grid.minor.x = element_blank(), #Remove intermediate vertical grid lines\n    panel.grid.minor.y = element_blank() #Remove intermediate horizontal grid loines\n  ) + \n  guides(fill = guide_legend(ncol = 5, bycol = TRUE)) + #Make legend a 5x2 box\n \n  #Annotations\n  #World War II\n  annotate(\"rect\", xmin = 76.6, xmax = 77.4, ymin = 15, ymax = 19,\n           color = \"black\", fill = NA, size = 0.3) +\n  annotate(\"rect\", xmin = 77.6, xmax = 78.4, ymin = 7, ymax = 15,\n           color = \"black\", fill = NA, size = 0.3) +\n  annotate(\"label\", x = 75, y = 22, label = \"World War II\", hjust = 0, angle = 0,\n           fill = \"white\", label.size = NA, family = \"Arial\", size = 2.5) +\n  annotate(\"curve\", x = 78, xend = 78, y = 21, yend = 17, \n           arrow = arrow(length = unit(0.1, \"cm\"), type = \"closed\"),\n           size = 0.25, curvature = -0.4) +\n  \n  #Changes to pension laws\n  annotate(\"rect\", xmin = 88.6, xmax = 89.4, ymin = 5, ymax = 16, \n           color = \"black\", fill = NA, size = 0.2) +\n  annotate(\"rect\", xmin = 92.6, xmax = 93.4, ymin = 7, ymax = 42, \n           color = \"black\", fill = NA, size = 0.2) +\n  annotate(\"rect\", xmin = 94.6, xmax = 95.4, ymin = 7, ymax = 25, \n           color = \"black\", fill = NA, size = 0.2) +\n  annotate(\"label\", x = 89, y = 21, label = paste(\"Changes to\", \"pension laws\", sep = \"\\n\"),\n           hjust = 0.5, angle = 0, fill = \"white\", label.size = NA, family = \"Arial\", size = 2.5) +\n  annotate(\"curve\", x = 90, xend = 90, y = 18, yend = 14, \n           arrow = arrow(length = unit(0.1, \"cm\"), type = \"closed\"),\n           size = 0.25, curvature = -0.4) +\n  annotate(\"curve\", x = 89.5, xend = 92, y = 23, yend = 28, \n           arrow = arrow(length = unit(0.1, \"cm\"), type = \"closed\"),\n           size = 0.25, curvature = -0.4) +\n  annotate(\"curve\", x = 89.5, xend = 94, y = 23, yend = 24,\n           arrow = arrow(length = unit(0.1, \"cm\"), type = \"closed\"),\n           size = 0.25, curvature = -0.2) +\n  \n  #Abscam scandal\n  annotate(\"rect\", xmin = 95.6, xmax = 96.4, ymin = 6, ymax = 11, \n           color = \"black\", fill = NA, size = 0.2) +\n  annotate(\"rect\", xmin = 96.6, xmax = 97.4, ymin = 5, ymax = 8, \n           color = \"black\", fill = NA, size = 0.2) +\n  annotate(\"label\", x = 97, y = 15, label = paste(\"Abscam\", \"scandal\", sep = \"\\n\"),\n           hjust = 0, angle = 0, fill = \"white\", label.size = NA, family = \"Arial\", size = 2.5) +\n  annotate(\"curve\", x = 99, xend = 97, y = 13, yend = 11, \n           arrow = arrow(length = unit(0.1, \"cm\"), type = \"closed\"),\n           size = 0.25, curvature = -0.4) +\n  annotate(\"curve\", x = 99, xend = 98, y = 13, yend = 9,\n           arrow = arrow(length = unit(0.1, \"cm\"), type = \"closed\"),\n           size = 0.25, curvature = -0.2) +\n  \n  #Abramoff scandal\n  annotate(\"rect\", xmin = 107.6, xmax = 108.4, ymin = 5, ymax = 7, \n           color = \"black\", fill = NA, size = 0.2) +\n  annotate(\"label\", x = 109, y = 13, label = paste(\"Abramoff\", \"scandal\", sep = \"\\n\"),\n           hjust = 1, angle = 0, fill = \"white\", label.size = NA, family = \"Arial\", size = 2.5) +\n  annotate(\"curve\", x = 108, xend = 108, y = 11, yend = 8, \n           arrow = arrow(length = unit(0.1, \"cm\"), type = \"closed\"),\n           size = 0.25, curvature = 0) +\n  \n  ##MeToo\n  annotate(\"rect\", xmin = 114.6, xmax = 115.4, ymin = 8, ymax = 11, \n           color = \"black\", fill = NA, size = 0.2) +\n  annotate(\"label\", x = 116, y = 20, label = \"#MeToo\", hjust = 1, angle = 0, fill = \"white\", \n           label.size = NA, family = \"Arial\", size = 2.5) +\n  annotate(\"curve\", x = 115, xend = 115, y = 19, yend = 13, \n           arrow = arrow(length = unit(0.1, \"cm\"), type = \"closed\"),\n           size = 0.25, curvature = 0) +\n  \n  #y-axis label\n  annotate(\"label\", x = -Inf, y = 40, label = \"resignations\", hjust = 0, angle = 0, \n           fill = \"white\", label.size = NA, family = \"Courier New\", \n           color = \"darkgrey\", size = 3.5)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\n\nIn making a presentation quality table, I will use the same data to create a heat map of the reasons for congressional resignations by state.\n\n#Additional packages\nlibrary(formattable)\nlibrary(gt)\n\n\nAttaching package: 'gt'\n\n\nThe following object is masked from 'package:formattable':\n\n    currency\n\nlibrary(webshot2)\n\n#Split district column\ndf2 &lt;- df %&gt;%\n  separate(District, c(\"State\", \"District\"), sep = \"-\")\n\n#Create a summary table by counts\nsummary_table &lt;- df2 %&gt;%\n  group_by(State, Category) %&gt;%\n  summarise(Count = n()) %&gt;%\n  spread(key = Category, value = Count, fill = 0)\n`summarise()` has grouped output by 'State'. You can override using the\n`.groups` argument.\n#Calculate total resignations by state\nsummary_table &lt;- summary_table %&gt;%\n  ungroup() %&gt;%\n  mutate(across(where(is.numeric), as.numeric)) %&gt;%\n  rowwise() %&gt;%\n  mutate(TOTAL = sum(across(where(is.numeric)), na.rm = TRUE)) %&gt;%\n  ungroup()\n\n#Create gt table\ngt_table &lt;- summary_table %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Congressional Resignations by State and Category\"\n  ) %&gt;%\n  fmt_number(\n    columns = -State,\n    decimals = 0\n  ) %&gt;%\n  cols_label(\n    `OTHER OFFICE` = \"Other Office\",\n    `ELECTION OVERTURNED` = \"Election Over- turned\",\n    `PRIVATE SECTOR` = \"Private Sector\",\n    `HEALTH/FAMILY` = \"Health/ Family\",\n    `EARLY DEPARTURE` = \"Early Departure\",\n    `MILITARY SERVICE` = \"Military Service\",\n    `SEXUAL MISCONDUCT` = \"Sexual Misconduct\",\n    `CONSENSUAL SEX SCANDALS` = \"Consensual Sex Scandals\",\n    `OTHER SCANDAL` = \"Other Scandal\",\n    `OTHER REASON` = \"Other Reason\",\n    `TOTAL` = \"Total\"\n  ) %&gt;%\n  data_color(\n    columns = `OTHER OFFICE`,\n    fn = scales::col_numeric(\n      palette = c(\"white\", \"#F5B902\"),\n      domain = NULL\n    )\n  ) %&gt;%\n  data_color(\n    columns = `ELECTION OVERTURNED`,\n    fn = scales::col_numeric(\n      palette = c(\"white\", \"grey20\"),\n      domain = NULL\n    )\n  ) %&gt;%\n  data_color(\n    columns = `PRIVATE SECTOR`,\n    fn = scales::col_numeric(\n      palette = c(\"white\", \"#038FD5\"),\n      domain = NULL\n    )\n  ) %&gt;%\n  data_color(\n    columns = `HEALTH/FAMILY`,\n    fn = scales::col_numeric(\n      palette = c(\"white\", \"#44AB43\"),\n      domain = NULL\n    )\n  ) %&gt;%\n  data_color(\n    columns = `EARLY DEPARTURE`,\n    fn = scales::col_numeric(\n      palette = c(\"white\", \"#AE66AB\"),\n      domain = NULL\n    )\n  ) %&gt;%\n  data_color(\n    columns = `MILITARY SERVICE`,\n    fn = scales::col_numeric(\n      palette = c(\"white\", \"#A4522E\"),\n      domain = NULL\n    )\n  ) %&gt;%\n  data_color(\n    columns = `SEXUAL MISCONDUCT`,\n    fn = scales::col_numeric(\n      palette = c(\"white\", \"#E81D2C\"),\n      domain = NULL\n    )\n  ) %&gt;%\n  data_color(\n    columns = `CONSENSUAL SEX SCANDALS`,\n    fn = scales::col_numeric(\n      palette = c(\"white\", \"#FFB6C1\"),\n      domain = NULL\n    )\n  ) %&gt;%\n  data_color(\n    columns = `OTHER SCANDAL`,\n    fn = scales::col_numeric(\n      palette = c(\"white\", \"#FFE3D7\"),\n      domain = NULL\n    )\n  ) %&gt;%\n  data_color(\n    columns = `OTHER REASON`,\n    fn = scales::col_numeric(\n      palette = c(\"white\", \"#BEBEBE\"),\n      domain = NULL\n    )\n  ) %&gt;%\n  data_color(\n    columns = `TOTAL`,\n    fn = scales::col_numeric(\n      palette = c(\"white\", \"orange\"),\n      domain = NULL\n    )\n  ) %&gt;%\n  tab_footnote(\n    footnote = \"Colors within each column correspond to the representative colors in the stacked bargraph representation. Darker shades within each column correspond to higher counts.\",\n  )\n\n#Save gt table as an image\ngtsave(gt_table, \"state_resignations_table.png\", expand = 10)\nfile:////var/folders/xc/6xhkp8517gb10rdqk_xm0kp40000gn/T//RtmpYbatW8/filebca622871aef.html screenshot completed\n\nThe resulting table is shown below."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#replicating-a-figure",
    "href": "presentation-exercise/presentation-exercise.html#replicating-a-figure",
    "title": "Presentation Exercise",
    "section": "",
    "text": "The figure from this article I have chosen to replicate is shown below: \nThe figure is a stacked bar graph showing the distribution of reasons for congressional resignations in each term from the 57th Congress to the 115th Congress in years ranging from 1901 to 2018.\n\n#Load required packages\nlibrary(here)\n\nhere() starts at /Users/cgnorris/Documents/GitHub/MADA (EPID 8060E)/connornorris-MADA-portfolio\n\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.4     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(grid)\n\n\n#Load data\ndata_path &lt;- here(\"presentation-exercise\", \"congressional_resignations.csv\")\ndata &lt;- read_csv(data_path)\n\nRows: 615 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Member, Party, District, Congress, Resignation Date, Reason, Source...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#Get data structure and summary\nstr(data)\n\nspc_tbl_ [615 × 8] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Member          : chr [1:615] \"Pat Tiberi\" \"Al Franken\" \"Trent Franks\" \"John Conyers\" ...\n $ Party           : chr [1:615] \"R\" \"D\" \"R\" \"D\" ...\n $ District        : chr [1:615] \"OH-12\" \"MN-SEN\" \"AZ-08\" \"MI-13\" ...\n $ Congress        : chr [1:615] \"115th\" \"115th\" \"115th\" \"115th\" ...\n $ Resignation Date: chr [1:615] \"1/15/2018\" \"1/2/2018\" \"12/8/2017\" \"12/5/2017\" ...\n $ Reason          : chr [1:615] \"Took a job with the Ohio Business Roundtable\" \"Sexual harassment/groping/unwanted kissing\" \"Sexual harassment/surrogacy pressure\" \"Sexual harassment/inappropriate touching\" ...\n $ Source          : chr [1:615] \"New York Times\" \"Minneapolis Star Tribune\" \"CNN\" \"Washington Post\" ...\n $ Category        : chr [1:615] \"D\" \"X\" \"X\" \"X\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Member = col_character(),\n  ..   Party = col_character(),\n  ..   District = col_character(),\n  ..   Congress = col_character(),\n  ..   `Resignation Date` = col_character(),\n  ..   Reason = col_character(),\n  ..   Source = col_character(),\n  ..   Category = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nsummary(data)\n\n    Member             Party             District           Congress        \n Length:615         Length:615         Length:615         Length:615        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n Resignation Date      Reason             Source            Category        \n Length:615         Length:615         Length:615         Length:615        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n\n\nThe following code for reproducing the figure was generated by ChatGPT following the prompt, “Using the data in the attached csv file, write code in R to reproduce the attached graph.” A png screenshot of the figure and the data in a csv file were provided as input along with the prompt.\n\ndf &lt;- data\n\n# Convert Congress to numeric\ndf$Congress &lt;- as.numeric(gsub(\"th|rd|nd|st\", \"\", df$Congress))\n\n# Define category color mapping\ncategory_colors &lt;- c(\n  \"OTHER OFFICE\" = \"#FFC72C\",\n  \"PRIVATE SECTOR\" = \"#007CC3\",\n  \"EARLY DEPARTURE\" = \"#8B5E83\",\n  \"SEXUAL MISCONDUCT\" = \"#E81D2C\",\n  \"ELECTION OVERTURNED\" = \"#1D1D1D\",\n  \"HEALTH/FAMILY\" = \"#509E2F\",\n  \"MILITARY SERVICE\" = \"#A05A2C\",\n  \"CONSENSUAL SEX SCANDALS\" = \"#FFB6C1\",\n  \"OTHER SCANDAL\" = \"#D3D3D3\",\n  \"OTHER REASON\" = \"#BEBEBE\"\n)\n\n# Aggregate data for plotting\ndf_plot &lt;- df %&gt;%\n  count(Congress, Category) %&gt;%\n  filter(!is.na(Congress))\n\n# Create stacked bar plot\nggplot(df_plot, aes(x = Congress, y = n, fill = Category)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = category_colors) +\n  labs(\n    title = \"Congressional resignations over time\",\n    subtitle = \"Number of congressional resignations by session and public reason for resignation since 1901\",\n    x = \"Congress\",\n    y = \"Number of Resignations\",\n    fill = \"Reason\"\n  ) +\n  theme_minimal()\n\nWarning: No shared levels found between `names(values)` of the manual scale and the\ndata's fill values.\nNo shared levels found between `names(values)` of the manual scale and the\ndata's fill values.\n\n\n\n\n\n\n\n\n\nThe bars themselves are correct, but many other elements are missing. The colors aren’t there, the theme doesn’t match the original, and there’s no legend.\n\n#Separate rows with multiple reported reasons for resignation\ndf &lt;- df %&gt;%\n  separate_rows(Category, sep = \" & \") %&gt;%\n  mutate(across(where(is.character), str_trim)) #Remove any possible white space\n\n# Define category code mapping\ndf &lt;- df %&gt;%\n  mutate(Category = case_when(\n    Category == \"X\" ~ \"SEXUAL MISCONDUCT\",\n    Category == \"A\" ~ \"CONSENSUAL SEX SCANDALS\",\n    Category == \"B\" ~ \"OTHER SCANDAL\",\n    Category == \"C\" ~ \"OTHER OFFICE\",\n    Category == \"D\" ~ \"PRIVATE SECTOR\",\n    Category == \"E\" ~ \"HEALTH/FAMILY\",\n    Category == \"F\" ~ \"OTHER REASON\",\n    Category == \"G\" ~ \"EARLY DEPARTURE\",\n    Category == \"H\" ~ \"MILITARY SERVICE\",\n    Category == \"I\" ~ \"ELECTION OVERTURNED\",\n    TRUE ~ NA_character_\n  ))\n\n#Define category color mapping\ncategory_colors &lt;- c(\n  \"OTHER OFFICE\" = \"#F5B902\",\n  \"ELECTION OVERTURNED\" = \"#1D1D1D\",\n  \"PRIVATE SECTOR\" = \"#038FD5\",  \n  \"HEALTH/FAMILY\" = \"#44AB43\",\n  \"EARLY DEPARTURE\" = \"#AE66AB\",  \n  \"MILITARY SERVICE\" = \"#A4522E\",\n  \"SEXUAL MISCONDUCT\" = \"#E81D2C\",\n  \"CONSENSUAL SEX SCANDALS\" = \"#FFB6C1\",\n  \"OTHER SCANDAL\" = \"#FFE3D7\",\n  \"OTHER REASON\" = \"#BEBEBE\"\n)\n\n#Ensure stacking order in the correct sequence\ndf$Category &lt;- factor(df$Category, levels = names(category_colors))\n\n#Aggregate data for plotting\ndf_plot &lt;- df %&gt;%\n  count(Congress, Category) %&gt;%\n  filter(!is.na(Congress))\n\n\n#Replot\nggplot(df_plot, aes(x = Congress, y = n, fill = Category)) +\n  geom_bar(stat = \"identity\", position = position_stack(reverse = TRUE)) + #Specifify stacked bar graph\n  scale_fill_manual(values = category_colors) + #Fill with colors as defined above\n  scale_x_continuous(breaks = seq(60, 115, by = 5), expand = c(0.01, 0.01)) + #X-axis formatting\n  scale_y_continuous(breaks = seq(0, 40, by = 10)) + #Y-axis formatting\n  #Make figure, axis titles\n  labs(\n    title = \"Congressional resignations over time\",\n    subtitle = \"Number of congressional resignations by session and public reason for resignation since 1901\",\n    x = \"Congress\",\n    y = \" \",\n    fill = \"Category\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"), #Format plot title\n    plot.subtitle = element_text(hjust = 0.5), #Format plot subtitle\n    legend.position = \"top\", #Specify legend position\n    legend.title = element_blank(), #No legend title\n    legend.text = element_text(size = 7), #Format legend text\n    legend.key.size = unit(0.3, \"cm\"), #Format legend keys\n    legend.spacing.x = unit(0.5, \"cm\"), #Format legend spacing left to right\n    legend.spacing.y = unit(0.3, \"cm\"), #Format legend spacign top to bottom\n    legend.box = \"horizontal\", #Format legend orientation (vertical vs. horizontal)\n    legend.direction = \"horizontal\", #Format lengend orientation (vertical vs. horizontal)\n    legend.box.just = \"center\", #Format legend justification\n    axis.title.x = element_text(size = 12, face = \"bold\"), #Format x axis title\n    axis.text.x = element_text(size = 10, family = \"Courier New\", color = \"grey40\"), #Format x-axis text\n    axis.text.y = element_text(size = 10, family = \"Courier New\", color = \"grey40\"), #Format y-axis text\n    panel.grid.major.x = element_blank(), #Remove vertical grid lines\n    panel.grid.minor.x = element_blank(), #Remove intermediate vertical grid lines\n    panel.grid.minor.y = element_blank() #Remove intermediate horizontal grid loines\n  ) + \n  guides(fill = guide_legend(ncol = 5, bycol = TRUE)) + #Make legend a 5x2 box\n \n  #Annotations\n  #World War II\n  annotate(\"rect\", xmin = 76.6, xmax = 77.4, ymin = 15, ymax = 19,\n           color = \"black\", fill = NA, size = 0.3) +\n  annotate(\"rect\", xmin = 77.6, xmax = 78.4, ymin = 7, ymax = 15,\n           color = \"black\", fill = NA, size = 0.3) +\n  annotate(\"label\", x = 75, y = 22, label = \"World War II\", hjust = 0, angle = 0,\n           fill = \"white\", label.size = NA, family = \"Arial\", size = 2.5) +\n  annotate(\"curve\", x = 78, xend = 78, y = 21, yend = 17, \n           arrow = arrow(length = unit(0.1, \"cm\"), type = \"closed\"),\n           size = 0.25, curvature = -0.4) +\n  \n  #Changes to pension laws\n  annotate(\"rect\", xmin = 88.6, xmax = 89.4, ymin = 5, ymax = 16, \n           color = \"black\", fill = NA, size = 0.2) +\n  annotate(\"rect\", xmin = 92.6, xmax = 93.4, ymin = 7, ymax = 42, \n           color = \"black\", fill = NA, size = 0.2) +\n  annotate(\"rect\", xmin = 94.6, xmax = 95.4, ymin = 7, ymax = 25, \n           color = \"black\", fill = NA, size = 0.2) +\n  annotate(\"label\", x = 89, y = 21, label = paste(\"Changes to\", \"pension laws\", sep = \"\\n\"),\n           hjust = 0.5, angle = 0, fill = \"white\", label.size = NA, family = \"Arial\", size = 2.5) +\n  annotate(\"curve\", x = 90, xend = 90, y = 18, yend = 14, \n           arrow = arrow(length = unit(0.1, \"cm\"), type = \"closed\"),\n           size = 0.25, curvature = -0.4) +\n  annotate(\"curve\", x = 89.5, xend = 92, y = 23, yend = 28, \n           arrow = arrow(length = unit(0.1, \"cm\"), type = \"closed\"),\n           size = 0.25, curvature = -0.4) +\n  annotate(\"curve\", x = 89.5, xend = 94, y = 23, yend = 24,\n           arrow = arrow(length = unit(0.1, \"cm\"), type = \"closed\"),\n           size = 0.25, curvature = -0.2) +\n  \n  #Abscam scandal\n  annotate(\"rect\", xmin = 95.6, xmax = 96.4, ymin = 6, ymax = 11, \n           color = \"black\", fill = NA, size = 0.2) +\n  annotate(\"rect\", xmin = 96.6, xmax = 97.4, ymin = 5, ymax = 8, \n           color = \"black\", fill = NA, size = 0.2) +\n  annotate(\"label\", x = 97, y = 15, label = paste(\"Abscam\", \"scandal\", sep = \"\\n\"),\n           hjust = 0, angle = 0, fill = \"white\", label.size = NA, family = \"Arial\", size = 2.5) +\n  annotate(\"curve\", x = 99, xend = 97, y = 13, yend = 11, \n           arrow = arrow(length = unit(0.1, \"cm\"), type = \"closed\"),\n           size = 0.25, curvature = -0.4) +\n  annotate(\"curve\", x = 99, xend = 98, y = 13, yend = 9,\n           arrow = arrow(length = unit(0.1, \"cm\"), type = \"closed\"),\n           size = 0.25, curvature = -0.2) +\n  \n  #Abramoff scandal\n  annotate(\"rect\", xmin = 107.6, xmax = 108.4, ymin = 5, ymax = 7, \n           color = \"black\", fill = NA, size = 0.2) +\n  annotate(\"label\", x = 109, y = 13, label = paste(\"Abramoff\", \"scandal\", sep = \"\\n\"),\n           hjust = 1, angle = 0, fill = \"white\", label.size = NA, family = \"Arial\", size = 2.5) +\n  annotate(\"curve\", x = 108, xend = 108, y = 11, yend = 8, \n           arrow = arrow(length = unit(0.1, \"cm\"), type = \"closed\"),\n           size = 0.25, curvature = 0) +\n  \n  ##MeToo\n  annotate(\"rect\", xmin = 114.6, xmax = 115.4, ymin = 8, ymax = 11, \n           color = \"black\", fill = NA, size = 0.2) +\n  annotate(\"label\", x = 116, y = 20, label = \"#MeToo\", hjust = 1, angle = 0, fill = \"white\", \n           label.size = NA, family = \"Arial\", size = 2.5) +\n  annotate(\"curve\", x = 115, xend = 115, y = 19, yend = 13, \n           arrow = arrow(length = unit(0.1, \"cm\"), type = \"closed\"),\n           size = 0.25, curvature = 0) +\n  \n  #y-axis label\n  annotate(\"label\", x = -Inf, y = 40, label = \"resignations\", hjust = 0, angle = 0, \n           fill = \"white\", label.size = NA, family = \"Courier New\", \n           color = \"darkgrey\", size = 3.5)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#making-a-presentation-quality-table",
    "href": "presentation-exercise/presentation-exercise.html#making-a-presentation-quality-table",
    "title": "Presentation Exercise",
    "section": "",
    "text": "In making a presentation quality table, I will use the same data to create a heat map of the reasons for congressional resignations by state.\n\n#Additional packages\nlibrary(formattable)\nlibrary(gt)\n\n\nAttaching package: 'gt'\n\n\nThe following object is masked from 'package:formattable':\n\n    currency\n\nlibrary(webshot2)\n\n#Split district column\ndf2 &lt;- df %&gt;%\n  separate(District, c(\"State\", \"District\"), sep = \"-\")\n\n#Create a summary table by counts\nsummary_table &lt;- df2 %&gt;%\n  group_by(State, Category) %&gt;%\n  summarise(Count = n()) %&gt;%\n  spread(key = Category, value = Count, fill = 0)\n`summarise()` has grouped output by 'State'. You can override using the\n`.groups` argument.\n#Calculate total resignations by state\nsummary_table &lt;- summary_table %&gt;%\n  ungroup() %&gt;%\n  mutate(across(where(is.numeric), as.numeric)) %&gt;%\n  rowwise() %&gt;%\n  mutate(TOTAL = sum(across(where(is.numeric)), na.rm = TRUE)) %&gt;%\n  ungroup()\n\n#Create gt table\ngt_table &lt;- summary_table %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Congressional Resignations by State and Category\"\n  ) %&gt;%\n  fmt_number(\n    columns = -State,\n    decimals = 0\n  ) %&gt;%\n  cols_label(\n    `OTHER OFFICE` = \"Other Office\",\n    `ELECTION OVERTURNED` = \"Election Over- turned\",\n    `PRIVATE SECTOR` = \"Private Sector\",\n    `HEALTH/FAMILY` = \"Health/ Family\",\n    `EARLY DEPARTURE` = \"Early Departure\",\n    `MILITARY SERVICE` = \"Military Service\",\n    `SEXUAL MISCONDUCT` = \"Sexual Misconduct\",\n    `CONSENSUAL SEX SCANDALS` = \"Consensual Sex Scandals\",\n    `OTHER SCANDAL` = \"Other Scandal\",\n    `OTHER REASON` = \"Other Reason\",\n    `TOTAL` = \"Total\"\n  ) %&gt;%\n  data_color(\n    columns = `OTHER OFFICE`,\n    fn = scales::col_numeric(\n      palette = c(\"white\", \"#F5B902\"),\n      domain = NULL\n    )\n  ) %&gt;%\n  data_color(\n    columns = `ELECTION OVERTURNED`,\n    fn = scales::col_numeric(\n      palette = c(\"white\", \"grey20\"),\n      domain = NULL\n    )\n  ) %&gt;%\n  data_color(\n    columns = `PRIVATE SECTOR`,\n    fn = scales::col_numeric(\n      palette = c(\"white\", \"#038FD5\"),\n      domain = NULL\n    )\n  ) %&gt;%\n  data_color(\n    columns = `HEALTH/FAMILY`,\n    fn = scales::col_numeric(\n      palette = c(\"white\", \"#44AB43\"),\n      domain = NULL\n    )\n  ) %&gt;%\n  data_color(\n    columns = `EARLY DEPARTURE`,\n    fn = scales::col_numeric(\n      palette = c(\"white\", \"#AE66AB\"),\n      domain = NULL\n    )\n  ) %&gt;%\n  data_color(\n    columns = `MILITARY SERVICE`,\n    fn = scales::col_numeric(\n      palette = c(\"white\", \"#A4522E\"),\n      domain = NULL\n    )\n  ) %&gt;%\n  data_color(\n    columns = `SEXUAL MISCONDUCT`,\n    fn = scales::col_numeric(\n      palette = c(\"white\", \"#E81D2C\"),\n      domain = NULL\n    )\n  ) %&gt;%\n  data_color(\n    columns = `CONSENSUAL SEX SCANDALS`,\n    fn = scales::col_numeric(\n      palette = c(\"white\", \"#FFB6C1\"),\n      domain = NULL\n    )\n  ) %&gt;%\n  data_color(\n    columns = `OTHER SCANDAL`,\n    fn = scales::col_numeric(\n      palette = c(\"white\", \"#FFE3D7\"),\n      domain = NULL\n    )\n  ) %&gt;%\n  data_color(\n    columns = `OTHER REASON`,\n    fn = scales::col_numeric(\n      palette = c(\"white\", \"#BEBEBE\"),\n      domain = NULL\n    )\n  ) %&gt;%\n  data_color(\n    columns = `TOTAL`,\n    fn = scales::col_numeric(\n      palette = c(\"white\", \"orange\"),\n      domain = NULL\n    )\n  ) %&gt;%\n  tab_footnote(\n    footnote = \"Colors within each column correspond to the representative colors in the stacked bargraph representation. Darker shades within each column correspond to higher counts.\",\n  )\n\n#Save gt table as an image\ngtsave(gt_table, \"state_resignations_table.png\", expand = 10)\nfile:////var/folders/xc/6xhkp8517gb10rdqk_xm0kp40000gn/T//RtmpYbatW8/filebca622871aef.html screenshot completed\n\nThe resulting table is shown below."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "",
    "text": "The structure below is one possible setup for a data analysis project (including the course project). For a manuscript, adjust as needed. You don’t need to have exactly these sections, but the content covering those sections should be addressed.\nThis uses MS Word as output format. See here for more information. You can switch to other formats, like html or pdf. See the Quarto documentation for other formats.\nAsmith Joseph contributed to this exercise."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.1 General Background Information",
    "text": "2.1 General Background Information\nProvide enough background on your topic that others can understand the why and how of your analysis"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.2 Description of data and data source",
    "text": "2.2 Description of data and data source\nThe data contains information on fourteen individuals including height, weight, gender, education level, and the number of books that they have read within the past year."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.3 Questions/Hypotheses to be addressed",
    "text": "2.3 Questions/Hypotheses to be addressed\nState the research questions you plan to answer with this analysis.\nTo cite other work (important everywhere, but likely happens first in introduction), make sure your references are in the bibtex file specified in the YAML header above (here dataanalysis_template_references.bib) and have the right bibtex key. Then you can include like this:\nExamples of reproducible research projects can for instance be found in (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, Shen, & Handel, 2020)"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.1 Data aquisition",
    "text": "3.1 Data aquisition\nAs applicable, explain where and how you got the data. If you directly import the data from an online source, you can combine this section with the next."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.2 Data import and cleaning",
    "text": "3.2 Data import and cleaning\nWrite code that reads in the file and cleans it so it’s ready for analysis. Since this will be fairly long code for most datasets, it might be a good idea to have it in one or several R scripts. If that is the case, explain here briefly what kind of cleaning/processing you do, and provide more details and well documented code somewhere (e.g. as supplement in a paper). All materials, including files that contain code, should be commented well so everyone can follow along."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.3 Statistical analysis",
    "text": "3.3 Statistical analysis\nExplain anything related to your statistical analyses."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.1 Exploratory/Descriptive analysis",
    "text": "4.1 Exploratory/Descriptive analysis\nUse a combination of text/tables/figures to explore and describe your data. Show the most important descriptive results here. Additional ones should go in the supplement. Even more can be in the R and Quarto files that are part of your project.\nTable 1 shows a summary of the data.\nNote the loading of the data providing a relative path using the ../../ notation. (Two dots means a folder up). You never want to specify an absolute path like C:\\ahandel\\myproject\\results\\ because if you share this with someone, it won’t work for them since they don’t have that path. You can also use the here R package to create paths. See examples of that below. I recommend the here package, but I’m showing the other approach here just in case you encounter it.\n\n\n\n\nTable 1: Data summary table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\ncharacter.min\ncharacter.max\ncharacter.empty\ncharacter.n_unique\ncharacter.whitespace\nfactor.ordered\nfactor.n_unique\nfactor.top_counts\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\n\ncharacter\nEducation\n0\n1\n10\n21\n0\n4\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nfactor\nGender\n0\n1\nNA\nNA\nNA\nNA\nNA\nFALSE\n3\nM: 4, F: 3, O: 2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nHeight\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n165.66667\n15.97655\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nnumeric\nWeight\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n70.11111\n21.24526\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nnumeric\nBooks_Read\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n22.33333\n19.83683\n1\n3\n19\n44\n46\n▇▂▁▂▆"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.2 Basic statistical analysis",
    "text": "4.2 Basic statistical analysis\nTo get some further insight into your data, if reasonable you could compute simple statistics (e.g. simple models with 1 predictor) to look for associations between your outcome(s) and each individual predictor variable. Though note that unless you pre-specified the outcome and main exposure, any “p&lt;0.05 means statistical significance” interpretation is not valid.\nFigure 1 shows a scatterplot figure produced by one of the R scripts.\n\n\n\n\n\n\n\n\nFigure 1: Height and weight stratified by gender."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.3 Full analysis",
    "text": "4.3 Full analysis\nUse one or several suitable statistical/machine learning methods to analyze your data and to produce meaningful figures, tables, etc. This might again be code that is best placed in one or several separate R scripts that need to be well documented. You want the code to produce figures and data ready for display as tables, and save those. Then you load them here.\nExample Table 2 shows a summary of a linear model fit.\n\n\n\n\nTable 2: Linear model fit table.\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n149.2726967\n23.3823360\n6.3839942\n0.0013962\n\n\nWeight\n0.2623972\n0.3512436\n0.7470519\n0.4886517\n\n\nGenderM\n-2.1244913\n15.5488953\n-0.1366329\n0.8966520\n\n\nGenderO\n-4.7644739\n19.0114155\n-0.2506112\n0.8120871"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.1 Summary and Interpretation",
    "text": "5.1 Summary and Interpretation\nSummarize what you did, what you found and what it means."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.2 Strengths and Limitations",
    "text": "5.2 Strengths and Limitations\nDiscuss what you perceive as strengths and limitations of your analysis."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.3 Conclusions",
    "text": "5.3 Conclusions\nWhat are the main take-home messages?\nInclude citations in your Rmd file using bibtex, the list of references will automatically be placed at the end\nThis paper (Leek & Peng, 2015) discusses types of analyses.\nThese papers (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, et al., 2020) are good examples of papers published using a fully reproducible setup similar to the one shown in this template.\nNote that this cited reference will show up at the end of the document, the reference formatting is determined by the CSL file specified in the YAML header. Many more style files for almost any journal are available. You also specify the location of your bibtex reference file in the YAML. You can call your reference file anything you like, I just used the generic word references.bib but giving it a more descriptive name is probably better."
  },
  {
    "objectID": "starter-analysis-exercise/code/readme.html",
    "href": "starter-analysis-exercise/code/readme.html",
    "title": "Connor's Data Analysis Portfolio",
    "section": "",
    "text": "Place your various R or Quarto files in the appropriate folders.\nYou can either have fewer large scripts, or multiple scripts that do only specific actions. Those can be R or Quarto files. In either case, document the scripts and what goes on in them so well that someone else (including future you) can easily figure out what is happening.\nThe scripts should load the appropriate data (e.g. raw or processed), perform actions, and save results (e.g. processed data, figures, computed values) in the appropriate folders. Document somewhere what inputs each script takes and where output is placed.\nIf scripts need to be run in a specific order, document this. Either as comments in the script, or in a separate text file such as this readme file. Ideally of course in both locations.\nDepending on your specific project, you might want to have further folders/sub-folders."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda.html",
    "href": "starter-analysis-exercise/code/eda-code/eda.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at /Users/cgnorris/Documents/GitHub/MADA (EPID 8060E)/connornorris-MADA-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\nlibrary(ggplot2)\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          5     \n_______________________          \nColumn type frequency:           \n  character                1     \n  factor                   1     \n  numeric                  3     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate min max empty n_unique whitespace\n1 Education             0             1  10  21     0        4          0\n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean   sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0 133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2  45  55  70  80  110 ▇▂▃▂▂\n3 Books_Read            0             1  22.3 19.8   1   3  19  44   46 ▇▂▁▂▆\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\n\nhere() starts at /Users/cgnorris/Documents/GitHub/MADA (EPID 8060E)/connornorris-MADA-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 5 × 3\n  `Variable Name` `Variable Definition`                     `Allowed Values`    \n  &lt;chr&gt;           &lt;chr&gt;                                     &lt;chr&gt;               \n1 Height          height in centimeters                     numeric value &gt;0 or…\n2 Weight          weight in kilograms                       numeric value &gt;0 or…\n3 Gender          identified gender (male/female/other)     M/F/O/NA            \n4 Education       highest degree obtained                   less than high scho…\n5 Books_Read      number of books read within the past year numeric value &gt;0 or…\n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 5\n$ Height     &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"15…\n$ Weight     &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender     &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"F\"…\n$ Education  &lt;chr&gt; \"bachelor's\", \"less than high school\", \"bachelor's\", \"less …\n$ Books_Read &lt;dbl&gt; 19, 2, 34, 46, 1, 44, 7, 3, 43, 21, 11, 1, 35, 44\n\nsummary(rawdata)\n\n    Height              Weight          Gender           Education        \n Length:14          Min.   :  45.0   Length:14          Length:14         \n Class :character   1st Qu.:  55.0   Class :character   Class :character  \n Mode  :character   Median :  70.0   Mode  :character   Mode  :character  \n                    Mean   : 602.7                                        \n                    3rd Qu.:  90.0                                        \n                    Max.   :7000.0                                        \n                    NA's   :1                                             \n   Books_Read   \n Min.   : 1.00  \n 1st Qu.: 4.00  \n Median :20.00  \n Mean   :22.21  \n 3rd Qu.:41.00  \n Max.   :46.00  \n                \n\nhead(rawdata)\n\n# A tibble: 6 × 5\n  Height Weight Gender Education             Books_Read\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;                      &lt;dbl&gt;\n1 180        80 M      bachelor's                    19\n2 175        70 O      less than high school          2\n3 sixty      60 F      bachelor's                    34\n4 178        76 F      less than high school         46\n5 192        90 NA     bachelor's                     1\n6 6          55 F      high school diploma           44\n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nEducation\n0\n1\n10\n21\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55\n70\n90\n7000\n▇▁▁▁▁\n\n\nBooks_Read\n0\n1.00\n22.21\n18.18\n1\n4\n20\n41\n46\n▇▃▂▃▆\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nEducation\n0\n1\n10\n21\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nBooks_Read\n0\n1.00\n21.31\n18.59\n1\n3.00\n19\n43\n46\n▇▃▂▂▆\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nEducation\n0\n1\n10\n21\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nBooks_Read\n0\n1.00\n21.31\n18.59\n1\n3.00\n19\n43\n46\n▇▃▂▂▆\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nEducation\n0\n1\n10\n21\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179.0\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85.0\n110\n▇▂▃▃▂\n\n\nBooks_Read\n0\n1\n22.27\n20.08\n1\n2.5\n19\n43.5\n46\n▇▂▁▂▆\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nEducation\n0\n1\n10\n21\n0\n4\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179.0\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85.0\n110\n▇▂▃▃▂\n\n\nBooks_Read\n0\n1\n22.27\n20.08\n1\n2.5\n19\n43.5\n46\n▇▂▁▂▆\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nEducation\n0\n1\n10\n21\n0\n4\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nBooks_Read\n0\n1\n22.33\n19.84\n1\n3\n19\n44\n46\n▇▂▁▂▆\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Load required libraries\n\n#Code by Asmith\n# library(readxl)\n# library(dplyr)\n# \n# # Load the new data\n# new_data &lt;- read_excel(\"exampledata2.xlsx\")\n# print(\"exampledata2.xlsx.\")\n# \n# # Inspect the data\n# cat(\"Data Structure:\\n\")\n# str(new_data)\n# \n# cat(\"First few rows of the data:\\n\")\n# head(new_data)\n# \n# # Clean the data\n# new_data &lt;- new_data %&gt;%\n#   mutate(\n#     Height = as.numeric(as.character(Height)),\n#     Gender = toupper(Gender),\n#     Gender = ifelse(Gender %in% c(\"M\", \"F\", \"O\"), Gender, NA)\n#   )\n# \n# # Handle missing values in critical columns\n# cleaned_data &lt;- new_data %&gt;%\n#   filter(!is.na(Height) & !is.na(Weight))\n# \n# # Save the cleaned data\n# saveRDS(cleaned_data, file = \"processeddata2.rds\")\n# print(\"Data cleaned and saved to processeddata2.rds.\")\n\n\n\nProcessing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\n\nhere() starts at /Users/cgnorris/Documents/GitHub/MADA (EPID 8060E)/connornorris-MADA-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata2.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 5 × 3\n  `Variable Name` `Variable Definition`                     `Allowed Values`    \n  &lt;chr&gt;           &lt;chr&gt;                                     &lt;chr&gt;               \n1 Height          height in centimeters                     numeric value &gt;0 or…\n2 Weight          weight in kilograms                       numeric value &gt;0 or…\n3 Gender          identified gender (male/female/other)     M/F/O/NA            \n4 Education       highest degree obtained                   less than high scho…\n5 Books_Read      number of books read within the past year numeric value &gt;0 or…\n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 5\n$ Height     &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"15…\n$ Weight     &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender     &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"F\"…\n$ Education  &lt;chr&gt; \"bachelor's\", \"less than high school\", \"bachelor's\", \"less …\n$ Books_Read &lt;dbl&gt; 19, 2, 34, 46, 1, 44, 7, 3, 43, 21, 11, 1, 35, 44\n\nsummary(rawdata)\n\n    Height              Weight          Gender           Education        \n Length:14          Min.   :  45.0   Length:14          Length:14         \n Class :character   1st Qu.:  55.0   Class :character   Class :character  \n Mode  :character   Median :  70.0   Mode  :character   Mode  :character  \n                    Mean   : 602.7                                        \n                    3rd Qu.:  90.0                                        \n                    Max.   :7000.0                                        \n                    NA's   :1                                             \n   Books_Read   \n Min.   : 1.00  \n 1st Qu.: 4.00  \n Median :20.00  \n Mean   :22.21  \n 3rd Qu.:41.00  \n Max.   :46.00  \n                \n\nhead(rawdata)\n\n# A tibble: 6 × 5\n  Height Weight Gender Education             Books_Read\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;                      &lt;dbl&gt;\n1 180        80 M      bachelor's                    19\n2 175        70 O      less than high school          2\n3 sixty      60 F      bachelor's                    34\n4 178        76 F      less than high school         46\n5 192        90 NA     bachelor's                     1\n6 6          55 F      high school diploma           44\n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nEducation\n0\n1\n10\n21\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55\n70\n90\n7000\n▇▁▁▁▁\n\n\nBooks_Read\n0\n1.00\n22.21\n18.18\n1\n4\n20\n41\n46\n▇▃▂▃▆\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nEducation\n0\n1\n10\n21\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nBooks_Read\n0\n1.00\n21.31\n18.59\n1\n3.00\n19\n43\n46\n▇▃▂▂▆\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nEducation\n0\n1\n10\n21\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nBooks_Read\n0\n1.00\n21.31\n18.59\n1\n3.00\n19\n43\n46\n▇▃▂▂▆\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nEducation\n0\n1\n10\n21\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179.0\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85.0\n110\n▇▂▃▃▂\n\n\nBooks_Read\n0\n1\n22.27\n20.08\n1\n2.5\n19\n43.5\n46\n▇▂▁▂▆\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nEducation\n0\n1\n10\n21\n0\n4\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179.0\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85.0\n110\n▇▂▃▃▂\n\n\nBooks_Read\n0\n1\n22.27\n20.08\n1\n2.5\n19\n43.5\n46\n▇▂▁▂▆\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nEducation\n0\n1\n10\n21\n0\n4\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nBooks_Read\n0\n1\n22.33\n19.84\n1\n3\n19\n44\n46\n▇▂▁▂▆\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/results/readme.html",
    "href": "starter-analysis-exercise/results/readme.html",
    "title": "Connor's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains results produced by the code, such as figures and tables.\nDepending on the size and type of your project, you can either place it all in a single folder or create sub-folders. For instance you could create a folder for figures, another for tables. Or you could create a sub-folder for dataset 1, another for dataset 2. Or you could have a subfolder for exploratory analysis, another for final analysis. The options are endless, choose whatever makes sense for your project. For this template, there is just a a single folder, but having sub-folders is often a good idea."
  },
  {
    "objectID": "starter-analysis-exercise/data/raw-data/readme.html",
    "href": "starter-analysis-exercise/data/raw-data/readme.html",
    "title": "Connor's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains a simple made-up data-set in an Excel file.\nIt contains the variables Height, Weight and Gender of a few imaginary individuals.\nThe dataset purposefully contains some faulty entries that need to be cleaned.\nGenerally, any dataset should contain some meta-data explaining what each variable in the dataset is. (This is often called a Codebook.) For this simple example, the codebook is given as a second sheet in the Excel file.\nThis raw data-set should generally not be edited by hand. It should instead be loaded and processed/cleaned using code."
  }
]