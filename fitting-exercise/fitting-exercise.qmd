---
title: "Model Fitting Exercise"
editor: visual
---

# Model Fitting

For this exercise, I will work with data on the drug candidate Mavoglurant and practice modeling in the `tidymodels` framework.

```{r, message = FALSE}
#Load required packages
library(tidymodels)
library(tidyverse)
library(here)
library(gt)
library(knitr)
library(kableExtra)
```

```{r, message = FALSE}
#Define random seed value
rngseed = 1234

#Load Mavoglurant data
data_path <- here("fitting-exercise", "Mavoglurant_A2121_nmpk.csv")
data <- read_csv(data_path)
```

## Data Exploration

```{r}
#Get structure of data
summary(data)
str(data)
```

```{r}
#Plot dependent variable over time, grouped by each person and dose
ggplot(data, aes(x = TIME, y = DV, group = as.factor(ID), color = as.factor(DOSE))) +
  geom_line() +
  labs(x = "Time", y = "Mavoglurant", color = "Dose") +
  theme_minimal()
```

## Data Processing

```{r}
#Filter data
data <- data %>%
  filter(OCC == 1) #One entry per person

#Add together all dose values
dose_sum <- data %>%
  filter(TIME != 0) %>% #No time zeros (for dose summation)
  group_by(ID) %>% #Ensure only DV values for one individual are added together
  summarize(Y = sum(DV, na.rm = T))

#Create data frame where only time = 0
time_zero <- data %>%
  filter(TIME == 0)

#Join time_zero and dose_sum together to get working dataset
df <- left_join(time_zero, dose_sum, by = "ID")

#Check structure
str(df) #120 x 18 tibble -> good

#Filter to only relevant variables, convert others to factors
df <- df %>%
  select(Y, DOSE, AGE, SEX, RACE, WT, HT) %>%
  mutate(SEX = as.factor(SEX)) %>%
  mutate(RACE = as.factor(RACE))
```

## Further Exploratory Data Analysis

```{r}
#### Summary tables ####
#Get base summary statistics
summary(df)

#Summary statistics of Y grouped by DOSE
YxDOSE <- df %>%
  group_by(DOSE) %>%
  summarize(
    Mean = round(mean(Y, na.rm = T), 2),
    Median = round(median(Y, na.rm = T), 2),
    SD = round(sd(Y, na.rm = T), 2),
    Min = round(min(Y, na.rm = T), 2),
    Max = round(max(Y, na.rm = T), 2)
  ) %>%
  gt() %>%
  tab_header(
    title = "Summary of Drug Total (Y) by Dose"
  )
YxDOSE

#Summary statistics of Y by Sex and Race
YxSexRace <- df %>%
  group_by(SEX, RACE) %>%
  summarize(
    Mean = round(mean(Y, na.rm = T), 2),
    SD = round(sd(Y, na.rm = T), 2),
    n = n()
  ) %>%
  gt() %>%
  tab_header(
    title = "Summary of Drug Total by Sex and Race"
  )
YxSexRace
```

```{r}
#### Variable Distributions ####
#Plot histograms of select variables
df %>%
  select(Y, DOSE, AGE, HT, WT) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value") %>%
  ggplot(aes(x = Value)) +
  geom_histogram(bins = 30, fill = "steelblue") +
  facet_wrap(~ Variable, scales = "free") + 
  theme_minimal() +
  labs(
    x = "Value",
    y = "Count",
    title = "Histograms of Age, Dose, Height, Weight, and Drug Levels"
  )

#Plot boxplots of drug levels by dose
ggplot(df, aes(x = as.factor(DOSE), y = Y, fill = as.factor(DOSE))) +
  geom_boxplot() +
  labs(x = "Dose", y = "Total Drug (Y)", fill = "Dose",
       title = "Drug Levels by Dose") +
  theme_minimal()

#Plot boxplots of drug levels by sex
ggplot(df, aes(x = SEX, y = Y, fill = SEX)) +
  geom_boxplot() +
  labs(x = "Sex", y = "Total Drug (Y)", fill = "Sex",
       title = "Drug Levels by Sex") +
  theme_minimal()

#Plot boxplots of drug levels by race
ggplot(df, aes(x = RACE, y = Y, fill = RACE)) +
  geom_boxplot() +
  labs(x = "Race", y = "Total Drug (Y)", fill = "Race",
       title = "Drug Levels by Race") +
  theme_minimal()

#Plot scatterplot of drug levels by age
ggplot(df, aes(x = AGE, y = Y)) +
  geom_point(color = "blue") +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  labs(x = "Age", y = "Total Drug (Y)", title = "Total Drug vs. Age") +
  theme_minimal()

#Plot scatterplot of drug levels by weight
ggplot(df, aes(x = WT, y = Y)) +
  geom_point(color = "green") +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  labs(x = "Weight", y = "Total Drug (Y)", title = "Total Drug vs. Weight") +
  theme_minimal()

#Plot scatter plot of drug levels by height
ggplot(df, aes(x = HT, y = Y)) +
  geom_point(color = "purple") +
  geom_smooth(method = "lm", color = "red", se = FALSE) +
  labs(x = "Height", y = "Total Drug (Y)", title = "Total Drug vs. Height") +
  theme_minimal()

```

The distribution of age was generally bimodal, with packs around 26 and 37. The distribution of height is slightly right skewed, with a center around 1.8. The distribution of weight was relatively normal, with a center around 83. The distribution of the total drug amount was slightly left skewed, with a center at between 2000 and 2500. For a dosage of 25, the mean drug amount was 1782.67; for a dosage of 37.5, the mean drug amount was 2463.91; and for a dosage of 50, the mean drug amount was 3238.87. Upon visual inspection, there seems to be no association between age and total drug amount and slight negative associations between total drug amount and height and weight.

## Modeling

```{r}
#### Linear modeling of Y ####

#Build model object
lm_model <- linear_reg() #Define model object as a linear regression

#Fit a model predicting Y from DOSE (model 1)
dose_Y_fit <- lm_model %>% fit(Y ~ DOSE, data = df)

#Fit a model predicting Y from all other predictors (model 2)
all_Y_fit <- lm_model %>% fit(Y ~ ., data = df)

#Calculate RMSE and R-squared for both models
metrics1 <- predict(dose_Y_fit, df) %>%
  bind_cols(df) %>%
  metrics(truth = Y, estimate = .pred)

metrics2 <- predict(all_Y_fit, df) %>%
  bind_cols(df) %>%
  metrics(truth = Y, estimate = .pred)

#Print metrics for both models
print(metrics1)
print(metrics2)

```

For the model fitting the total drug amount from the dose, the RMSE was 666.462 and the R^2^ was 0.516. For the model fitting the total drug amount from all available predictors, the RMSE was 590.853 and the R^2^ was 0.619.

```{r}
#### Logistic regression of sex ####
#Build logistic regression model object
log_reg <- logistic_reg()

#Fit model predicting sex from dose (model 3)
dose_sex_fit <- log_reg %>% fit(SEX ~ DOSE, data = df)

#Fit model predicting sex from all predictors (model 4)
all_sex_fit <- log_reg %>% fit(SEX ~ ., data = df)

#Calculate accuracy values for both models
m3_acc <- predict(dose_sex_fit, df) %>% #Predict sex using model
  bind_cols(df) %>% #Attach predictions to original dataset
  accuracy(truth = SEX, estimate = .pred_class) #Compute accuracy

m4_acc <- predict(all_sex_fit, df) %>%
  bind_cols(df) %>%
  accuracy(truth = SEX, estimate = .pred_class)

#Compute ROC-AUC values for both models
m3_roc_auc <- predict(dose_sex_fit, df, type = "prob") %>% #Predict probabilities of each sex
  bind_cols(df) %>%
  roc_auc(truth = SEX, .pred_1) #Calculate ROC-AUC

m4_roc_auc <- predict(all_sex_fit, df, type = "prob") %>%
  bind_cols(df) %>%
  roc_auc(truth = SEX, .pred_1)

#Print metrics
print(paste("Accuracy for model fitting sex from dose:", round(m3_acc$.estimate, 3)))
print(paste("ROC-AUC for model fitting sex from dose:", round(m3_roc_auc$.estimate, 3)))
print(paste("Accuracy for model fitting sex from all predictors:", round(m4_acc$.estimate, 3)))
print(paste("ROC-AUC for model fitting sex from all predictors:", round(m4_roc_auc$.estimate, 3)))
```

The models that used all available predictors had a better fit than the models that used just dose as a predictor.

# Model Improvement

For this exercise, I will expand upon previous work to fit simple models and do additional performance assessment.

## Further Data Wrangling

```{r}
#Remove race from the dataset
df <- select(df, -RACE)

#Confirm sucessful removal
summary(df)
```

```{r}
#Set random seed for data splitting
set.seed(rngseed)

#Split data into a 75% training set and a 25% testing set
data_split <- initial_split(df, prop = 3/4)

#Set data frames for the training and testing sets
train <- training(data_split)
test <- testing(data_split)
```

## New Model Fitting

```{r}
#### Fit new linear models using training data ####
#Fit a model predicting Y from DOSE (model 1)
dose_Y_fit <- lm_model %>% fit(Y ~ DOSE, data = train)

#Fit a model predicting Y from all other predictors (model 2)
all_Y_fit <- lm_model %>% fit(Y ~ ., data = train)
```

## Model Performance Assessment 1

```{r}
#Calculate RMSE and R-squared for both models using training data
metrics1 <- predict(dose_Y_fit, train) %>%
  bind_cols(train) %>%
  metrics(truth = Y, estimate = .pred)

metrics2 <- predict(all_Y_fit, train) %>%
  bind_cols(train) %>%
  metrics(truth = Y, estimate = .pred)

#Print metrics for both models
print(metrics1)
print(metrics2)

#Fit a null model using the training data
null_mod <- null_model() %>%
  set_engine("parsnip") %>%
  set_mode("regression")

null_fit <- 
  null_mod %>%
  fit(Y ~ 1, data = train)

#Compute RMSE and R-squared for a null model
metrics_null <- predict(null_fit, train) %>%
  bind_cols(train) %>%
  metrics(truth = Y, estimate = .pred)

#Print metrics for the null model
print(metrics_null)
```

## Model Performance Assessment 2

```{r, warning=FALSE}
#### Evaluate the model using cross-validation (CV) with the training data ####
#Reset random seed
set.seed(rngseed)

#Set CV folds (n = 10)
folds <- vfold_cv(train, v = 10)

#Define workflow for CV
cv_wf <-
  workflow() %>%
  add_model(lm_model)

#Fit a model predicting Y from dose using the CV folds
model1_fit_cv <- 
  cv_wf %>%
  add_formula(Y ~ DOSE) %>%
  fit_resamples(folds)

#Fit a model predicting Y from all predictors using the CV folds
model2_fit_cv <-
  cv_wf %>%
  add_formula(Y ~ .) %>%
  fit_resamples(folds)

#Create a recipe with only the outcome (removing all predictors for a null model)
null_recipe <- recipe(Y ~ 1, data = train)

#Define a new workflow for a null model
cv_wf_null <- 
  workflow() %>%
  add_model(null_mod) %>%
  add_recipe(null_recipe)

#Fit the null model using CV folds
null_fit_cv <- fit_resamples(cv_wf_null, folds)

#Display metrics of all models
collect_metrics(model1_fit_cv)
collect_metrics(model2_fit_cv)
collect_metrics(null_fit_cv)
```

After completing the ten-fold cross-validation, the null model performed largely the same, and the model fitting Y from all predictors performed better than the model fitting Y from just dose. However, the difference in performance between the two non-null models was less pronounced.

```{r}
#### Re-running CV using a new seed ####
#Reset seed
set.seed(22)

#Set CV folds (n = 10)
folds <- vfold_cv(train, v = 10)

#Fit a model predicting Y from dose using the CV folds
model1_fit_cv <- 
  cv_wf %>%
  add_formula(Y ~ DOSE) %>%
  fit_resamples(folds)

#Fit a model predicting Y from all predictors using the CV folds
model2_fit_cv <-
  cv_wf %>%
  add_formula(Y ~ .) %>%
  fit_resamples(folds)

#Fit the null model using CV folds
null_fit_cv <- fit_resamples(cv_wf_null, folds)

#Display metrics of all models
collect_metrics(model1_fit_cv)
collect_metrics(model2_fit_cv)
collect_metrics(null_fit_cv)
```

With a new seed, the mean RMSE values of the models predicting Y from all predictors and from just dose were about the same. However, the standard errors of both metrics were lower, and the standard error for the model with dose as the only predictor was slightly lower than the standard error of the model with all predictors.

## Part 2

### This section was added by Alexis Gonzalez 

### Creating data frames with observed and predicted values

```{r}
dose_preds <- predict(dose_Y_fit, new_data = train) %>%
  bind_cols(train$Y) %>%
    mutate(Model = "Model 1")
colnames(dose_preds) <-c("Predicted", "Observed", "Model")
```

```{r}
all_preds <- predict(all_Y_fit, new_data = train) %>%
  bind_cols(train$Y) %>%
    mutate(Model = "Model 2")
colnames(all_preds) <-c("Predicted", "Observed", "Model")
```

```{r}
#Null model ( i had issues with null model so I am making the predictions and everything over again)
null_mod_new <- null_model(mode = "regression") %>%
  set_engine("parsnip") %>%
  fit(Y ~ 1, data= train) 

  null_preds <- predict(null_mod_new, new_data = train) %>%
  bind_cols(train$Y) %>%
    mutate(Model = "Null")
colnames(null_preds) <-c("Predicted", "Observed", "Model")

```

### Plotting the values on a scatter plot observed vs predicted

```{r}
#combining all into 1 dataframe
all_models <- bind_rows(dose_preds,all_preds,null_preds)
```

```{r}
#PLOT
ggplot(all_models, aes(x= Observed, y=Predicted, color = Model, shape = Model)) +
  geom_point(alpha = 0.7, size = 3) +
  geom_abline(slope = 1, intercept = 0, linetype = "solid", color = "black") +
  labs(title = "Observed vs. Predicted values for Each Model",
       x = "Observed",
       y = "Predicted") +
  theme_minimal() +
  scale_color_manual(values = c("pink","purple","blue")) +
  scale_shape_manual(values = c(16,17,18)) +
  coord_cartesian(xlim = c(0,5000), ylim = c(0,5000)) +
theme(legend.title = element_blank())

```

### Plotting the residuals for Model 2, the one with all of the variables

```{r}
#Calculate the residuals
Model2 <- all_preds %>%
  mutate(Residuals = Observed - Predicted)

```

```{r}
ggplot(Model2, aes(x = Observed, y=Residuals)) +
  geom_point(color = "blue", alpha = 0.6) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residuals vs. Fitted for Model 2",
       x = "Fitted Values",
       y = "Residuals") +
  theme_minimal()+
  coord_cartesian(xlim = c(0,5000), ylim = c(-5000,5000))


```

### Model predictions and uncertainty

```{r}
set.seed(rngseed)
```

#### Create 100 bootstraps

```{r}
bootstrap_samples <- lapply(1:100, function(i) {
  sample(nrow(train), replace = TRUE)  
})

```

#### Creating a loop to fit model 2 to each bootstrap

```{r}
bootstrap_preds <- vector("list", 100)

for (i in 1:100) {
  boot_model <- lm(Y ~ . , data = train[bootstrap_samples[[i]], ])
  predictions <- predict(boot_model, newdata = train)
  bootstrap_preds[[i]] <- predictions
}
predictions_df <- do.call(cbind, bootstrap_preds) 

```

```{r}
preds <- predictions_df|> apply(2, quantile,  c(0.025, 0.5, 0.975)) |>  t()

```

#### Plot the data

```{r}
# 1. Point estimate (mean of the original model predictions)
point_estimate <- rowMeans(predictions_df)  # mean of predictions from all bootstrap samples

# 2. Median of the bootstrap predictions
median_preds <- apply(predictions_df, 1, median)  # median across rows

# 3. Confidence intervals (95% CI) for bootstrap predictions
lower_bound <- apply(predictions_df, 1, function(x) quantile(x, 0.025))  # 2.5 percentile
upper_bound <- apply(predictions_df, 1, function(x) quantile(x, 0.975))  # 97.5 percentile

```

```{r}
# Combine observed values, point estimates, and confidence intervals into a data frame
plot_data <- data.frame(
  Observed = train$Y,  # Observed values
  PointEstimate = point_estimate,
  Median = median_preds,
  LowerCI = lower_bound,
  UpperCI = upper_bound
)

```

```{r}
ggplot(plot_data, aes(x = Observed)) +
  geom_point(aes(y = PointEstimate), color = "black", shape = 16, size = 2, alpha = 0.7) + 
  geom_point(aes(y = Median), color = "purple", shape = 17, size = 2, alpha = 0.7) +
  geom_point(aes(y = LowerCI), color = "pink", shape = 18, size = 2, alpha = 0.7) +
  geom_point(aes(y = UpperCI), color = "pink", shape = 18, size = 2, alpha = 0.7) +
  geom_abline(slope = 1, intercept = 0, linetype = "solid", color = "black") +
  labs(title = "Observed vs. Predicted (Bootstrap Confidence Intervals)",
       x = "Observed",
       y = "Predicted (Point Estimate, Median, CI)") +
  theme_minimal() +
  theme(axis.text.x = element_text(size = 8, angle = 45, hjust = 1)) +
  scale_color_manual(values = c("black", "purple", "pink"))

```

The orginal model for Model 2 and the bootstrapped model look very similar indicating that model is performing consistently. This also suggest that the model if well fit and will adapt well to new data. This data is also fit well to the line, the closer it is to that 45 degree line, the closer the actual and observed values are.

## Part 3

```{r}
#Make predictions on the testing data using the model with all predictors
test_preds <- predict(all_Y_fit, new_data = test) %>%
  bind_cols(test) %>% #Add preds to test dataset for plotting
  mutate(designation = rep("Testing", nrow(test))) #Add column to designate data was part of testing set

#Make data frame with predictions for training data for plotting
train_preds <- predict(all_Y_fit, new_data = train) %>%
  bind_cols(train) %>%
  mutate(designation = rep("Training", nrow(train)))

#Combine training and testing data predictions
full_preds <- rbind(train_preds, test_preds)
```

```{r}
#Plot observed vs. predicted values for training and testing data
ggplot(data = full_preds, aes(x = Y, y = .pred, color = designation)) +
  geom_point() +
  labs(
    x = "Observed",
    y = "Predicted",
    title = "Observed vs. Predicted Values for Training and Testing Data",
    color = "Designation"
  ) +
  geom_abline(intercept = 0, slope = 1) +
  scale_x_continuous(limits = c(0, 5000)) +
  scale_y_continuous(limits = c(0, 5000)) +
  theme_minimal()
```

The model that predicted Y from all available predictors performed decently well and certainly better than a null model or a model where dosage was the only predictor. When comparing the observed values vs. the predicted values from a model in a scatter plot, a perfect fit would result in the points lining up in a straight diagonal line. The plot above shows some variability in the predicted values, but the trend of the plots is in the right direction. The model was built off of a training subset of the overall data, and the model's performance in making predictions off of the rest of the data set had similar patterns as it's performance in making predictions off of the data used to fit the model. This indicates that the model is generalizable and could be used in other similar circumstances.
